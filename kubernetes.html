<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kubernetes | My References</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Kubernetes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<link rel="canonical" href="http://localhost:4000/kubernetes" />
<meta property="og:url" content="http://localhost:4000/kubernetes" />
<meta property="og:site_name" content="My References" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kubernetes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","headline":"Kubernetes","url":"http://localhost:4000/kubernetes"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="My References" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">My References</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Kubernetes</h1>
  </header>

  <div class="post-content">
    
<h2 id="docker-images">Docker images</h2>

<p>There are some gotchas to keep in mind about Docker images.</p>

<ul>
  <li>
    <p>Files that are removed by subsequent layers in the system are actually still present in the images; they are just inaccessible.  For example if layer A has a large file named BigFile and layer B removed the BigFile, layer C which builds on will still have the BigFile and that will have impact every time you need to publish or pull the image C even though the file is inaccessible.</p>
  </li>
  <li>
    <p>Each layer is an independent delta from the layer below it. Every time you change a layer, it changes every layer that comes after it. Changing the preceding layers means that they need to be rebuilt, re-pushed, and re-pulled to deploy your image to development. For example if you have a layer with source code and a layer that adds the needed libraries you want the source code layer which is more likely to change to be the subsequent layer.  In general you want to order your layers from least likely to change to most likely to change to optimize the image size for pushing and pulling.</p>
  </li>
  <li>
    <p>Secrets and images should never be mixed. Never put secrets into your images.</p>
  </li>
</ul>

<h3 id="multistage-image-builds">Multistage image builds</h3>

<p>One of the most common ways to unintentionally build large images is to do the actual program compilation as part of the construction of the application container image. This may leave development tools and artifacts in your image which can be quite large.  To resolve this issue Docker intruduced <strong>multistage builds</strong>. With multi stage builds instead of producing a single image a Docker file can produce multiple images with each image considered a stage. Artifacts can be copied from preceding stages to the current stage.</p>

<h2 id="pods">Pods</h2>

<p>Kubernetes groups multiple containers into a single atomic unit called a <em>Pod</em>. A Pod represents a collection of application containers and volumes running in the same execution environment. Pods not containers are the smallest deployable artifact in a Kubernetes cluster. All containers in a pod land on the same machine. Each container within a Pod runs in its own cgroup, but they share a number of Linux namespaces. Applications running int he same Pod share the same IP address and port space (network namespace), have the same hostname (UTS namespace) and can communicate using native inter-process communication channels over System V IPC or POSIX message queues (IPC namespace).  Applications in different pods are isolated from each other. They have different IP addresses, different host names, and more. Containers in different Pods running on the same node may as well be on different servers.</p>

<p>When designing Pods ask yourself if these containers will work the same if they are on different machines. If the answer is no Pod is the correct grouping for the containers. If the answer is yes then multiple Pods is probably the correct answer.</p>

<p>Groups of containers, can group together images developed by different teams into a single deployable unit.</p>

<p>Pods are described in a Pod manifest. The Pod manifest is just a text file representation of the Kubernetes API object. The Kubernetes API server accepts and processes Pod manifests before storing them in persistent storage (etcd). The scheduler also uses the Kubernetes API to find Pods that haven’t been scheduled to a node. The scheduler then places the Pods onto nodes depending on the resources and other constraints expressed in the Pod manifests. Kubernetes tries to ensure that Pods from the same application are distributed onto different machines for reliability. Once scheduled to a node, Pods don’t move and must be explicitly destroyed and rescheduled or a node fails.  To create multiple pods you want to use ReplicaSets.</p>

<p>Below is an example of a Pod manifest file.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kuard-data"</span>
      <span class="na">nfs</span><span class="pi">:</span>
        <span class="na">server</span><span class="pi">:</span> <span class="s">my.nfs.server.local</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/exports"</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-arm64:blue</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
      <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
          <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">resources</span><span class="pi">:</span>
        <span class="na">requests</span><span class="pi">:</span>
          <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>
          <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>
        <span class="na">limits</span><span class="pi">:</span>
          <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1000m"</span>
          <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">256Mi"</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/data"</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kuard-data"</span>
      <span class="na">livenessProbe</span><span class="pi">:</span>
        <span class="na">httpGet</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/healthy</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">5</span>
        <span class="na">timeoutSeconds</span><span class="pi">:</span> <span class="m">1</span>
        <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">10</span>
        <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">3</span>
      <span class="na">readinessProbe</span><span class="pi">:</span>
        <span class="na">httpGet</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/ready</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">30</span>
        <span class="na">timeoutSeconds</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p>Kubernetes tracks both the <em>requests</em> and <em>upper limits</em> for resources for each Pod that runs on a machine. Resources requested by a Pod are guaranteed to be present on the node, while a Pod’s limit is the maximum amount of a given resource that a Pod can consume. A Pod’s limit can be higher than its request, in which case the extra resources are supplied on a best effort basis. They are not guaranteed to be present on a node.</p>

<h3 id="health-checks">Health Checks</h3>

<p>If Kubernetes detects that the main process of your application is not running it will restart it. In most cases a simple process check is insufficient to verify health (process may be running but deadlocked). To address this Kubernetes introduced health checks for application <em>liveness</em>. Since these health checks are application specific you have to define them in your Pod manifest (see example above).  In addition to HTTP checks, Kubernetes supports tcpSocket health checks that open a TCP socket; if the connection is successful, the probe succeeds.  Kubernetes also allows <code class="language-plaintext highlighter-rouge">exec</code> probes. These execute a script or program int he context of the container.  If this script returns a zero exit code, the probe succeeds; otherwise it fails.</p>

<p>Kubernetes also provides a <em>readiness</em> probe. Kubernetes makes a distinction between liveness and readiness.  Liveness determines if an application is running properly. Containers that fail liveness are restarted. Readiness describes when a container is ready to serve user requests. Containers that fail readiness checks are removed from load balancers. Readiness probes are configured similarly to liveness probes.  Combining readiness and liveness probes helps ensure only healthy containers are running within the cluster.</p>

<h3 id="resource-management">Resource management</h3>

<p>Kubernetes allows users to specify two different resource metrics. Resource <strong>requests</strong> specify the minimum amount of a resource required to run the application. Resource <strong>limits</strong> specify the maximum amount of a resource that an application can consume.</p>

<h4 id="resource-requests-minimum-required-resources">Resource requests: minimum required resources</h4>

<p>A Pod requests resources required to run its containers. Kubernetes guarantees that these resources are available to the Pod. Most commonly CPU and memory is requested, but Kubernetes supports others such as GPUs.</p>

<p>Resources are requested per container not per Pod. The total resources requested by the Pod is the sum of all resources requested by the containers in the Pod. The reason for this is containers may have different CPU requirements.</p>

<h4 id="request-limit-details">Request limit details</h4>

<p>The scheduler makes sure that the resource requests are available and do not exceed the capacity of the node. The request specifies a minimum, but not the maximum a Pod can use. A Pod can go above its requested limits as long as they are available on the system. CPU requests are handled using <code class="language-plaintext highlighter-rouge">cpu-shares</code> functionality in the Linux kernel.  For memory since the OS cannot just remove memory from a process once its allocated so when system runs out of memory <code class="language-plaintext highlighter-rouge">kubelet</code> terminates containers whose memory usage is greater thant heir requested memory. These container are restarted but with less available memory for the container to use.  Limits can be used to place an upper bound and not let a containers use unlimited resources.</p>

<h3 id="persisting-data-with-volumes">Persisting Data with Volumes</h3>

<h4 id="using-volumes-with-pods">Using volumes with pods</h4>

<p>To add a volume to a Pod manifest you need to add the <code class="language-plaintext highlighter-rouge">spec.volumes</code> section which defines all the volumes that may be accessed by containers in the Pod manifest. Not all containers are required to mount all of the volumes int the Pod. You also need to add <code class="language-plaintext highlighter-rouge">volumeMounts</code> array in the container definition which defines volumes mounted into a particular container and the path they are mounted at (there are examples of both in the sample manifest). Two different containers in a Pod can mount the same volume at different mount paths.</p>

<p>Below are some examples of how volumes can be used in Kubernetes</p>

<h4 id="communicationsynchronization">Communication/synchronization</h4>

<p>Have two containers mount same file system to use it as a communication/synchronization mechanism. One container can periodiclaly pull down latest code from Git and another container can serve that code content for example.</p>

<h4 id="cache">Cache</h4>

<p>Volume can be used as a cache while container is running. The volume will survive a restart so the cached content will persist till contaienr is deleted.</p>

<h4 id="persistent-data">Persistent Data</h4>

<p>Sometimes a volume will be used for truly persistent data the life span of which is independent of the container using it. The volume should also be able to move from host to host. Kubernetes supports a bunch of options for this including NFS, iSCSI, EBS, Azure Filet Disk, etc.</p>

<h4 id="mounting-the-host-filesystem">Mounting the host filesystem</h4>

<p>If you don’t need a persistent volume but do need access to underlying file system for example access to /dev for devices. For these scenarios Kubernetes supports <code class="language-plaintext highlighter-rouge">hostPath</code> volumes which can mount arbitrary locations on the worker node into the container.</p>

<h4 id="persisting-data-using-remote-disks">Persisting Data Using Remote Disks</h4>

<p>Kubernetes supports NFS and iSCSI as mentioned, but cloud providers have support for this as well, and will often create the volume for you if one des not exist.</p>

<h2 id="namespaces">Namespaces</h2>

<p>Provide isolation and access control so that each microservice can control the degree to which other services interact with it. Namespaces can be used to isolate developer environments. For example instead of setting up a cluster for each developer you can have one cluster and have a namespace for each developer. This makes it conceivable to deploy and test every single commit.</p>

<h2 id="ingress">Ingress</h2>

<p>Objects provide an easy to use frontend that can combine multiple micro-services into a single external API surface area.</p>

<h2 id="nodes">Nodes</h2>

<p>Nodes are divided into <strong>master</strong> nodes and <strong>worker</strong> nodes. The master node will run containers such as the API server scheduler, etc. which manage the cluster. Kubernetes does not generally schedule work onto master nodes to ensure user workloads don’t harm the overall operation of the cluster.</p>

<h2 id="system-containers-vs-application-containers">System containers vs application containers</h2>

<p>System containers seek to mimic virtual machines and often run a full boot process. They often include a set of system services typically found on a VM, such as ssh, cron and syslog. Over time this approach became seen as poor practice and application containers gained favor. Application containers commonly run a single problem. While a single program per container may seem like an unnecessary constrains, it provides the perfect level of granularity for composing scalable applications and is a design philosophy heavily used by Pods.</p>

<h2 id="cluster-components">Cluster Components</h2>

<p>Many of the components that make up Kubernetes are deployed using Kubernetes itself.  Some of these components are…</p>

<h3 id="kubernetes-proxy">Kubernetes proxy</h3>

<p>The kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster. To do this job the, the proxy must be present on every node int he cluster.</p>

<h3 id="the-docker-container-runtime">The Docker Container Runtime</h3>

<p>Kubernetes relies on a container runtime to actually run containers.  The interface to this container runtime is defined by the <strong>Container Runtime Interface (CRI)</strong> standard.  The CRI API is implemented by a number of different programs, including <code class="language-plaintext highlighter-rouge">containerd-cri</code> built by Docker and the <code class="language-plaintext highlighter-rouge">cir-o</code> implementation from Red Hat.</p>

<h3 id="kubelet">Kubelet</h3>

<p>Kubelet is the daemon on each node that launches containers. Under the hood Kubernetes uses <code class="language-plaintext highlighter-rouge">cgroup</code> technology to restrict resource utilization.</p>

<h3 id="controller-manager">Controller-manager</h3>

<p>The <code class="language-plaintext highlighter-rouge">controller-manager</code> is responsible for running various controllers that regulate behavior in the cluster; for example, ensuring that tall of the replicas of a service are available and healthy.</p>

<h3 id="scheduler">scheduler</h3>

<p>The <code class="language-plaintext highlighter-rouge">sceduler</code> is responsible for placing different pods onto different nodes in the cluster.</p>

<h3 id="etcd">etcd</h3>

<p><code class="language-plaintext highlighter-rouge">etcd</code> server is the storage for the cluster where all of the API objects are stored.</p>

<h3 id="kubernetes-dns">Kubernetes DNS</h3>

<p>Kubernetes runs a DNS server, which provides naming and discovery for the services that are defined in the cluster. This DNS server also runs as a replicated service on the cluster. Depending on the size of your cluster, you may see one or more DNS servers running in your cluster.</p>

<h3 id="kubernetes-ui">Kubernetes UI</h3>

<p>The UI is run as a single replica, but it is still managed by a Kubernetes deployment for reliability and upgrades. You can use kubectl proxy to access this UI.  If you run the command <code class="language-plaintext highlighter-rouge">kubectl proxy</code> you should be able to get tot he UI at <code class="language-plaintext highlighter-rouge">http://localhost: 8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</code>.</p>

<p>This component may not always be installed.</p>

<h2 id="kubectl">Kubectl</h2>

<p><code class="language-plaintext highlighter-rouge">kubectl</code> is the official Kubernetes client. Its a command line tool for interacting with Kubernetes API. It can be used to manage most Kubernetes objects as well as explore and verity the overall health of the cluster.</p>

<h3 id="checking-cluster-status">Checking cluster status</h3>

<p><code class="language-plaintext highlighter-rouge">kubectl version</code> will display the version of the kubectl tool as well as the version of the server.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get componentstatuses</code> will give you a quick diagnostic for the cluster.</p>

<h3 id="listing-kubernetes-worker-nodes">Listing Kubernetes worker nodes</h3>

<p><code class="language-plaintext highlighter-rouge">kubectl get nodes</code> lists nodes</p>

<p><code class="language-plaintext highlighter-rouge">kubectl describe nodes node-1</code> Give you details about a node hardware, resource utilizations and pods its running.</p>

<h3 id="working-with-namespaces">Working with namespaces</h3>

<p>By default, kubectl tool interacts with the <code class="language-plaintext highlighter-rouge">default</code> namespace. If you want to use a different namespace, you can pass the <code class="language-plaintext highlighter-rouge">--namespace</code> flag for example <code class="language-plaintext highlighter-rouge">kubectl --namespace=mystuff</code></p>

<p>If you want to change the default namespace more permanently you can use a <strong>context</strong>. This gets recorded in a kubectl file usually located ate <code class="language-plaintext highlighter-rouge">$HOME/.kube/config</code>.  To create a context with a different default you would run the command <code class="language-plaintext highlighter-rouge">kubectl config set-context my-context --namespace=mystuff</code>. This will create a new context but does not actually start using it. To use this context you run the command <code class="language-plaintext highlighter-rouge">kubectl config use-context my-context</code>. Contexts can be used to manage different clusters or different users for authenticating to those clusters using the <code class="language-plaintext highlighter-rouge">--users</code> or <code class="language-plaintext highlighter-rouge">--clusters</code> flags with the <code class="language-plaintext highlighter-rouge">set-context</code> command.</p>

<h3 id="kubectl-display-formats">kubectl display formats</h3>

<p>You can use the <code class="language-plaintext highlighter-rouge">-o wide</code> modifier to get more details (in less human readable format that Kubernetes tries to limit to a single line) when using <code class="language-plaintext highlighter-rouge">get</code> to display resources. You can also use <code class="language-plaintext highlighter-rouge">-o json</code> to get a JSON output which will have full details. Another common task is extracting specific field from the object.  <code class="language-plaintext highlighter-rouge">kubectl</code> uses the JSONPATH query language to select fields int eh returned object. For example</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get pods my-pod -o jsonpath --template={.status.podIP}</code></p>

<h3 id="kubectl-describe">kubectl describe</h3>

<p>If you are interested in details for an object use the command <code class="language-plaintext highlighter-rouge">kubectl describe &lt;resource-name&gt; &lt;obj-name&gt;</code>. This will provide a rich multi line human readable description of the object as well as related objects and events.</p>

<h3 id="creating-updating-and-destroying-kubernetes-objects">Creating, updating, and destroying Kubernetes objects</h3>

<p>Objects in Kubernetes are represented as JSON or YAML files. You can use these YAML or JSON files to create, update or delete object on the Kubernetes server.</p>

<p>If for example you have a simple object stored in <code class="language-plaintext highlighter-rouge">obj.yaml</code>; you can use <code class="language-plaintext highlighter-rouge">kubectl</code> to create this object by running the command <code class="language-plaintext highlighter-rouge">kubectl apply -f obj.yaml</code>. All the details for the object being created will be obtained from the file.  if you make changes to the file, you can use the same command to apply those changes. The <code class="language-plaintext highlighter-rouge">apply</code> tool will only modify objects that are different from teh current objects in the cluster. If an object already exists it will simply exit successfully without making any changes.</p>

<p>If you want to see what the <code class="language-plaintext highlighter-rouge">apply</code> command will do without making the changes you can use the <code class="language-plaintext highlighter-rouge">--dry-run</code> flag to print the objects to the terminal without sending them to the server.</p>

<p>If you want to make iterative changes instead of editing a local file you can use the command <code class="language-plaintext highlighter-rouge">kubectl edit &lt;resource-name&gt; &lt;obj-name&gt;</code> which will download the latest object state and launch an editor for you. After you save the file, it will be automatially uploaded back tot he Kubernetes cluster.</p>

<p>The <code class="language-plaintext highlighter-rouge">apply</code> command also records the history of previous configurations in an annotation. You can manipulate these records with the <code class="language-plaintext highlighter-rouge">eit-last-applied</code>, <code class="language-plaintext highlighter-rouge">set-last-applied</code> and <code class="language-plaintext highlighter-rouge">view-last-applied</code> commands.  For example <code class="language-plaintext highlighter-rouge">kubectl apply -f myobj.yaml view-last-applied</code> will show you the last state that was applied to the object. The <code class="language-plaintext highlighter-rouge">-f</code> option is to specify the file name.</p>

<p>When you want to delete an object use the command <code class="language-plaintext highlighter-rouge">kubectl delete -f obj.yaml</code>. <strong><em>Note:</em></strong> <code class="language-plaintext highlighter-rouge">kubectl</code> will not prompt for a confirmation before deleting an object.  There is however a terminating grace period of 30 seconds by default (At least for Pods need to confirm if for other objects). When a Pod is transitioned to Terminating state it stops taking request.  The grace period allows the Pod to finish any active reqeusts that it may be in the middle of processing before it is terminated. When a Pod is deleted any data in the container is deleted as well.  If you need to persist data than you need to use <code class="language-plaintext highlighter-rouge">PersistentVolumes</code></p>

<p>To create a deployment in a non declarative way you can use the command <code class="language-plaintext highlighter-rouge">kubectl create deployment &lt;deployment_name&gt; --image=&lt;image&gt; --replicas=&lt;replica_count&gt;</code> with real values that looks like <code class="language-plaintext highlighter-rouge">kubectl create deployment alpaca-prod --image=gcr.io/kuar-demo/kuard-arm64:blue --replicas=3 --port=8080</code></p>

<h2 id="kubernetes-api">Kubernetes API</h2>

<p>Everything contained in Kubernetes is represented by a RESTful resource. These objects exist at unique HTTP paths; for example, <code class="language-plaintext highlighter-rouge">https://your-k8s.com/api/v1/namespaces/default/pods/my-pod</code> leads to the representation of a Pod in the default namespace. The <code class="language-plaintext highlighter-rouge">kubectl</code> command makes HTTP requests to these URLs to access Kubernetes objects that reside at these paths.</p>

<h2 id="labeling-and-annotating-objects">Labeling and annotating objects</h2>

<p>Labels and annotations are tags for your objects. You can update the labels and annotation on any Kubernetes object using the <code class="language-plaintext highlighter-rouge">annotate</code> and <code class="language-plaintext highlighter-rouge">label</code> commands. The syntax for both is identical. To add a label use the command <code class="language-plaintext highlighter-rouge">kubectl label pods bar color=red</code>. By default <code class="language-plaintext highlighter-rouge">label</code> and <code class="language-plaintext highlighter-rouge">annotate</code> will not let you overwrite an existing label. To remove a label you would use the command <code class="language-plaintext highlighter-rouge">kubectl label pods bar color-</code></p>

<h2 id="debugging-commands">Debugging Commands</h2>

<p>To see the logs for a running container use <code class="language-plaintext highlighter-rouge">kubectl logs &lt;pod-name&gt;</code>.  If you have multiple containers in your Pod you can specify the container with the <code class="language-plaintext highlighter-rouge">-c</code> flag.  You can also use the <code class="language-plaintext highlighter-rouge">-f</code> flag to stream the logs.</p>

<p>To execute commands in a running container use the command <code class="language-plaintext highlighter-rouge">kubectl exec -it &lt;pod-name&gt; -- bash</code>. This will provide you with an interactive shell inside the running container so that you can perform more debugging.  If you don’t have bash or another terminal you can always attach to the running process using <code class="language-plaintext highlighter-rouge">kubectl attach -it &lt;pod-name&gt;</code>. This will attach to the running process and allow you to send input to the running process (assuming process is set up to read from stdin)</p>

<p>You can copy files to and from a container using the cp command: <code class="language-plaintext highlighter-rouge">kubectl cp &lt;pod-name&gt;:&lt;/path/to/remote/file&gt; &lt;/path/to/local/file&gt;</code>. Generally speaking copying files to a pod is an anti patterns, but it may be useful when putting out a fire.</p>

<p>If you want to access your Pod via the network, you can use the <code class="language-plaintext highlighter-rouge">port-forward</code> commands to forward network traffic from the local machine tot he Pod. This enabled you to securely tunnel network traffic through to containers that might not be exposed anywhere on the public network. For example the following command opens up a connection that forward traffic from the local machine on port 8080 to the remote container on port 80.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl port-forward &lt;pod-name&gt; 8080:80</code></p>

<p>You can also use the <code class="language-plaintext highlighter-rouge">port-forward</code> command with services by specifying <code class="language-plaintext highlighter-rouge">services/&lt;service-name&gt;</code> instead of <code class="language-plaintext highlighter-rouge">&lt;pod-name&gt;</code>, but note that the requests for a service will be forwarded to a single Pod, not go though the service load balancer.</p>

<p>If you are interested in how your cluster is using resources you can use <code class="language-plaintext highlighter-rouge">kubectl top nodes</code> or <code class="language-plaintext highlighter-rouge">kubectl top pods</code> to see the pods running and their resource utilization.</p>

<h2 id="tools-and-resources">Tools and resources</h2>

<ul>
  <li>
    <p>The <a href="https://kind.sigs.k8s.io">kind</a> project lets you run Kubernetes using containers as nodes so you can play around in your local environment.</p>
  </li>
  <li>
    <p>There are plugins for VSCode, IntelliJ, Eclipse etc for working with your cluster.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">kubectl</code> supports integration with your shell to enable tab completion for both commands and resources. You may need to install the <code class="language-plaintext highlighter-rouge">bash-completion</code> package to make this work.</p>
  </li>
  <li>
    <p>For log aggregators look into <code class="language-plaintext highlighter-rouge">fluentd</code> and <code class="language-plaintext highlighter-rouge">elasticsearch</code>.</p>
  </li>
</ul>

<h2 id="labels-and-annotations">Labels and Annotations</h2>

<p><em>Labels</em> are key value pairs that can be attached to Kubernetes objects such as Pods and ReplicaSets. They are useful for attaching identifying information to Kubernetes objects. Labels provide the foundation for grouping objects. <em>Annotations</em>, on the other hand, provide a storage mechanism that resembles labels. Annotations are key value pairs designed to hold non-identifying information that can be leveraged by tools and libraries.</p>

<h3 id="labels">Labels</h3>

<p>Labels are key/value pairs where both the key and value are represented by strings. Label keys can be broken down into two parts: an optional prefix and a name, separated by a slash. The prefix, if specified must be a DNS sub-domain with a 253 character limit. The key name is required and must be shorted than 63 characters. Names must also start and end with an alphanumeric character and permit the use of dashes <code class="language-plaintext highlighter-rouge">-</code>, underscores<code class="language-plaintext highlighter-rouge">_</code> and dots <code class="language-plaintext highlighter-rouge">.</code>. Label values are strings with a maximum length of 63 characters. The contents of the label values follow the same rules as for label keys. Below are some examples of label keys and values</p>

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>acme.com/app-version</td>
      <td>1.0.0</td>
    </tr>
    <tr>
      <td>appVersion</td>
      <td>1.0.0</td>
    </tr>
    <tr>
      <td>app.version</td>
      <td>1.0.0</td>
    </tr>
    <tr>
      <td>kubernetes.io/cluster-service</td>
      <td>true</td>
    </tr>
  </tbody>
</table>

<p>When domain names are use in labels and annotations they are expected to be aligned to that particular entity in some way. For example a project might define a canonical set of labels used to identify the various stages of application deployment (e.g staging, canary, production)</p>

<h4 id="applying-and-removing-labels">Applying and removing labels</h4>

<p>To apply or update a label on a running object use the command similar to <code class="language-plaintext highlighter-rouge">kubectl label deployments alpaca-prod "ver=1"</code> <strong><em>Note:</em></strong> this command only labels the deployment not the objects which the deployment creates. To change those objects you would need to change the template embedded in the deployment.</p>

<p>You can view labels on an object using the <code class="language-plaintext highlighter-rouge">kubectl describe</code> command.  You can also use the <code class="language-plaintext highlighter-rouge">--show-labels</code> option in kubectl to see the labels applied to an object. For example <code class="language-plaintext highlighter-rouge">kubectl get pods --show-labels</code></p>

<p>To remove a label use a minus at end of label command specifying a key for example <code class="language-plaintext highlighter-rouge">kubectl label deployments alpaca-prod "env-"</code>. The minus <code class="language-plaintext highlighter-rouge">-</code> at end of label name removes it.</p>

<p>You may see a label called <code class="language-plaintext highlighter-rouge">pod-template-hash</code> when viewing labels. This label is applied by the deployment so it can keep track of which Pods were generated from which template version and allows the deployment to manage updates ina clean way.</p>

<h4 id="label-selectors">Label Selectors</h4>

<p>Label selectors are used to filter Kubernetes objects based on a set of labels. Label selectors are used by both end users and by different types of objects (for example how a ReplicaSet relates to its Pods).</p>

<p>You use the <code class="language-plaintext highlighter-rouge">--selector</code> flag or <code class="language-plaintext highlighter-rouge">-l</code> for short to use label selectors in kubectl. Below are a few examples.</p>

<p>List pods with <code class="language-plaintext highlighter-rouge">ver</code> label set to 2 <code class="language-plaintext highlighter-rouge">kubectl get pods --selector="ver=2"</code></p>

<p>If you specify two selectors separated by a comma only objects that satisfy both will be returned. This is a logical AND operation: <code class="language-plaintext highlighter-rouge">kubectl get pods -l="app=badnicoot,ver-2"</code></p>

<p>You can also ask if a label is one of a set of values. <code class="language-plaintext highlighter-rouge">kubectl get pods -l="app in (alpaca,bandicoot)"</code></p>

<p>We can also check if a label is set or not. <code class="language-plaintext highlighter-rouge">kubectl get deployments -l="canary"</code>. This example we are asking for all deployments where the <code class="language-plaintext highlighter-rouge">canary</code> label is set to anything.</p>

<p>Below is a summary of the Selector operations:
| Operator | Description |
| ——– | ———– |
| key=value | key is set to value |
| key!=value | key is not set to value |
| key in (value1, value2) | key is one of value1 or value2 |
| key notin (value1, value2) | key is not one of value1 or value2 |
| key | key is set |
| !key | key is not set |</p>

<p>You can combine positive and negative selectors for example <code class="language-plaintext highlighter-rouge">kubectl get pods -l "ver=2,!canary"</code></p>

<h4 id="labels-in-kubernetes-architecture">Labels in Kubernetes architecture</h4>

<p>Kubernetes is a purposefully decoupled system. There is no hierarchy and all components operate independently. When objects need to be related to each other they are related with labels.  For example ReplicaSets which create multiple replicas of a Pod, find the Pods that they are managing via a selector. When you want to restrict network traffic in your cluster, you would use networkPolicy in conjunction with specific labels to identify Pods that should or should not be allowed to communicate with each other. Labels are the glue that holds a Kubernetes application together.</p>

<h3 id="annotations">Annotations</h3>

<p>Annotations provide a place to store additional metadata for Kubernetes object with the sole purpose of assisting tools and libraries.  They are a way for other programs driving Kubernetes via an API to store some data with an object. Annotations can be used for the tool itself, or to pass configuration information between external systems. While labels are used to identify and group objects, annotation are used to provide extra information about where an object came from, how to use it, or policy around that object.</p>

<p>When in doubt use an annotation and promote it to a label if there is need to use it as a selector.</p>

<p>Annotations are used to…</p>

<ul>
  <li>Keep track of a “reason” for the latest update to an object</li>
  <li>Communicate a specialized scheduling policy to a specialized scheduler</li>
  <li>Extend data bout the last tool to update the resource and how it was updated (detect changes and do smart merge)</li>
  <li>Attach build, release, or image information that isn’t appropriate for labels (Git hash, timestamp PR number etc)</li>
  <li>Enable the Deployment object to keep track of ReplicaSets that it is managing for rollouts</li>
  <li>Prototype alpha functionality in Kubernetes (instead of creating first class API filed, the parameters for that functionality are encoded in an annotation)</li>
</ul>

<p>Annotations are used in various places in Kubernetes with the primary use case being rolling deployments. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state.</p>

<h4 id="defining-annotations">Defining annotations</h4>

<p>Annotations keys use the same format as label keys but because they are often used to communicate information between tools the namespace part of the key is more important.  Example keys would be <code class="language-plaintext highlighter-rouge">deployment.kubernetes.io/revision</code> or <code class="language-plaintext highlighter-rouge">kubernetes.io/change-cause</code>.</p>

<p>The value component of an annotation is a free-form field and there is no validation that any format is being followed. Its not uncommon to have a JSON document encoded as a string and stored as an annotation.</p>

<p>Annotations are defined in the common <code class="language-plaintext highlighter-rouge">metadata</code> section in every Kubernetes object.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">example.com/icon-url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https://example.com/icon.png"</span>
<span class="nn">...</span>
</code></pre></div></div>

<h2 id="service-discovery">Service Discovery</h2>

<p>Kubernetes is a dynamic system where Pods are placed on nodes, and the numbers of Pods running can vary based on load. This makes it easy to run a lot of things but you also need to be able to find those running things. This class of problem is referred to as <strong>service discovery</strong>. Service discovery tools help solve the problem of finding which processes are listening at which addresses for which services.</p>

<h3 id="the-service-object">The Service object</h3>

<p>In Kubernetes service discovery starts with a <strong>Service object</strong>. A Service object is a way to create a named label selector, but it has some other functionality.</p>

<p>To imperatively create a Service object use the command similar to <code class="language-plaintext highlighter-rouge">kubectl expose deployment alpaca-prod</code> (assuming you have the alpaca-prod) deployment already running. The <code class="language-plaintext highlighter-rouge">kubectl expose</code> command will pull both the label selector and the relevant ports from the deployment definition to set up the service. It will also assign a virtual IP (called <strong>cluster IP</strong>) to the service. This is a special IP address which Kubernetes will use to load balance across all the Pods that are identified by the selector.  This process of having Pods that match a selector get load balanced in the cluster IP is the service discovery mechanism (just my take away need to confirm)</p>

<p>You can get a list of services with the command <code class="language-plaintext highlighter-rouge">kubectl get services</code></p>

<h3 id="service-dns">Service DNS</h3>

<p>Because the cluster IP is virtual, it is stable and it is appropriate to give it a DNS address. Kubernetes provides a DNS service exposed to Pods running in the cluster and provides DNS names to cluster IPs.</p>

<p>An example of the DNS name and how it breaks down is <code class="language-plaintext highlighter-rouge">alpaca-prod.default.svc.cluster.local</code> where…</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">alpaca-prod</code> is the name of the service in questions</li>
  <li><code class="language-plaintext highlighter-rouge">default</code> is the namespace that the service is in</li>
  <li><code class="language-plaintext highlighter-rouge">svc</code> is for recognizing that this is a service</li>
  <li><code class="language-plaintext highlighter-rouge">cluster.local</code> is the base domain name for the cluster</li>
</ul>

<p>When referring to a service in your won namespace you can just use the service name.  You can also refer to a service in another namespace by having the namespace in the specified name (<code class="language-plaintext highlighter-rouge">alpaca-prod.default</code> for example) or you can use the FQDN.</p>

<h3 id="readiness-checks">Readiness Checks</h3>

<p>One nice things the Service object does is track if a Pod is ready to accept requests or not. This is useful if your applications needs some time to initialize when it starts up. This is the readiness check functionality.  You can specify the readiness check in your manifest file with code similar to</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">speck</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="s">template</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
      <span class="s">...</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">alpaca-prod</span>
      <span class="na">readinessProbe</span><span class="pi">:</span>
        <span class="na">httpGet</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/ready</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">2</span>
        <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">0</span>
        <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">3</span>
        <span class="na">successThreshold</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p>In the example above the readiness check will look for successful GET request to <code class="language-plaintext highlighter-rouge">/ready</code> endpoint on port 8080. It will check every 2 seconds starting as soon as the Pod comes up. If three successive checks fail then the Pod will be considered not ready. If one checks succeeds the Pod will again be considered ready.  Only ready pods are sent traffic.  This functionality is not just useful at startup its a also a good way for an overloaded or sick server to signal to the system that it doesn’t want to receive traffic anymore, and is a good way to implement a graceful shutdown. Server can signal it no longer wants to receive traffic complete all jobs or connections and shut down.</p>

<h3 id="exposing-a-service-outside-a-cluster">Exposing a service outside a cluster</h3>

<p>You can use the <code class="language-plaintext highlighter-rouge">NodePort</code> functionality to have your service be accessible from any cluster node. Essentially if you specify <code class="language-plaintext highlighter-rouge">spec.type</code> as <code class="language-plaintext highlighter-rouge">NodePort</code> or use <code class="language-plaintext highlighter-rouge">--type=NodePort</code> when calling expose command Kubernetes will assign a port to your service that can be used from any cluster node to access the service.</p>

<p>The Service object operates at Layer 4 of the OSI model which means that it only forward TCP and UDP connections and doesn’t look inside of those connections.</p>

<h4 id="cloud-integration">Cloud Integration</h4>

<p>If your cloud supports it, you can use the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> type which builds on the <code class="language-plaintext highlighter-rouge">NodePort</code> concept. Essential the cloud provider will create a load balancer and direct it at nodes in your cluster. This functionality is cloud provider specific, but is a way to get your application exposed/usable to the world.</p>

<h3 id="endpoints">Endpoints</h3>

<p>Some applications (and the system itself) want to be able to use services without a cluster IP. This can be done with the <strong>Endpoints</strong> object. For every Service object Kubernetes creates a buddy Endpoints object that contains the IP address of that service.  You an view the Endpoints object with the command <code class="language-plaintext highlighter-rouge">kubectl describe endpoints alpaca-prod</code>. To use a service an application that is aware of Endpoints (likely an application written tow ork with Kubernetes) can talk to Kubernetes API directly to look up endpoints and call them. The Kubernetes API even has the capability to “watch” objects and be notified as soon as they change. This allows the client to react immediately as soon as the IPs associated with a service change. Most applications don’t use this and just use stable IP addresses that don’t change often, but the alternative is there.</p>

<h3 id="kube-proxy-and-cluster-ips">kube-proxy and cluster IPs</h3>

<p>Cluster IPs are stable virtual IPs that load-balance traffic across all of the endpoints in a service. This is performed by a component running on every node in the cluster called <code class="language-plaintext highlighter-rouge">kube-proxy</code>. <code class="language-plaintext highlighter-rouge">kube-proxy</code> watches for new services in the cluster via the API server and then programs a set of <code class="language-plaintext highlighter-rouge">iptables</code> rules in the kernel of that host to rewrite the destinations of packets so they care directed at one of the endpoints for that service. If the set of endpoints for a service changes (due to Pods coming and going ro due to failed readiness checks) the set of <code class="language-plaintext highlighter-rouge">iptables</code> rules is rewritten.</p>

<p>The cluster IP itself is usually assigned by the API server as the service is created, however when creating the service the user can specify a specific cluster IP.  Once set the cluster IP cannot be modified without deleting and recreating the Service object.</p>

<h3 id="connecting-with-other-environments">Connecting with other environments</h3>

<p>When you are connecting Kubernetes to legacy resources outside of the cluster, you can use selector-less services to declare a Kubernetes ser‐ vice with a manually assigned IP address that is outside of the cluster. That way, Kubernetes service discovery via DNS works as expected, but the network traffic itself flows to an external resource.</p>

<p>Connecting external resources to Kubernetes services is somewhat trickier. If your cloud provider supports it, the easiest thing to do is to create an “internal” load balancer that lives in your virtual private network and can deliver traffic from a fixed IP address into the cluster. You can then use traditional DNS to make this IP address available to the external resource. Another option is to run the full kube-proxy on an external resource and program that machine to use the DNS server in the Kubernetes cluster. Such a setup is significantly more difficult to get right and should really only be used in on-premise environments. There are also a variety of open source projects (for example, Hashicorp’s Consul) that can be used to manage connectivity between in-cluster and out-of-cluster resources.</p>

<h2 id="http-load-balancing-with-ingress">HTTP Load Balancing with Ingress</h2>

<p>There are some challenges with using <code class="language-plaintext highlighter-rouge">NodePort</code> based Service object to expose your service outside of the cluster. For one since <code class="language-plaintext highlighter-rouge">NodePort</code> Service objects operate at Layer 4, you will need to have clients connecting on to a unique port per service.  If you use a <code class="language-plaintext highlighter-rouge">LoadBalander</code> based Service object you will need to allocate an expensive cloud based load balancer outside your cluster for each service.</p>

<p>For Layer 7 (HTTP) applications we can do better. Outside of Kubernetes this problem is typically solved with a reverse proxy which decodes the incoming requests and sends them to the right upstream service. In Kubernetes this functionality is called <strong>Ingress</strong>.  Ingress is a Kubernetes native way to implement the virtual hosting pattern described above. This virtual hosting pattern requires the administrator to manage the load balancer configuration file. In a dynamic environment as the set of virtual hosts expands, this can be very complex. Ingress simplifies this by standardizing that configuration, moving it to a standard Kubernetes object, and merging multiple Ingress objects into a single config for the load balancer.</p>

<p>The ingress controller is a software system exposed outside the cluster using a service type: <code class="language-plaintext highlighter-rouge">LoadBalancer</code>. It then proxies requests to “upstream” servers.  The configuration of how it does this is the result of reading and monitoring Ingress objects.</p>

<h3 id="ingress-spec-versus-ingress-controllers">Ingress spec versus Ingress controllers</h3>

<p>The implementation of Ingress is very different from other regular resource objects in Kubernetes. Ingress is split into a common resource specification and a controller implementation. There is not “standard” Ingress controller that is built into Kubernetes, and the user must install one of the many optional implementations.</p>

<p>Users can create and modify Ingress objects just like every other object; but by default, there is not code running to actually oct on those objects. It is up to the users (or the distribution they are using) to install and manage an outside controller. In this way the, the controller is pluggable.</p>

<p>There are multiple reasons for this pluggable approach in Ingress. First there is no single HTTP load balancer that can universally be used. In addition to many software load balancers (both open source and proprietary), there are also load balancing capabilities provided by cloud providers such as ELB on AWS, and hardware based load balancers. The second reason is that Ingress was added to Kubernetes before the standard extensibility options. In the future Ingress may change to follow those new extensibility options.</p>

<h3 id="configuring-dns">Configuring DNS</h3>

<p>To make Ingress work well, you need to configure DNS entries to the external address of your load balancer. You can map multiple host names to a single external endpoint and the Ingress controller will direct incoming requests to the appropriate upstream service based on that hostname.</p>

<h3 id="using-hostname">Using hostname</h3>

<p>The most common use of directing traffic based on properties of a request is using hostnames. The yaml file below defines an Ingress object that will route requests to <code class="language-plaintext highlighter-rouge">alpaca.example.com</code> to the alpaca service.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">host-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">alpaca.example.com</span>
  <span class="na">http</span><span class="pi">:</span> <span class="na">paths</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">backend</span><span class="pi">:</span>
    <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
    <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<h4 id="default-http-backends">Default http backends</h4>

<p>Certain Ingress controllers use a concept of a <code class="language-plaintext highlighter-rouge">default-http-backend</code>. This is used if a request comes in that does not have any http backend defined.  The default backend is uses as the default.</p>

<h3 id="paths">Paths</h3>

<p>You can use Ingress to direct traffic based on not just he hostname, but also the path in the HTTP request.  We can do this by specifyng a path in the <code class="language-plaintext highlighter-rouge">paths</code> entry. In the example below, we are directing everything coming into <code class="language-plaintext highlighter-rouge">http://bandicoot.example.com</code> to the bandicoot service, but we send <code class="language-plaintext highlighter-rouge">http://bandicoot.example.com/a</code> to the alpaca service. This type of scenario can be used to host multiple services on different paths of a single domain.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">path-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">bandicoot.example.com</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">bandicoot</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/a/"</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<p>When there are multiple paths on the same host listed in the Ingress system, the longest prefix matches. In the example above, traffic staring with /a/ is forwarded to the alpaca service, while all other traffic (starting with /) is directed to the bandicoot service.  As requests get proxied to the upstream service, the path remains unmodified. That means a request to <code class="language-plaintext highlighter-rouge">bandicoot.example.com/a/</code> shows up to the upstream server that is configured for that request hostname path.</p>

<h3 id="advanced-ingress-topics-and-gotchas">Advanced Ingress topics and gotchas</h3>

<p>Many of Ingress’ extended features are exposed via annotations on the Ingress object. Be careful, as these annotations can be hard to validate and are easy to get wrong. Many of these annotations apply to the entire Ingress object and so can be more general than you might like. To scope the annotations down you can always split a single Ingress object into multiple Ingress objects. The ingress controller should read them and merge them together.</p>

<h4 id="running-multiple-ingress-controllers">Running multiple Ingress Controllers</h4>

<p>If you are running multiple Ingress controllers on a single cluster you can control which Ingress object is meant for which ingress controller using the <code class="language-plaintext highlighter-rouge">kubernetes.io/ingress.class</code> annotation. The value should be a string that specifies which Ingress controller should look at this object. The ingress controllers themselves, then, should be configured wth that same string and should only respect those Ingress objects with the correct annotation. If <code class="language-plaintext highlighter-rouge">kubernetes.io/ingress.class</code> annotation is missing, behavior is undefined. Its likely multiple controllers will fight to satisfy the Ingress and write the status field of the Ingress objects.</p>

<h4 id="multiple-ingress-objects">Multiple Ingress Objects</h4>

<p>If you specify multiple Ingress objects, the Ingress controllers should read them all and try to merge them into a coherent configuration. If duplicate or conflicting configurations are specified, the behavior is undefined.</p>

<h4 id="ingress-and-namespaces">Ingress and namespaces</h4>

<p>For security an Ingress object can only refer to an upstream service in the same namespace. This means that you can’t use an Ingress object to point a subpath to a service in another namespace. On the other hand multiple multiple Ingress objects in the different namespaces can specify sub-paths for the same host. These Ingress objects are then merged together to come up with the final config for the Ingress controller.  This cross-namespace behavior means that it is necessary that Ingress be coordinated globally across the cluster; otherwise an Ingress object in one namespace can cause issues for another namespace. Advanced users may try to force controls on this using admission controller, but there are no restrictions out of the box.</p>

<h4 id="path-rewriting">Path rewriting</h4>

<p>Some ingress controller implementations support, optionally, doing path rewriting. This can modify the path in the HTTP request as it gets proxied. This is typically specified with an annotation ont he Ingress controller (check the documentation for controller you are using). Be careful with path rewriting as if your web applications tries to link within itself using absolute paths the rewrite may cause issues.</p>

<h4 id="serving-tls">Serving TLS</h4>

<p>To serve TLS via Ingress you will need to specify the certificate and key.  You can create the secret using <code class="language-plaintext highlighter-rouge">kubectl create secret tls &lt;secret-name&gt; --cert &lt;certificate-pem-file&gt; --key &lt;private-key-pem-file&gt;</code>. Once you have the certificate uploaded, you can reference it in an Ingress object. If multiple Ingress objects specify certificates for the same hostname, the behavior is undefined.</p>

<p>Check out <a href="https://github.com/cert-manager/cert-manager">cert-mager</a> for setting up an API driven local certificate authority.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tls-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">tls</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">alpaca.example.com</span>
      <span class="na">secretName</span><span class="pi">:</span> <span class="s">tls-secret-name</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">alpaca.example.com</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">backend</span><span class="pi">:</span>
      <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
      <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<h4 id="alternate-ingress-implementations">Alternate Ingress Implementations</h4>

<p>Cloud providers have their own L7 based Ingress implementation that exposes the specific cloud based load balancer.  Instead of configuring the lod balancer to run in a Pod, these controllers take Ingress objects and use them to configure the cloud based load balancer via an API. This reduces load on the cluster, but you need to pay for the load balancer resources used.</p>

<p>The most popular generic Ingress controller is probably the open source <a href="https://github.com/kubernetes/ingress-nginx/">NGINX Ingress controller</a>. There is also a commercial version based on this project.   <a href="https://github.com/emissary-ingress/emissary">Embassador</a> and <a href="https://github.com/solo-io/gloo">Gloo</a> are options you should look at if you are looking to build an API gateway.  <a href="https://traefik.io">Traefik</a> is a reverse proxy that can also function as an Ingress controller. It has a set of features and dashboards that are very developer-friendly.</p>

<h2 id="replicasets">ReplicaSets</h2>

<p>In real world use cases you rarely run a single Pod. Typically multiple pods are run for redundancy, scale, sharding or other reasons. A replicated set of Pods should be a single entity (to prevent from repetitive work). The <strong>ReplicaSet</strong> object in Kubernetes acts as a cluster wide Pod manager, ensuring that the right types and numbers of Pods are running at all times. ReplicaSets not Pods tend to be the building blocks of applications in Kubernetes. ReplicaSets provide self-healing for our applications at the infrastructure level. Pods managed by ReplicaSets are automatically rescheduled under certain failure conditions such as node failures and network partitions.</p>

<h3 id="adopting-exiting-containers">Adopting exiting containers</h3>

<p>Though ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. They then use the general Kubernetes APIs to create the Pods they are managing. This decoupling of Pods and ReplicaSets supports several important behaviors.  Because ReplicaSets are decoupled from the Pods they manage, you can create a ReplicaSet that will “adopt” an existing Pod and scale out additional copies of those containers. This is a path to go from a single imperatively declared Pod to a replicated set of Pods managed by ReplicaSet.</p>

<h3 id="quarantining-containers">Quarantining Containers</h3>

<p>When a server misbehaves, Pod-level health checks will automatically restart the Pod. If your health checks are incomplete, a Pod can be misbehaving, but still be a part of the replicated set. In this scenario you can kill the Pod, but then developers would only have logs to troubleshoot with.  A better option is to leave the Pod running but quarantine it so that it is not sent traffic. You can do this modifying the set of labels on the Pod. This will disassociate it from the ReplicaSet, but leave it running for developers to troubleshoot.</p>

<h3 id="designing-with-replicasets">Designing with ReplicaSets</h3>

<p>ReplicaSets are designed to represent a single, scalable microservice inside your architecture. The key characteristic of ReplicaSets is that every Pod that is created by the ReplicaSet controller is entirely homogenous. Typically, these Pods are then fronted by a Kubernetes service load balancer, which spreads traffic across the Pods that make up the service. Generally speaking ReplicaSets are designed for stateless (or nearly stateless) services. When it is scaled down an arbitrary Pod wil be selected for deletions and this should not impact your application.</p>

<h3 id="replicaset-spec">ReplicaSet spec</h3>

<p>All ReplicaSets must have a unique name (defined using the <code class="language-plaintext highlighter-rouge">metadata.name</code> field), a <code class="language-plaintext highlighter-rouge">spec</code> section that describe the number of Pods (replicas) that should be running in the cluster, and a Pod template that describes the Pod to be created.</p>

<p>Below is an example of a minimal ReplicaSet</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">kuard</span>
        <span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2"</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">gcr.io/kuar-demo/kuard-amd64:green"</span>
</code></pre></div></div>

<h3 id="inspecting-a-replicaset">Inspecting a ReplicaSet</h3>

<p>You can use the command <code class="language-plaintext highlighter-rouge">kubectl describe rs &lt;ReplicaSet name&gt;</code> to get details about a running ReplicaSet</p>

<h3 id="finding-a-replicaset-from-a-pod">Finding a ReplicaSet from a Pod</h3>

<p>The ReplicaSet controller adds an annotation to every Pod that it creates. The key for the annotation is <code class="language-plaintext highlighter-rouge">kubernetes.io/created-by</code> If you run the command <code class="language-plaintext highlighter-rouge">kubectl get pods &lt;pod-name&gt; -o yaml</code> it will provide the ReplicaSet that is managing the Pod. Note that these annotations are best effort and while added when the Pod is created could be removed by a Kubernetes user at any time.</p>

<h3 id="finding-a-set-of-pods-for-a-replicaset">Finding a set of Pods for a ReplicaSet</h3>

<p>You can determine the Pods managed by a ReplicaSet by using a label query. You can use <code class="language-plaintext highlighter-rouge">kubectl describe</code> to find the set of labels from the ReplicaSet. Then do a label query such as <code class="language-plaintext highlighter-rouge">kubectl get pods -l app=kuard,version=2</code> to find the Pods. This is the same query that ReplicaSet executes to determine the current numbers of Pods.</p>

<h3 id="scaling-replicasets">Scaling ReplicaSets</h3>

<p>Replica sets are scaled up or down by updating the <code class="language-plaintext highlighter-rouge">spec.replicas</code> key on the ReplicaSet object stored in Kubernetes. When a ReplicaSet is scaled up, new Pods are submitted to the Kubernetes API using the Pod template defined on the ReplicaSet.</p>

<h4 id="imperative-scaling-replicasets-with-kubectl">Imperative scaling ReplicaSets with kubectl</h4>

<p><code class="language-plaintext highlighter-rouge">kubectl scale replicasets &lt;ReplicaSet name&gt; --replicas&lt;numer&gt;</code> for example <code class="language-plaintext highlighter-rouge">kubectl scale replicasets kuard --replicas=4</code></p>

<p>This works, and can be useful for demos or troubleshooting, but in typical operations declarative approach should be used.</p>

<h4 id="declaratively-scaling-replicasets-with-kubectl-apply">Declaratively scaling ReplicaSets with kubectl apply</h4>

<p>The declarative approach is to make changes in a version controlled file and then applying those changes to our cluster with the command <code class="language-plaintext highlighter-rouge">kubectl apply -f &lt;manifest_file.yaml&gt;</code> for example <code class="language-plaintext highlighter-rouge">kubectl apply -f kuard-rs.yaml</code></p>

<h4 id="autoscaling-a-replicaset">Autoscaling a ReplicaSet</h4>

<p>Kubernetes supports scaling ReplicaSets using <strong>HPA (Horizontal Pod Autoscaling)</strong>. HPA requires the presence of a <code class="language-plaintext highlighter-rouge">heapster</code> Pod on your cluster which keeps track of metrics and provides an API for consuming those metrics that HPA uses to make decisions. Most Kubernetes installations include <code class="language-plaintext highlighter-rouge">heapster</code> by default.</p>

<h5 id="side-not-on-scaling">Side not on scaling</h5>

<p>Kubernetes does not currently support vertical scaling (adding CPU for example) but this is planned. Many solution offer cluster auto scaling where extra nodes are added as needed.</p>

<h4 id="autoscaling-based-on-cpu">Autoscaling based on CPU</h4>

<p>Scaling based on CPU is the most common  use case for Pod autoscaling. Its most useful for request-based systems that consume CPU proportionally to the number of requests they are receiving while using a relatively static amount of memory.</p>

<p>Use the command <code class="language-plaintext highlighter-rouge">kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80</code></p>

<p>To view, modify or delete this resource you can use the standard kubectl commands and the <code class="language-plaintext highlighter-rouge">horizontalpodautoscalers</code> resource (you can shorten that to <code class="language-plaintext highlighter-rouge">hpa</code>) for example <code class="language-plaintext highlighter-rouge">kubectl get hpa</code>.</p>

<p>Because of the decoupled nature of Kubernetes, there is no direct link between HPA and the ReplicaSet. This leads to some anti patterns. Its a bad idea to combine both autoscaling and imperative or declarative management of the number of replicas. If both you and an autoscaler are attempting to modify the number of replicas, it’s highly likely that you will clash, resulting in unexpected behavior.</p>

<h4 id="deleting-replicasets">Deleting ReplicaSets</h4>

<p>When a ReplicaSet is no longer required, it can be deleted using the <code class="language-plaintext highlighter-rouge">kubectl delete</code> command. For examples <code class="language-plaintext highlighter-rouge">kubectl delete rs kuard</code>. By default this also deletes the Pods that are managed by the ReplicaSet.  If you don’t want to delete the Pods use the command similar to <code class="language-plaintext highlighter-rouge">kubectl delete rs kuard --cascade=false</code></p>

<h2 id="deployments">Deployments</h2>

<p>The Deployment object exists to manage the release of new versions. Deployments enable you to easily move from one version of your code to the next.</p>

<p>Like with all Kubernetes objects, a deployment can be represented as a declarative YAML object that provides the details about what you want to run. Below is a manifest for a single instance of the <code class="language-plaintext highlighter-rouge">kuard</code> application.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">run</span><span class="pi">:</span> <span class="s">kuard</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">template</span><span class="pi">:</span>
      <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">run</span><span class="pi">:</span> <span class="s">kuard</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
</code></pre></div></div>

<p>To run the deployment specified by the above script you would run the command <code class="language-plaintext highlighter-rouge">kubectl create -f kuard-deployment.yaml</code></p>

<h3 id="deployment-internals">Deployment internals</h3>

<p>Similar to how ReplicaSets manage Pods; deployments manage ReplicaSets. As with all relationships in Kubernetes the deployment to ReplicaSets relationship is defined by labels and a label selector. You can see the label selector by looking at the Deployment object with the below command.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get deployments kuard -o jsonpath --template {.spec.selector.matchLabels}</code></p>

<p>You can imperatively scale a deployment using the command <code class="language-plaintext highlighter-rouge">kubectl scale deployments kuard --replicas=2</code>.  <strong><em>Note:</em></strong> If you want to take any actions on the ReplicaSets managed by a deployment you need to delete the deployment. Otherwise the self healing nature of Kubernetes will keep taking actions to align the ReplicaSets with the stored specification for the deployment.</p>

<h3 id="creating-deployments">Creating deployments</h3>

<p>As a starting point you can generate the YAML manifest for an imperatively declared deployment to create a declarative onw.</p>

<p>To get the deployment as a YAML file use the following command…</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get deployments kuard --export -o yaml &gt; kuard-deployment.yaml</code> followed by <code class="language-plaintext highlighter-rouge">kubectl replace -f kuard-deployment.yaml --save-config</code></p>

<p>The reason you need to run the second command (with –save-config) is that it adds an annotation so that when applying changes in the future, kubectl will know what the last applied configuration was for smarter merging of conflicts.  If you always use <code class="language-plaintext highlighter-rouge">kubectl apply</code>, this step is only required after the fist time you create a deployment using <code class="language-plaintext highlighter-rouge">kubectl create -f</code>.</p>

<p>The deployment spec has a very similar structure to the ReplicaSet sepc. There is a Pod template which contains a number of containers that are created for each replica managed by the deployment. In addition to the Pod specification, there is also a <code class="language-plaintext highlighter-rouge">strategy</code> object.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="na">strategy</span><span class="pi">:</span>
  <span class="na">rollingUpdate</span><span class="pi">:</span>
    <span class="na">maxSurge</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">maxUnavailable</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">RollingUpdate</span>
<span class="nn">...</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">strategy</code> object dictates the different ways in which a rollout of the new software can proceed. There are two strategies supported: <code class="language-plaintext highlighter-rouge">Recreate</code> and <code class="language-plaintext highlighter-rouge">RollingUpdate</code> (discussed later in book)</p>

<h3 id="managing-deployments">Managing deployments</h3>

<p>To get information about a deployment use the command similar to <code class="language-plaintext highlighter-rouge">kubectl describe deployments kuard</code></p>

<p>From the output of the above command, two of the most important pieces of information are <code class="language-plaintext highlighter-rouge">OldReplicaSets</code> and <code class="language-plaintext highlighter-rouge">NewReplicaSets</code>. These fields point to the ReplicaSet object this deployment is currently managing. if a deployment is in the middle of a rollout, both fields will be set to a value. If a rollout is complete, <code class="language-plaintext highlighter-rouge">OldReplicaSets</code> will be set to <code class="language-plaintext highlighter-rouge">&lt;none&gt;</code>.</p>

<p>You can also use the command <code class="language-plaintext highlighter-rouge">kubectl rollout history</code> to obtain the history of rollouts associated with a particular deployment. If you have a current deployment in progress you can use <code class="language-plaintext highlighter-rouge">kubectl rollout status</code> to obtain the current status of a rollout.</p>

<h3 id="updating-deployments">Updating deployments</h3>

<h4 id="scaling-a-deployment">Scaling a deployment</h4>

<p>To scale up a deployment, you would edit your YAML file to increase the number of replicas. Once you have saved and committed the change, you can use the <code class="language-plaintext highlighter-rouge">kubectl apply</code> command to put it into effect.</p>

<h4 id="updating-a-container-image">Updating a container image</h4>

<p>Updating a container is also an update to a YAML file and a <code class="language-plaintext highlighter-rouge">kubectl apply</code> command call. It is a good idea to add an annotation to the deployment to record some information about the update.  Make sure to add this annotation to the template and not the deployment in the YAML file as <code class="language-plaintext highlighter-rouge">kubectl apply</code> command uses the field in the Deployment object. Also do not update the <code class="language-plaintext highlighter-rouge">change-cause</code> annotation when scaling as a change to that annotation will trigger a new rollout.</p>

<p>Once you make the change and call <code class="language-plaintext highlighter-rouge">kubectl apply</code> you can monitor the rollout via the <code class="language-plaintext highlighter-rouge">kubectl rollout</code> command.</p>

<p>If you are in the middle of a rollout and you want to temporarily pause the rollout (if you are seeing issues and want to investigate for example) you can use the command similar to <code class="language-plaintext highlighter-rouge">kubectl rollout pause deployments kuard</code>. To resume the rollout use <code class="language-plaintext highlighter-rouge">kubectl rollout resume deployments kuard</code>.</p>

<h4 id="rollout-history">Rollout history</h4>

<p>You can see the deployment history by running the command <code class="language-plaintext highlighter-rouge">kubectl rollout histoyr deployment kuard</code> If you are interested in more details about a particular revision, you can add the <code class="language-plaintext highlighter-rouge">--revision</code> flag as in <code class="language-plaintext highlighter-rouge">kubectl rollout history deployment kuard --revision=2</code></p>

<p>If you need to undo a rollout to the prior version you can use the command <code class="language-plaintext highlighter-rouge">kubectl rollout undo deployment kuard</code>.  The undo command works regardless of the state of the rollout. You can undo both partially completed and fully completed rollouts. <strong><em>Note</em></strong>: This is the imperative approach. The preferred declarative way is to update the YAML file to the version you are rolling back to and run that deployment.</p>

<p>By default the revision history of a deployment is kept attached to the Deployment object. If you have deployment that you plan to keep for a long time and you make frequent updates you should limit the revision history. You can do this by setting the <code class="language-plaintext highlighter-rouge">revisionHistoryLimit</code> property in the deployment specification.</p>

<h3 id="deployment-strategies">Deployment strategies</h3>

<p>As mentioned prior, Kubernetes supports two different rollout strategies, <code class="language-plaintext highlighter-rouge">Recreate</code> and <code class="language-plaintext highlighter-rouge">RollingUpdate</code></p>

<h4 id="recreate-strategy">Recreate Strategy</h4>

<p>The recreate strategy simply updates the ReplicaSet it manages to us the new image and terminates all of the Pods associated with the deployment. The ReplicaSet notices that it no longer has any replicas, and re-creates all Pods using the new image. Once the pods are re-created, they are running the new version.  This strategy will almost certainly result in some downtime. Its typically uses for test deployments.</p>

<h4 id="rollingupdate-strategy">RollingUpdate strategy</h4>

<p>RollingUpdate can be used to roll out a new version of your service while it is still receiving user traffic, without any downtime. It works by updating a few Pods at a time until all of the Pods are running the new version of your software.</p>

<p>This means that for a period of time, both the new and the old version of your service will be receiving and serving traffic. The software you are deploying needs to support this state.</p>

<h5 id="configuring-a-rolling-update">Configuring a rolling update</h5>

<p>There are two parameters you can use to tune the rolling update behavior; <code class="language-plaintext highlighter-rouge">maxUnavailable</code> and <code class="language-plaintext highlighter-rouge">maxSurge</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">maxUnavailable</code> parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (3 for example meaning maximum of 3 Pods can be unavailable) or as a percentage (20% meaning a maximum of 20% of the desired number of replicas can be unavailable).  Generally you would use percentage, but setting hard value of 1 lets you deploy one Pod at a time.</p>

<p>Using reduced capacity is one options of doing a deployment, but it may not always be a valid option (if you do not have dips in traffic for example).  The other option is to create extra instances and take out the old ones which is done with the <code class="language-plaintext highlighter-rouge">maxSurge</code> parameter. The <code class="language-plaintext highlighter-rouge">maxSurge</code> parameter controls how many extra resources can be created to achieve a rollout.  For example you can set <code class="language-plaintext highlighter-rouge">maxUnavailable</code> to 0 and <code class="language-plaintext highlighter-rouge">maxSurge</code> to 20%. The rolling update will scale up by 20% first then start removing the old instances.</p>

<p>Setting <code class="language-plaintext highlighter-rouge">maxSurge</code> to 100% is equivalent to a blue/green deployment. The deployment controller first scales the new version to 100% of the old version. Once the new version is healthy, it immediately scales the old version down to 0%</p>

<h5 id="slowing-rollouts-to-ensure-service-health">Slowing rollouts to ensure service health</h5>

<p>The deployment controller examines the Pod’s status with readiness checks. If you want to use deployments to reliably roll out your service you must implement health checks for the containers in your Pod. Without these checks deployment controller is running blind.</p>

<p>Just because a pod comes up healthy does not mean it does not have any issues. Memory leaks or low frequency bugs can take some time to be noticed. In most real world scenarios you want to wait some time before moving on to the next Pod. For deployments this time to wait can be defined by the <code class="language-plaintext highlighter-rouge">minReadySeconds</code> parameter.  Setting <code class="language-plaintext highlighter-rouge">minReadySeconds</code> to 60 means a deployment has to wait for 60 seconds after a Pod comes up healthy until it moves on to the next one.</p>

<p>You also want to set a timeout for the rollout should you have a scenario where Pods are just not coming up healthy. If you are using automated system to deploy a timout is a good place to fire off a ticket and roll back the deployment. To set a timeout use the <code class="language-plaintext highlighter-rouge">progressDeadlineSeconds</code> parameter. Its important to note that his setting is for deployment progress not the overall length of a deployment. Every time the deployment creates or deletes a Pod the clock is reset for the timeout.</p>

<h4 id="deleting-a-deployment">Deleting a Deployment</h4>

<p>To imperatively delete a deployment ue the command similar to <code class="language-plaintext highlighter-rouge">kubectl delete deployment kuard</code>.  To declaratively delete a deployment <code class="language-plaintext highlighter-rouge">kubectl delete -f kuard-deployment.yaml</code>.</p>

<p>Deleting a deployment deletes the entire service including the ReplicaSets and Pods. If you want to leave those intact you can use the <code class="language-plaintext highlighter-rouge">--cascade=false</code> option to only delete the Deployment object.</p>

<h4 id="monitoring-a-deployment">Monitoring a deployment</h4>

<p>The status of a deployment can be obtained from the <code class="language-plaintext highlighter-rouge">status.conditions.array</code> where there will be a <code class="language-plaintext highlighter-rouge">Condition</code> whose <code class="language-plaintext highlighter-rouge">Type</code> is <code class="language-plaintext highlighter-rouge">Progressing</code> and whose <code class="language-plaintext highlighter-rouge">Status</code> is <code class="language-plaintext highlighter-rouge">False</code>. A deployment in such a state has failed and will not progress further.  How long you should wait for this state is controlled by the <code class="language-plaintext highlighter-rouge">spec.progressDeadlineSeconds</code> property.</p>

<h2 id="daemonsets">DaemonSets</h2>

<p>DaemonSets are a construct that is used to make sure that a Pod is running across a set of nodes in a Kubernetes cluster. DeamonSets are used to deploy daemons such as log collectors and monitoring agents, which typically need to run on every node. You can use labels to run DaemonSet Pods on specific nodes; for example you may want to run special intrusion detection software on nodes that are exposed tot he edge network. You can also use DaemonSets to install software on nodes in a cloud-based cluster. If you haver a need to have specific software on every node of your cluster DaemonSets is the way to accomplish this.  You can even mount the host filesystem and run scripts that install RPM/DEB packages onto the host operating system.  DaemonSets are particularly useful on auto scaled Kubernetes clusters.</p>

<h3 id="daemonset-scheduler">DaemonSet Scheduler</h3>

<p>By default, a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of labels. DaemonSets determine which node a Pod will run on at Pod creating time by specifying the <code class="language-plaintext highlighter-rouge">nodeName</code> field in the Pod spec. As a result, Pods created by daemonSets are ignored by the Kubernetes scheduler and are driven by the DaemonSet controller. If a new node is added tot he cluster, the DaemonSet controller will notice that it is missing a Pod and will add the Pod to the new node.</p>

<h3 id="creating-daemonsets">Creating DaemonSets</h3>

<p>Below is an example of creating a <code class="language-plaintext highlighter-rouge">fluentd</code> logging agent on every node in the cluster.</p>

<p>DaemonSets require a unique name across all DaemonSets in a given Kubernetes namespace. Each DaemonSet must include a Pod template spec, which will be used to create Pods as needed.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">DaemonSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">fluentd</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">fluentd</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">fluentd</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">fluent/fluentd:v0.14.10</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">200Mi</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s">100m</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">200Mi</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlog</span>
          <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/log</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlibdockercontainers</span>
          <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/lib/docker/containers</span>
          <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlog</span>
        <span class="na">hostPath</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/var/log</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">varlibdockercontainers</span>
        <span class="na">hostPath</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/var/lib/docker/containers</span>
</code></pre></div></div>

<h3 id="limiting-daemonsets-to-specific-nodes">Limiting DaemonSets to specific nodes</h3>

<p>The first step in limiting DaemonSets to specific nodes is to add the desired set of labels to a subset of nodes. This can be achieved using the <code class="language-plaintext highlighter-rouge">kubectl label</code> command. Fore example the command below adds the <code class="language-plaintext highlighter-rouge">ssd=true</code> label to a single node:</p>

<p><code class="language-plaintext highlighter-rouge">kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true</code></p>

<p>Using a label selector, we can filter nodes based on labels. The command below lists nodes which have the <code class="language-plaintext highlighter-rouge">ssd</code> label set to <code class="language-plaintext highlighter-rouge">true</code>.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get nodes --selector ssd=true</code></p>

<h3 id="node-selectors">Node selectors</h3>

<p>Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The DaemonSet configuration in the example below limits NGINX to running only on nodes with the <code class="language-plaintext highlighter-rouge">ssd=true</code> label set.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s2">"</span><span class="s">DaemonSet"</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">ssd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-fast-storage</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">ssd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">ssd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.10.0</span>
</code></pre></div></div>

<p><strong><em>Note:</em></strong> Removing labels from a node that are required by a DaemonSet’s node selector will cause the Pod being managed by that DaemonSet to be removed from the node.</p>

<h3 id="updating-a-daemonset">Updating a DaemonSet</h3>

<p>Prior to Kubernetes 1.6 the only way to update Pods managed by a DaemonSet was to update the DaemonSet and then manually delete each Pod that was managed by the DaemonSet so that it would be recreated with the new configuration. With version 1.6, DaemonSets gained an equivalent to the Deployment object that manages a DaemonSet rollout.</p>

<h4 id="rolling-update-of-a-damoneset">Rolling update of a DamoneSet</h4>

<p>Daemon sets can be rolled out using the same <code class="language-plaintext highlighter-rouge">RollingUpdate</code> strategy that deployments use. You can configure the update strategy using the <code class="language-plaintext highlighter-rouge">spec.updateStategy.type</code> field, which should have the value <code class="language-plaintext highlighter-rouge">RollingUpdate</code>. When a DaemonSet has an update strategy of <code class="language-plaintext highlighter-rouge">RollingUpdate</code>, any change to the <code class="language-plaintext highlighter-rouge">spec.template</code> field (or subfields) in the DaemonSet will initiate a rolling update.</p>

<p>There are two parameters that control the rolling update of a DaemonSet.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">spec.minReadySeconds</code> - determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods. Its good practice to set this to a reasonable long value of 30 to 60 seconds to make sure the Pod comes up healthy.</li>
  <li><code class="language-plaintext highlighter-rouge">spec.updateStrategy.rollingUpdate.maxUnavailable</code> - indicates how many Pods may be simultaneously updated by the rolling update. Setting this to 1 is a safe approach as only one node of the cluster at a time will be impacted.</li>
</ul>

<p>Once a rolling update has started you can use the <code class="language-plaintext highlighter-rouge">kubectl rollout</code> command to see the current status of a DaemonSet rollout. For example <code class="language-plaintext highlighter-rouge">kubectl rollout status daemonSets my-daemon-set</code>.</p>

<h3 id="deleting-a-daemon-set">Deleting a Daemon Set</h3>

<p><code class="language-plaintext highlighter-rouge">kubectl delete -f fluentd.yaml</code></p>

<p><strong><em>Note:</em></strong> Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. You can use the <code class="language-plaintext highlighter-rouge">--cascade=false</code> option to prevent that from happening.</p>

<h2 id="jobs">Jobs</h2>

<p>The Job construct in kubernetes lets you run one time short-lives task. A job creates Pods that run until successful termination (i.e. exit with 0).</p>

<h3 id="the-job-object">The Job object</h3>

<p>The Job object is responsible for creating and managing Pods defined in a template in the job specification. These Pods generally run until successful completion. The Job object coordinates running a number of Pods in parallel. If the Pod fails before a successful termination, the job controller will create a new Pod based on the pod template in the job specification.</p>

<p>Given that Pods have to be scheduled, theres is a chance that your job wil not execute if the required resources are not found by the scheduler. Also, due to the nature of distributed systems there is a small chance, during certain failure scenarios, that duplicate Pods will be created for a specific task.</p>

<h3 id="job-patterns">Job patterns</h3>

<p>Jobs are designed to manage batch-like workloads where work items are processed by one or more Pods. By default, each job runs a single Pod once until successful termination. This job pattern is defined by two primary attributes of a job, namely the number of job completions and the number of Pods to run in parallel. In the case of run once until completion for example, the <code class="language-plaintext highlighter-rouge">completions</code> and <code class="language-plaintext highlighter-rouge">parallelism</code> parameters are set to 1.</p>

<table>
  <thead>
    <tr>
      <th>Job pattern type</th>
      <th>Use case</th>
      <th>Behavior</th>
      <th>Completions</th>
      <th>Parallelism</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>One shot</td>
      <td>Database migrations</td>
      <td>A single Pod running once until successful completion</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Parallel fixed comletions</td>
      <td>Multiple pods processing a set of work in parallel</td>
      <td>Once or more Pods running one or more times until reaching a fixed completion count</td>
      <td>1+</td>
      <td>1+</td>
    </tr>
    <tr>
      <td>Work queue</td>
      <td>Multiple Pods processing from a centralized work queue</td>
      <td>One or more Pods running once until successful termination</td>
      <td>1</td>
      <td>2+</td>
    </tr>
  </tbody>
</table>

<h4 id="one-shot">One shot</h4>

<p>One shot jobs provide a way to run a single Pod once until successful termination. A Pod must be created and submitted to the Kubernetes API. This is done using a Pod template defined in the job configuration. Once a job is up and running, the Pod backing the job must be monitored for successful termination. A job can fail for any number of reasons, including an application error, an uncaught exception during runtime, or a node failure before the jbo has a chance to complete. In all cases, the job controller is responsible for recreating the Pod until a successful termination occurs.</p>

<p>There are multiple ways to create a one-shot job in Kubernetes. The easiest is to use the <code class="language-plaintext highlighter-rouge">kubectl</code> command line tools for example…</p>

<p><code class="language-plaintext highlighter-rouge">kubectl run -i oneshot --image=gcr.io/kuar-demo/kuard-amd64:blue --restart=OnFailure -- --keygen-enable --keygen-exit-on-complete --keygen-num-to-gen 10</code></p>

<p>Some things to note about the above example</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">-i</code> option indicates that this is an interactive command. <code class="language-plaintext highlighter-rouge">kubectl</code> will wait until the job is running and then show the log output from the first (and in this example only) Pod in the job.</li>
  <li><code class="language-plaintext highlighter-rouge">--restart=OnFailure</code> is the option that tells <code class="language-plaintext highlighter-rouge">kubectl</code> to create a Job object.  Restart policy can be set to Never if you do not wish for kubernetes to keep trying to restart the Pod if it fails. The behavior with Never will be to keep crating new Pods which can create a lot of Junk in your cluster so the <code class="language-plaintext highlighter-rouge">OnFailre</code> restart option is preferred. Kubelet has mechanisms to do a crash loop backoff so as to not keep eating resources on the cluster.</li>
  <li>All of the options after <code class="language-plaintext highlighter-rouge">--</code> are command line arguments to the container image. These instruct our test server (kuard) to genearte 10 4096 bit SSH keys and then exit.</li>
  <li><code class="language-plaintext highlighter-rouge">kubectl</code> often misses the first couple of lines of the output with the <code class="language-plaintext highlighter-rouge">-i</code> option.</li>
</ul>

<p>After the job has completed, the Job object and related Pod are still around. This is so that you can inspect the log output. Note that this job won’t show up in <code class="language-plaintext highlighter-rouge">kubectl get jobs</code> unless you pass the <code class="language-plaintext highlighter-rouge">-a</code> flag. Without this flag <code class="language-plaintext highlighter-rouge">kubectl</code> hides completed jobs. To delete the job use the command <code class="language-plaintext highlighter-rouge">kubectl delete jobs oneshot</code>.</p>

<p>You can also use a manifest file to create a job.  Below is an example.  You would then submit the job with the command <code class="language-plaintext highlighter-rouge">kubectl apply -f job-oneshot.yaml</code>.  You can view the results of the job by looking at the logs of the Pod that was created using <code class="language-plaintext highlighter-rouge">kubectl describe jobs oneshot</code> to find the Pod and <code class="language-plaintext highlighter-rouge">kubectl logs oneshot-4kfdt</code> to see the logs from Pod.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">batch/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Job</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">oneshot</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
        <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
    <span class="na">args</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-enable"</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-exit-on-complete"</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-num-to-gen=10"</span>
    <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
</code></pre></div></div>

<p>One thing to consider is a job may get stuck and not make progress. To handle this you want to use a liveness probe to determine if the job should be restarted.</p>

<h3 id="parallelism">Parallelism</h3>

<p>If for example you wnt to generate 100 keys by having 10 runs of kuard with each generating 10 keys while limiting to just running 5 pods at a time this will translate to <code class="language-plaintext highlighter-rouge">completions</code> of 10 and <code class="language-plaintext highlighter-rouge">parallelism</code> of 5.  Below is an example of such a job manifest.  You can start this job with the command <code class="language-plaintext highlighter-rouge">kubectl apply -f job-parallel.yaml</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">batch/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Job</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">parallel</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">chapter</span><span class="pi">:</span> <span class="s">jobs</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">parallelism</span><span class="pi">:</span> <span class="m">5</span>
  <span class="na">completions</span><span class="pi">:</span> <span class="m">10</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">chapter</span><span class="pi">:</span> <span class="s">jobs</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
        <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
        <span class="na">args</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-enable"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-exit-on-complete"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-num-to-gen=10"</span>
      <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
</code></pre></div></div>

<h3 id="work-queues">Work Queues</h3>

<p>The manifest example below is telling the job to start up fie Pods in parallel. As the <code class="language-plaintext highlighter-rouge">completions</code> parameter is unset, we put the job into a worker pool mode. Once the first Pod exits with a zero exit code, the job will start winding down and will not start any new Pods. This means that none of the workers should exit until the work is done and they are all in the process of finishing up.  Note that in the book a work to be done queue was set up for the example workers.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">batch/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Job</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">message-queue</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">consumer</span>
    <span class="na">chapter</span><span class="pi">:</span> <span class="s">jobs</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">consumers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">parallelism</span><span class="pi">:</span> <span class="m">5</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">message-queue</span>
        <span class="na">component</span><span class="pi">:</span> <span class="s">consumer</span>
        <span class="na">chapter</span><span class="pi">:</span> <span class="s">jobs</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">worker</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">gcr.io/kuar-demo/kuard-amd64:blue"</span>
        <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
        <span class="na">args</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-enable"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-exit-on-complete"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-memq-server=http://queue:8080/memq/server"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--keygen-memq-queue=keygen"</span>
      <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
</code></pre></div></div>

<p>For the example manifest above we can clean up using labels with the command <code class="language-plaintext highlighter-rouge">kubectl delete rs,svc,job -l chapter=jobs</code></p>

<h3 id="cronjobs">CronJobs</h3>

<p>Below is an example of a CronJob definition in Kubernetes. Note the <code class="language-plaintext highlighter-rouge">sepc.schedule field</code>, which contains the interval for the CronJob in standard cron format. As with other definitions you can schedule it with the command <code class="language-plaintext highlighter-rouge">kubectl create -f cron-job.yaml</code> and use <code class="language-plaintext highlighter-rouge">kubectl describe &lt;cron-job&gt;</code> to get the details.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">batch/v1beta1</span>
    <span class="s">kind</span><span class="pi">:</span> <span class="s">CronJob</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">example-cron</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="c1"># Run every fifth hour</span>
      <span class="na">schedule</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0</span><span class="nv"> </span><span class="s">*/5</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*"</span>
      <span class="na">jobTemplate</span><span class="pi">:</span>
        <span class="na">spec</span><span class="pi">:</span>
          <span class="na">template</span><span class="pi">:</span>
            <span class="na">spec</span><span class="pi">:</span>
              <span class="na">containers</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">batch-job</span>
                <span class="na">image</span><span class="pi">:</span> <span class="s">my-batch-image</span>
              <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">OnFailure</span>
</code></pre></div></div>

<h2 id="configmaps-and-secrets">ConfigMaps and Secrets</h2>

<h3 id="configmaps">ConfigMaps</h3>

<p>ConfigMaps are used to provide configuration information for workloads. This information can be a short string or a file. One way to think of ConfigMaps is a Kubernetes object that defines a set ov variables that can be used when defining the environment or command line for your containers.</p>

<h3 id="creating-configmaps">Creating ConfigMaps</h3>

<p>To imperatively create a ConfigMap which makes a <code class="language-plaintext highlighter-rouge">my-config.txt</code> file available to a Pod you would use a command similar to …</p>

<p><code class="language-plaintext highlighter-rouge">kubectl create configmap my-config --from-file=my-config.txt --from-literal=extra-param=extra-value --from-literal=another-param=another-value</code></p>

<p>In this example the context of <code class="language-plaintext highlighter-rouge">my-config.txt</code> are below…</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample config file that I might use to configure an application
parameter1 = value1
parameter2 = value2
</code></pre></div></div>

<p>The YAML for the above command would …</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
    <span class="s">data</span><span class="pi">:</span>
      <span class="na">another-param</span><span class="pi">:</span> <span class="s">another-value</span>
      <span class="na">extra-param</span><span class="pi">:</span> <span class="s">extra-value</span>
      <span class="s">my-config.txt</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s"># This is a sample config file that I might use to configure an application</span>
        <span class="s">parameter1 = value1</span>
        <span class="s">parameter2 = value2</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s">...</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">my-config</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">13556"</span>
      <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/default/configmaps/my-config</span>
      <span class="na">uid</span><span class="pi">:</span> <span class="s">3641c553-f7de-11e6-98c9-06135271a273</span>
</code></pre></div></div>

<p>You can get the YAML for the ConfigMap imperatively created with the command <code class="language-plaintext highlighter-rouge">kubectl get configmaps my-config -o yaml</code></p>

<h3 id="using-a-configmap">Using a ConfigMap</h3>

<p>There are three main ways to use a ConfigMap:</p>

<ul>
  <li>Filesystem - You can mount a ConfigMap into a Pod. A file is created for each entry based on the key name. The contents of that file are set to the value.</li>
  <li>Environment variable - A ConfigMap can be used to dynamically set the value of an environment variable</li>
  <li>Command line arguments - Kubernetes supports dynamically creating the command line for a container based on ConfigMap values.</li>
</ul>

<p>The manifest below shows examples of these three use cases.</p>

<p>For the filesystem method, we create a new volume inside the Pod and give it the name <code class="language-plaintext highlighter-rouge">config-volume</code>. We then define this volume to be a ConfigMap volume and point at the ConfigMap to mount. We have to specify where this gets mounted into the <code class="language-plaintext highlighter-rouge">kuard</code> containers with a <code class="language-plaintext highlighter-rouge">volumeMount</code>. In this case we are mounting it a <code class="language-plaintext highlighter-rouge">/config</code>. Environments variables are specified with a special <code class="language-plaintext highlighter-rouge">valueFrom</code> member. This references the ConfigMap and the data key to use within that ConfigMap. Command line arguments build on environment variables. Kubernetes will perform the correct substitution with a special <code class="language-plaintext highlighter-rouge">$(&lt;env-var-name&gt;)</code> syntax.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard-config</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">test-container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
      <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
      <span class="na">command</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">/kuard"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">$(EXTRA_PARAM)"</span>
      <span class="na">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ANOTHER_PARAM</span>
          <span class="na">valueFrom</span><span class="pi">:</span>
            <span class="na">configMapKeyRef</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">my-config</span>
              <span class="na">key</span><span class="pi">:</span> <span class="s">another-param</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">EXTRA_PARAM</span>
          <span class="na">valueFrom</span><span class="pi">:</span>
            <span class="na">configMapKeyRef</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">my-config</span>
              <span class="na">key</span><span class="pi">:</span> <span class="s">extra-param</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">config-volume</span>
          <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/config</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">config-volume</span>
      <span class="na">configMap</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">my-config</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>

<h2 id="secrets">Secrets</h2>

<p>Secrets are similar to ConfigMaps but focus on making sensitive available to the workload. They can be used for things like credentials or TLS certificates. Secrets are exposed to Pds via explicit declaration in Pod manifests and the Kubernetes API. In this way, the Kubernetes secrets API provides an application centric mechanism for exposing sensitive configuration information to applications in a way that’s easy to audit and leverages native OS isolation primitives.</p>

<p><strong><em>Note:</em></strong> By default, Kubernetes secrets are stored in plain text in <code class="language-plaintext highlighter-rouge">etcd</code> storage for the cluster. This may not be sufficient security depending on your requirements. Anyone who has cluster admin rights will be able to read all of the secrets in the cluster. Recent versions of Kubernetes add support for encrypting the secrets with a user supplied key which is generally integrated into a cloud key store. Most cloud key stores have integrations with Kubernetes flexible volumes, enabling you to skip Kubernetes secrets entirely and rely on the cloud providers key store.</p>

<h3 id="creating-secrets">Creating Secrets</h3>

<p>Secretes hold one or more data elements as a collection of key/value paris and are creted either with Kubernetes API or the kubectl command-line tool.</p>

<p>Below is an example of using the <code class="language-plaintext highlighter-rouge">kubectl</code> command to created a secret which stored a TLS certificate and key from files local on the system where the command is being run. The command will create a secret with two elements <code class="language-plaintext highlighter-rouge">kuard.crt</code> and <code class="language-plaintext highlighter-rouge">kuard.key</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl create secret generic kuard-tls --from-file=kuard.crt --from-file=kuard.key</code></p>

<p>To view the secret use the command below</p>

<p><code class="language-plaintext highlighter-rouge">kubectl describe secrets kuard-tls</code></p>

<h3 id="consuming-secrets">Consuming Secrets</h3>

<p>Secrets can be consumed using the Kubernetes REST API by applications that know how to call that API directly. This approach may not be desirable as this makes your application not portable (will only work in Kubernetes). Another option is to use a <strong>secrets volume</strong>.</p>

<h4 id="secrets-volumes">Secrets volumes</h4>

<p>Secret data can be exposed to Pods using the secrets volume type. Secret volumes are managed by the <code class="language-plaintext highlighter-rouge">kubelet</code> and are created at Pod creation time. Secrets are stored on tmpfs volumes (aka RAM disks), and as such are not written to disk on nodes. Each element of a secret is stored in a separate file under the target mount point specified in the volume mount.</p>

<p>The below Pod manifest demonstrates how to declare a secrets volume which exposes the <code class="language-plaintext highlighter-rouge">kuard-tls</code> secret volume to <code class="language-plaintext highlighter-rouge">/tls</code> mount point.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard-tls</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard-tls</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
      <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tls-certs</span>
        <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tls"</span>
        <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tls-certs</span>
      <span class="na">secret</span><span class="pi">:</span>
        <span class="na">secretName</span><span class="pi">:</span> <span class="s">kuard-tls</span>
</code></pre></div></div>

<h3 id="private-docker-registries">Private Docker registries</h3>

<p>A special use case for secretes is to store access credentials for private Docker registries. Private Docker images can be stored across one or more private registries. This presents a challenge for managing credentials for each private registry on every possible node in the cluster. <strong>Image pull secrets</strong> leverage the secrets API to automate the distribution of private registry credentials. Image pull secrets are stored just like normal secrets but are consumed through the <code class="language-plaintext highlighter-rouge">spec.imagePullSecrets</code> Pod specification field.</p>

<p>Use the <code class="language-plaintext highlighter-rouge">create secret docker-registry to create this special kind of secret. For example </code>kubectl create secret docker-registry my-image-pull-secret –docker-username=<username> --docker-password=<password> --docker-email=<email-address>`</email-address></password></username></p>

<p>You then enable access to the private repository by referencing the image pull secret in the Pod manifest file as in the example below.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">kuard-tls</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">kuard-tls</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">gcr.io/kuar-demo/kuard-amd64:blue</span>
      <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tls-certs</span>
        <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tls"</span>
        <span class="na">readOnly</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">imagePullSecrets</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span>  <span class="s">my-image-pull-secret</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tls-certs</span>
      <span class="na">secret</span><span class="pi">:</span>
        <span class="na">secretName</span><span class="pi">:</span> <span class="s">kuard-tls</span>
</code></pre></div></div>

<p>If you are repeatedly pulling from the same registry, you can add the secrets to the default service account associated with each Pod to avoid having to specify the secrets in every Pod you create.</p>

<h3 id="naming-constraints">Naming Constraints</h3>

<p>The key names for data items inside of a secret or ConfigMap are defined to map to valid environment variable names. They may begin with a dot followed by a letter or number.  Following characters include dots, dashes, and underscores. Dots cannot be repreated and dots and underscores or dashes cannot be adjacent to each other. The below regular expression checks for valid names.</p>

<p><code class="language-plaintext highlighter-rouge">^[.[?[a-zAZ0-9[([.[?[a-zA-Z0-9[+[-_a-zA-Z0-9[?)*$</code></p>

<p>Below are some examples of valid and invalid names.</p>

<table>
  <thead>
    <tr>
      <th>Valid key name</th>
      <th>Invalid key name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">.auth_token</code></td>
      <td><code class="language-plaintext highlighter-rouge">Token..properties</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Key.pem</code></td>
      <td><code class="language-plaintext highlighter-rouge">auth file.json</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">config_file</code></td>
      <td><code class="language-plaintext highlighter-rouge">_password.txt</code></td>
    </tr>
  </tbody>
</table>

<p>When selecting a key name, consider that these keys can be exposed to Pods via a volume mount. Pick a name that is going to make sense when specified on a command line or in a config file. Storing a TLS key as key.pem is more clear than <code class="language-plaintext highlighter-rouge">tls-key</code> when configuring applications to access secrets.</p>

<p>As of Kubernetes 1.6 ConfigMaps are unable to store binary data and can only be UTF-8 text.  You can in theory encode binary data into a base64 string and put it into the YAML file, but that makes YAML files hard to manage.</p>

<p>Maximum size for a ConfigMap or secret is 1MB.</p>

<h3 id="managing-configmaps-and-secrets">Managing ConfigMaps and Secrets</h3>

<p>Secrets and ConfigMaps are managed through the Kubernetes API. The usual <code class="language-plaintext highlighter-rouge">create</code>, <code class="language-plaintext highlighter-rouge">delete</code>, <code class="language-plaintext highlighter-rouge">get</code> and <code class="language-plaintext highlighter-rouge">describe</code> commands work for manipulating these objects.</p>

<h4 id="listing">Listing</h4>

<p><code class="language-plaintext highlighter-rouge">kubectl get secrets</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl get configmaps</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl describe configmap my-config</code></p>

<h4 id="creating">Creating</h4>

<p>The easiest way to create a secret or ConfigMap is via <code class="language-plaintext highlighter-rouge">kubectl create secret generic</code> or <code class="language-plaintext highlighter-rouge">kubectl create configmap</code>. Ther are a variety of ways to specify the data items that go into the secret or ConfigMap. These can be combined in a single command.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">--from-file=&lt;filename&gt;</code> - Load from the file with the secret data key the same as the filename</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">--from-file=&lt;key&gt;=&lt;filename&gt;</code> - Load from the file with the secret data key explicitly specified</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">--from-file=&lt;directory&gt;</code> - Load all files in the specified directory where the filename is an acceptable key name</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">--from-literal=&lt;key&gt;=&lt;value&gt;</code> - Use the specified key/value pair directly</p>
  </li>
</ul>

<h4 id="updating">Updating</h4>

<p>You can update a ConfigMap or secret and have it reflected in running programs. There is no need to restart if the application is configured to reread configuration values. This is a rare feature but might be something you can add to your applications.</p>

<p>Below are three ways to update ConfigMaps or secrets.</p>

<h5 id="update-from-file">Update from file</h5>

<p>If you have a manifest for your ConfigMap or secret, you can just edit it directly and push a new version with <code class="language-plaintext highlighter-rouge">kubectl replace -f &lt;filename&gt;</code> You can also use <code class="language-plaintext highlighter-rouge">kubectl apply -f &lt;filename&gt;</code> if you previously created the resource with kubectl apply. Due to the way that datafiles are encoded into these objects, updating a configuration can be a bit cumbersome as there is no provision in <code class="language-plaintext highlighter-rouge">kubectl</code> to load data from an external file. The data must be stored directly in the YAML manifest. The most common use case is when the ConfigMap is defined as part of a directory or list of resources and everything is created and updated together. Oftentimes these manifests will be checked into source control. Be careful not put push secrets into a public location.</p>

<h5 id="recreate-and-update">Recreate and update</h5>

<p>If you store the inputs into your ConfigMaps or secrets as separate files on disk (as opposed to embedded into YAML directly), you can use <code class="language-plaintext highlighter-rouge">kubectl</code> to recreate the manifest and then use it to update the object. This will look something like…</p>

<p><code class="language-plaintext highlighter-rouge">kubectl create secret generic kuard-tls --from-file=kuard.crt --from-file=kuard.key --dry-run -o yaml | kubectl replace -f -</code></p>

<p>This command line first creates a new secret with the same name as our existing secret. If we just stopped there, the Kubernetes API server would return an error complaining that we are tying to create a secret that already exists. Instead, we tell <code class="language-plaintext highlighter-rouge">kubectl</code> not to actually send the data to the server but instead to dump the YAML that it <em>would have</em> sent to the API server to <code class="language-plaintext highlighter-rouge">stdout</code>. We then pipe that to <code class="language-plaintext highlighter-rouge">kubectl</code> replace and use <code class="language-plaintext highlighter-rouge">-f -</code> to tell it to read from <code class="language-plaintext highlighter-rouge">stdin</code>. In this way we can update a secret from files on disk without having to manually base64 encode the data.</p>

<h5 id="edit-current-version">Edit current version</h5>

<p>You can use <code class="language-plaintext highlighter-rouge">kubectl edit</code> to bring up a version of the ConfigMap in your editor so you can modify it (you can also do this with a secret but you will have to update the base64 encoding value manually)</p>

<p><code class="language-plaintext highlighter-rouge">kubectl edit configmap my-config</code></p>

<p>One you make the desired changes in your editor and save, the new version of the object will be pushed to the Kubernetes API server.</p>

<h5 id="live-updates">Live updates</h5>

<p>Once a ConfigMap or secret is updated using the API, it will automatically be pushed to all volumes that use that ConfigMap or secret. This update may take a few seconds. Currently there is not built in way to signal an application when a new version of ConfigMap is deployed. It is up to the application or some helper script to look for the config files to change and reload them.</p>

<h2 id="role-based-access-control-for-kubernetes">Role based access control for Kubernetes</h2>

<p>Role Based Access Control (RBAC) has been generally available since Kubernetes 1.8. It provides a mechanism for restricting access and actions on Kubernetes APIs to ensure that only appropriate users have access to APIs in the cluster. RBAC is only part of a good security solution. Anyone who can run arbitrary code inside the Kubernetes cluster can effectively obtain root privileges on the entire cluster. RBAC by itself is not sufficient to protect from this. You must isolate the Pods running in your cluster using hypervisor isolated containers or some sort of sandbox, or both.</p>

<p>Every request in Kubernetes is first <em>authenticated</em>. Authentication provides the identity of the caller issuing the request. This could be as simple as saying that the request is unauthenticated, or it could intergrade with a pluggable authentication provider (for example Azure Active Directory) to establish an identity within that third party system. Kubernetes does not have a built in identity store; it integrates other identity sources.</p>

<p>Once a user is identified the authorization phase determines if the user is authorized to perform the request. Authorization is a combination of the identity of the user, the resource (effectively the HTTP path), and the verb or action the user is attempting to perform. If user is not authorized a 403 is returned.</p>

<h3 id="identity-in-kubernetes">Identity in Kubernetes</h3>

<p>Every request that comes to Kubernetes is associated with some identity. Requests with no identity are associated with <code class="language-plaintext highlighter-rouge">system:unauthenticated</code> group. Kubernetes makes a distinction between user identities and service account identities. Service accounts are created and managed by by Kubernetes itself and are generally associated with components running inside the cluster. User accounts are all other accounts associated with actual users of the cluster (often associated with continuous delivery as a service running outside the cluster).</p>

<p>Kubernetes uses a generic interface for authentication providers. Each of the providers supplies a user name and optionally a set of groups to which the user belongs. Kubernetes supports a number of provides including basic HTTP authentication (deprecated) x509 client certificates, static token files on host, Cloud authentication (Azure AD, AWS IAM etc) and Authentication webhooks. Managed Kubernetes will configure authentication for you, but in your own cluster you get to decide.</p>

<h3 id="roles-and-role-bindings">Roles and role bindings</h3>

<p>Roles and role bindings are used to determine if a user is authorized to perform an action on an object once the user identity is known. A <strong>role</strong> is a set of capabilities. For example <code class="language-plaintext highlighter-rouge">appdev</code> role might represent the ability to create Pods and services. A <strong>role binding</strong> is an assignment of a role to one or more identities. Binding the <code class="language-plaintext highlighter-rouge">appdev</code> role to the user <code class="language-plaintext highlighter-rouge">alice</code> indicates that Alice has the ability to create Pods and services.</p>

<p>There are two related resources in Kubernetes that represent roles and role bindings. <code class="language-plaintext highlighter-rouge">Role</code> and <code class="language-plaintext highlighter-rouge">RoleBinding</code> which applies to just a namespace and <code class="language-plaintext highlighter-rouge">ClusterRole</code> and <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code> which apply across the whole cluster.</p>

<p><code class="language-plaintext highlighter-rouge">Role</code> resources are namespaced, and represent capabilities within that single namespace. You cannot use namespaced roles for non-namespaced resources (e.g., CustomResourceDefinitions), and binding a <code class="language-plaintext highlighter-rouge">RoleBinding</code> to a role only provides authorization within the Kubernetes namespace that contains both the <code class="language-plaintext highlighter-rouge">Role</code> and the <code class="language-plaintext highlighter-rouge">RoleDefinition</code>.</p>

<p>Below is an example of a role that gives an identity the ability to create and modify Pods and services.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span>
    <span class="s">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">pod-and-services</span>
    <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
      <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">pods"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">services"</span><span class="pi">]</span>
      <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">create"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">delete"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">patch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">update"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">]</span>
</code></pre></div></div>

<p>To bind the above <code class="language-plaintext highlighter-rouge">Role</code> to the user <code class="language-plaintext highlighter-rouge">alice</code> we need to create a <code class="language-plaintext highlighter-rouge">RoleBinding</code> exemplified in the below manifest.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">RoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pods-and-services</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">User</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">alice</span>
<span class="pi">-</span> <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Group</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mydevs</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-and-services</span>
</code></pre></div></div>

<p>If you want to make the same action but cluster wide you would use <code class="language-plaintext highlighter-rouge">ClusterRole</code> and <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code>. They are largely identical tot he namespaced peers, but have a larger scope.</p>

<h3 id="verbs-for-kubernetes-roles">Verbs for Kubernetes roles</h3>

<p>Commonly used vers in Kubernetes RBAC are listed elow.</p>

<table>
  <tbody>
    <tr>
      <td>Verb</td>
      <td>HTTP method</td>
      <td>Description</td>
    </tr>
    <tr>
      <td>—-</td>
      <td>———–</td>
      <td>———-_</td>
    </tr>
    <tr>
      <td>create</td>
      <td>POST</td>
      <td>Create a new resource</td>
    </tr>
    <tr>
      <td>delete</td>
      <td>DELETE</td>
      <td>Delete an existing resource</td>
    </tr>
    <tr>
      <td>get</td>
      <td>GET</td>
      <td>Get a resource</td>
    </tr>
    <tr>
      <td>list</td>
      <td>GET</td>
      <td>List a collection of resources</td>
    </tr>
    <tr>
      <td>patch</td>
      <td>PATCH</td>
      <td>Modify an existing resource via a partial change</td>
    </tr>
    <tr>
      <td>update</td>
      <td>PUT</td>
      <td>Modify an existing resource via a complete object</td>
    </tr>
    <tr>
      <td>watch</td>
      <td>GET</td>
      <td>Watch for streaming updates to a resource</td>
    </tr>
    <tr>
      <td>proxy</td>
      <td>GET</td>
      <td>Connect to resource via a streaming WebSocket proxy</td>
    </tr>
  </tbody>
</table>

<h3 id="using-build-in-roles">Using build-in roles</h3>

<p>Kubernetes has a large number of well-known system identities (e.g., a scheduler) that require a know set of capabilities. There are also built in cluster roles.  You can view these roles by running the command <code class="language-plaintext highlighter-rouge">kubectl get clusterroles</code>.  Most of these roles are for system utilities four are intended for generic end users.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">cluster-admin</code> role provides complete access to the entire cluster</li>
  <li><code class="language-plaintext highlighter-rouge">admin</code> role provides complete access to a complete namespace</li>
  <li><code class="language-plaintext highlighter-rouge">edit</code> role allows an end user to modify things in a namespace</li>
  <li><code class="language-plaintext highlighter-rouge">view</code> role allows for read only access to a namespace</li>
</ul>

<p>To see the cluster role binding use the command <code class="language-plaintext highlighter-rouge">kubectl get clusterrolebindings</code></p>

<h4 id="auto-reconciliation-of-built-in-roles">Auto-reconciliation of built in roles</h4>

<p>When the Kubernetes API server starts up, it automatically installs a number of default <code class="language-plaintext highlighter-rouge">ClusterRoles</code> that are defined int the code of the API server itself. This means if you modify those roles your changes will get discarded when the API server restarts (for example during an update).  To prevent this from happening you need to add the <code class="language-plaintext highlighter-rouge">rbac.authorization.kubernetes.io/autoupdate</code> annotation with a value of <code class="language-plaintext highlighter-rouge">false</code> to the built in ClusterRole resource. If this annotation is set to <code class="language-plaintext highlighter-rouge">false</code> the API server will not overwrite the modified ClusterRole resource.</p>

<p>By default, the Kubernetes API server installs a cluster role that allows <code class="language-plaintext highlighter-rouge">system:unauthenticated</code> users access to the API server’s API discovery endpoint. This is bad for any cluster exposed to a hostile environment such as the open internet. If you need to lock this down, ensure that the <code class="language-plaintext highlighter-rouge">--anonymous=auth=false</code> flag is set on your API serer.</p>

<h3 id="managing-rbac">Managing RBAC</h3>

<h4 id="testing-authorization-with-can-i">Testing authorization with can-i</h4>

<p>You can use the <code class="language-plaintext highlighter-rouge">can i</code> functionality of <code class="language-plaintext highlighter-rouge">kubectl</code> to verify if you have access to perform an action. You can also have users use this to validate their permissions.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl auth can-i create pods</code></p>

<p>You can also test sub-resources like logs or port forwarding with the <code class="language-plaintext highlighter-rouge">--subresource</code> command line flag</p>

<p><code class="language-plaintext highlighter-rouge">kubectl auth can-i get pods --subresource=logs</code></p>

<h4 id="managing-rbac-in-source-control">Managing RBAC in Source Control</h4>

<p>Like all resources in Kubernetes, RBAC resources are modeled using JSON or YAML so they can be source controlled.  <code class="language-plaintext highlighter-rouge">kubectl</code> has a <code class="language-plaintext highlighter-rouge">reconcile</code> command that operates similar to <code class="language-plaintext highlighter-rouge">kubectl apply</code> but for RBAC resources and will reconcile a text-based set of roles and role bindings with the current state of the cluster.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl auth reconcile -f some-rbac-config.yaml</code></p>

<p>If you want to see the changes before they are made you can add the <code class="language-plaintext highlighter-rouge">--dry-run</code> flag to the command.</p>

<h3 id="aggregating-clusterroles">Aggregating ClusterRoles</h3>

<p>RBAC supports the usage of an <strong>aggregation rule</strong> to combine multiple roles together in a new role. This new role combined all of the capabilities of all of the aggregate roles together, and any changes to any of the sub-roles will automatically be propagated back into the aggregate role. <code class="language-plaintext highlighter-rouge">ClusterRoles</code> to be aggregated are specified using label selectors. The <code class="language-plaintext highlighter-rouge">aggregationRule</code> field in the <code class="language-plaintext highlighter-rouge">ClusterRole</code> resource contains a <code class="language-plaintext highlighter-rouge">clusterRoleSelector</code> field, which in turn is a label selector. All <code class="language-plaintext highlighter-rouge">ClusterRole</code> resources that match this selector are dynamically aggregated into the <code class="language-plaintext highlighter-rouge">rules</code> array in the aggregate <code class="language-plaintext highlighter-rouge">ClusterRole</code> resource.</p>

<p>A best practice for managing <code class="language-plaintext highlighter-rouge">ClusterRole</code> resource is to create a number of fine-grained cluster roles and then aggregate them together to form higher-level or broadly defined cluster roles. Below is an example of the built in <code class="language-plaintext highlighter-rouge">edit</code> role defined to be the aggregate of all <code class="language-plaintext highlighter-rouge">ClusterRole</code> objects that have a label of <code class="language-plaintext highlighter-rouge">rbac.authrization.k8s.io/aggregate-to-edit</code> set to <code class="language-plaintext highlighter-rouge">true</code>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">edit</span>
  <span class="s">...</span>
<span class="na">aggregationRule</span><span class="pi">:</span>
  <span class="na">clusterRoleSelectors</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">matchLabels</span><span class="pi">:</span>
    <span class="s">rbac.authorization.k8s.io/aggregate-to-edit</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="s">...</span>
</code></pre></div></div>

<h3 id="using-groups-for-bindings">Using Groups for bindings</h3>

<p>When you bind a group to a <code class="language-plaintext highlighter-rouge">ClusterRole</code> or a namespace Role, anyone who is a member of that group gains access to the resource and verbs defined by that role. To enable any individual go gain access to the group’s role, that individual needs to be added to the group. Its better to use groups to control access for obvious reasons. Its hard to manage permissions at an individual person level.</p>

<p>To bind a group to a ClusterRole use a <code class="language-plaintext highlighter-rouge">Group</code> kind for the <code class="language-plaintext highlighter-rouge">subject</code> in the binding. In Kubernetes, groups are supplied by authentication providers. There is no strong notion of group in Kubernetes itself; just that an identify can be part of one or more groups, and these groups can be associated with a <code class="language-plaintext highlighter-rouge">Role</code> or <code class="language-plaintext highlighter-rouge">ClusterRole</code> via a binding.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Group</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-great-groups-name</span>
    <span class="s">...</span>
</code></pre></div></div>

<h2 id="integrating-storage-solutions-and-kubernetes">Integrating storage solutions and Kubernetes</h2>

<h3 id="importing-external-services">Importing External Services</h3>

<p>This section has a side discussion on a use case for namespaces.  In the YAML below you can specify my-database as the database and vary the namespace. That way depending on the namespace you are running in when <code class="language-plaintext highlighter-rouge">my-database</code> is referenced you will get a different instance. This is good for having a test environment.  The Kubernetes DNS service will return <code class="language-plaintext highlighter-rouge">my-database.test.svc.cluster.internal</code> in the test namespace and <code class="language-plaintext highlighter-rouge">my-database.prod.svc.cluster.internal</code> in the prod namespace.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-database</span>
  <span class="c1"># note 'prod' namespace here</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="nn">...</span>
</code></pre></div></div>

<p>A key difference between a service running in Kubernetes and one that is outside Kubernetes is that in Kubernetes label selectors can be used to identify the dynamic set of Pods what are the backends for a particular service. With a service running outside of Kubernetes this is not the case.</p>

<p>Lets assume we have a service running on the host <code class="language-plaintext highlighter-rouge">database.company.com</code> To import this external database service into Kubernetes, we start by creating a service without a Pod selector that references the DNS name of the database server.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">external-database</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ExternalName</span>
  <span class="na">externalName</span><span class="pi">:</span> <span class="s">database.company.com</span>
</code></pre></div></div>

<p>When a typical Kubernetes service is created, an IP address is also created and the Kubernetes DNS service is populated with an A record that points to that IP address. When you create a service of type <code class="language-plaintext highlighter-rouge">ExternalName</code>, the Kubernetes DNS service is instead populated with a CNAME record that points to the external name you specified (<code class="language-plaintext highlighter-rouge">database.company.com</code> in this example). When an application in the cluster does a DNS lookup for the hostname <code class="language-plaintext highlighter-rouge">external-database.svc.default.cluster</code>, the DNS protocol aliases that name to <code class="language-plaintext highlighter-rouge">database.company.com</code>. In this way all containers in Kubernetes believe that they are talking to a service that is backed with other containers.  This technique can also apply to calling cloud services such as cloud provided database which give you a URL for the resource.</p>

<p>If you don’t have a DNS name for the external service but just an IP address. The operation to import the service is a little different. You first crate a service without a label selector, but also without a <code class="language-plaintext highlighter-rouge">ExternalName</code> type we used before. This will make Kubernetes allocate a virtual IP address for this service and populate an A record for it.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">external-ip-database</span>
</code></pre></div></div>

<p>Since there is no selector for the service Kubernetes cannot populate the endpoints for the service for the load balancer to redirect traffic to.  The user is responsible for populating the end-points manually using an <code class="language-plaintext highlighter-rouge">Endpoints</code> resource as in the example below.  If you have more than one IP address for redundancy, you can repeat them in the <code class="language-plaintext highlighter-rouge">addresses</code> array. Once the endpoints are populated the load balancer will start redirecting traffic from you Kubernetes service to the IP address endpoints.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Endpoints</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">external-ip-database</span>
<span class="na">subsets</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">addresses</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">ip</span><span class="pi">:</span> <span class="s">192.168.0.1</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">3306</span>
</code></pre></div></div>

<p><strong><em>Note:</em></strong> External services do not perform any  health checking.</p>

<h3 id="running-reliable-singletons">Running reliable singletons</h3>

<p>The challenge of running storage solutions in Kubernetes is that primitives like ReplicaSet expect that every container is identical and replaceable, but for most storage solutions this is not the case. One option to address this is to run a single Pod that runs the database or other storage solution. This avoids the challenges of running replicated storages in Kubernetes since there is no replication. This is approach is acceptable for environments where limited downtime (for maintenance for example) is acceptable.</p>

<h4 id="running-a-mysql-singleton">Running a MySQL singleton</h4>

<p>To run a MySQL singleton you will need.</p>

<ul>
  <li>A persistent volume for the on-disk storage of the application managed independently of the lifespan of the running MySQL application.</li>
  <li>A mySQL Pod that will run the MySQL application</li>
  <li>A service that will expose this Pod to other containers in the cluster</li>
</ul>

<p>In this example we will use NFS for storage but Kubernetes has many other options which included cloud storage provider options. The YAML example below defines a <code class="language-plaintext highlighter-rouge">PersistentVolume</code> object which users NFS and has 1GB of space</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">volume</span><span class="pi">:</span> <span class="s">my-volume</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
  <span class="na">nfs</span><span class="pi">:</span>
    <span class="na">server</span><span class="pi">:</span> <span class="s">192.168.0.1</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/exports"</span>
</code></pre></div></div>

<p>With the persistent volume created we need to claim that persistent volume for our Pod.  This is done with a <code class="language-plaintext highlighter-rouge">PersistentVolumeClaim</code> object as in the example below. Note that the <code class="language-plaintext highlighter-rouge">selector</code> field is used to find the matching volume we defined earlier using a label. You can declare volumes directly inside a Pod specification, but this locks that Pod specification to a particular volume provider. By using volume claims, you can keep your Pod specifications cloud-agnostic; Just create different volumes specific to the cloud, and use a <code class="language-plaintext highlighter-rouge">PersistentVolumeClaim</code> to bind them together.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">volume</span><span class="pi">:</span> <span class="s">my-volume</span>
</code></pre></div></div>

<p>Now that we’ve claimed our volume, we can use a ReplicaSet to construct our singleton Pod. The user or ReplicaSet (instead of just a Pod) but this is better for reliability. Once scheduled to a machine, a bare Pod is bound to that machine forever. If the machine fails, then any Pods that are on that machine that are not being managed by a higher-level controller such as a ReplicaSet vanish with the machine and are not rescheduled elsewhere. For this reason in example below we use a ReplicaSet with a replica size of one for our database.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">mysql</span>
  <span class="s"># labels so that we can bind a Service to this Pod</span>
  <span class="s">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">mysql</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mysql</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">mysql</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">mysql</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="m">1</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">2Gi</span>
        <span class="na">env</span><span class="pi">:</span>
        <span class="c1"># Environment variables are not a best practice for security,</span>
        <span class="c1"># but we're using them here for brevity in the example.</span>
        <span class="c1"># See Chapter 11 for better options.</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_ROOT_PASSWORD</span>
          <span class="na">value</span><span class="pi">:</span> <span class="s">some-password-here</span>
        <span class="na">livenessProbe</span><span class="pi">:</span>
          <span class="na">tcpSocket</span><span class="pi">:</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">3306</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">3306</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
            <span class="c1"># /var/lib/mysql is where MySQL stores its databases</span>
            <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/lib/mysql"</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
        <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
          <span class="na">claimName</span><span class="pi">:</span> <span class="s">database</span>
</code></pre></div></div>

<p>The final step is to expose this as a Kubernetes services which is done in the example below. This will expose a service named <code class="language-plaintext highlighter-rouge">mysql</code> which we can access at the full domain name <code class="language-plaintext highlighter-rouge">mysql.svc.default.cluster</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mysql</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">3306</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
  <span class="na">selector</span><span class="pi">:</span>
<span class="na">app</span><span class="pi">:</span> <span class="s">mysql</span>
</code></pre></div></div>

<h3 id="dynamic-volume-provisioning">Dynamic volume provisioning</h3>

<p>Many clusters include <strong>dynamic volume provisioning</strong> which has the cluster operator creating one or more <code class="language-plaintext highlighter-rouge">StorageClass</code> objects. The example below shows a default storage class object that automatically provisions disk objects on the Microsoft Azure platform. Once a storage class has been created for a cluster, you can refer to this refer to this storage class in your persistent volume claim, rather than referring to any specific persistent volume.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">storageclass.beta.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">kubernetes.io/cluster-service</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">kubernetes.io/azure-disk</span>
</code></pre></div></div>

<p>When the dynamic provisioner sees this storage claim, it uses the appropriate volume driver to create the volume and bind it to your persistent volume claim. The example below shows usage of a <code class="language-plaintext highlighter-rouge">PersistentVolumeClaim</code> that uses the <code class="language-plaintext highlighter-rouge">default</code> storage class. The <code class="language-plaintext highlighter-rouge">volume.beta.kubernetes.io/storage-class</code> annotation is what links this claim back up to the storage class we created. <strong><em>Note:</em></strong> The lifespan of persistent volumes id dedicated by the reclamation policy of the <code class="language-plaintext highlighter-rouge">PersistentVolumeClaim</code> and the default is to bind the lifespan of the volume to that of the Pod. This means if you delete a Pod (with a scale down or other event) then the volumes is deleted as well. While this may be the desired behavior you need to be careful not to delete your persistent volumes.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-claim</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">volume.beta.kubernetes.io/storage-class</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">10Gi</span>
</code></pre></div></div>

<p>Persistent volumes are good for traditional applications that need storage, but if you need to develop a high-availability high-availability, scalable storage in a Kubernetes-native fashion you should look into <code class="language-plaintext highlighter-rouge">StatefulSet</code>.</p>

<h3 id="kubernetes-native-storage-with-statefulsets">Kubernetes native storage with StatefulSets</h3>

<p>StatefulSets are replicated groups of Pods, similar to ReplicaSets, but unlike RepilcaSets they have the following unique properties.</p>

<ul>
  <li>Each replica gets a persistent hostname with a unique index (e.g., database-0, database-1, etc.)</li>
  <li>Each replica is created in order from lowest to highest index, and creation will block until the Pod at the previous index is healthy and available. This also applies to scaling up.</li>
  <li>When a StatefulSet is deleted, each of the managed replica Pods is also deleted in order from highest to lowest. This also applied to scaling down the number of replicas.</li>
</ul>

<p>This set of features makes it easier to deploy storage applications on Kubernetes. For example, the combination of stable hostnames and ordering mean that all replcias, other thant he first one, can reliable reference <code class="language-plaintext highlighter-rouge">database-0</code> for the purposes of discovery and establishing replication quorum.</p>

<h4 id="manually-replicated-mongodb-with-statefulsets">Manually replicated MongoDB with StatefulSets</h4>

<p>In this section we set up a replicated MongoDB cluster as an example of using StatefulSets. The example below creates a MongoDB based replicated storage utilizing StatefulSets. When this manifest is deployed with <code class="language-plaintext highlighter-rouge">kubectl apply -f mongo-simple.yaml</code> you will see the pods coming up one at a time and getting unique names for each Pod.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StatefulSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mongo</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">serviceName</span><span class="pi">:</span> <span class="s2">"</span><span class="s">mongo"</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">mongo</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">mongodb</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">mongo:3.4.1</span>
        <span class="na">command</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">mongod</span>
        <span class="pi">-</span> <span class="s">--replSet</span>
        <span class="pi">-</span> <span class="s">rs0</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">27017</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">peer</span>
</code></pre></div></div>

<p>Once the StatefulSet is created, we also need to create a “headless” service to manage the DNS entries for the StatefulSet. In Kubernetes a service is called <strong>headless</strong>  if it doesn’t have a cluster virtual IP address. Since with StatefulSets each Pod has a unique identity, it doesn’t really make sense to have a load-balancing IP address for the replicated service. You can create a headless service using <code class="language-plaintext highlighter-rouge">clusterIP: None</code> in the service specification as in the example below.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mongo</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">27017</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">peer</span>
  <span class="na">clusterIP</span><span class="pi">:</span> <span class="s">None</span>
  <span class="na">selector</span><span class="pi">:</span>
<span class="na">app</span><span class="pi">:</span> <span class="s">mongo</span>
</code></pre></div></div>

<p>Once the service is created, there are four DNS entries that are populated. <code class="language-plaintext highlighter-rouge">mongo.default.svc.cluster.local</code> is created, but unlike with a standard service a DNS lookup on this hostname provides all the addresses in the StatefulSet. In addition, entries are created for <code class="language-plaintext highlighter-rouge">mongo-0.mongo.default.svc.cluster.local</code> and <code class="language-plaintext highlighter-rouge">mongo-1.mongo</code> and <code class="language-plaintext highlighter-rouge">mongo-2.mongo</code>. Each of these resolves to the specific IP address of the replica index in the StatefulSet. Thus, with StatefulSets you get well-defined, persistent names for each replica in the set. This useful when you are configuring a replicated storage solution.</p>

<p>At this point you would need to run the below command below in the Pod to set to make <code class="language-plaintext highlighter-rouge">rs0</code> with <code class="language-plaintext highlighter-rouge">mongo-0.mongo</code> be the initial primary.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nb">exec</span> <span class="nt">-it</span> mongo-0 mongo <span class="o">&gt;</span> rs.initiate<span class="o">(</span> <span class="o">{</span>
_id: <span class="s2">"rs0"</span>,
      members:[ <span class="o">{</span> _id: 0, host: <span class="s2">"mongo-0.mongo:27017"</span> <span class="o">}</span> <span class="o">]</span>
     <span class="o">})</span><span class="p">;</span>
OK
</code></pre></div></div>

<p>To automate the execution of this command we can add an additional containers to our Pods to perform the initialization.  You can use ConfigMap to add a script into the existing MongoDB image as in the example below. Its mounting a ConfigMap volume whose name is <code class="language-plaintext highlighter-rouge">mongo-init</code>. This ConfigMap holds a script that performs our initialization. It first determines whether it is running on <code class="language-plaintext highlighter-rouge">mongo-0</code> or not. If it si on <code class="language-plaintext highlighter-rouge">mongo-0</code> it creates teh ReplicaSet using the same command we ran manually. If its on a different replica it waits until the ReplicaSet exists and then it registers itself as a member of that ReplicaSet.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">...</span>
<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">init-mongo</span>
  <span class="na">image</span><span class="pi">:</span> <span class="s">mongo:3.4.1</span>
  <span class="na">command</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">bash</span>
  <span class="pi">-</span> <span class="s">/config/init.sh</span>
  <span class="na">volumeMounts</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
    <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/config</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
    <span class="na">configMap</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">mongo-init"</span>
</code></pre></div></div>

<p>The script below sleeps forever after initializing the cluster. Since every container in the Pod needs to have the same <code class="language-plaintext highlighter-rouge">RestartPolicy</code>. Since we don’t want the Mongo cluster to get restarted we need to have our initialization containers run forever as well or Kubernetes may think the Pod is unhealthy.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mongo-init</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="s">init.sh</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">#!/bin/bash</span>
    <span class="s"># Need to wait for the readiness health check to pass so that the</span>
    <span class="s"># mongo names resolve. This is kind of wonky.</span>
    <span class="s">until ping -c 1 ${HOSTNAME}.mongo; do</span>
      <span class="s">echo "waiting for DNS (${HOSTNAME}.mongo)..."</span>
<span class="s">sleep 2 done</span>
    <span class="s">until /usr/bin/mongo --eval 'printjson(db.serverStatus())'; do</span>
      <span class="s">echo "connecting to local mongo..."</span>
      <span class="s">sleep </span><span class="m">2</span>
    <span class="s">done</span>
    <span class="s">echo "connected to local."</span>
<span class="s">HOST=mongo-0.mongo:27017</span>
    <span class="s">until /usr/bin/mongo --host=${HOST} --eval 'printjson(db.serverStatus())'; do</span>
      <span class="s">echo "connecting to remote mongo..."</span>
      <span class="s">sleep </span><span class="m">2</span>
    <span class="s">done</span>
    <span class="s">echo "connected to remote."</span>
    <span class="s">if [[ "${HOSTNAME}" != 'mongo-0' ]]; then</span>
      <span class="s">until /usr/bin/mongo --host=${HOST} --eval="printjson(rs.status())" \</span>
            <span class="s">| grep -v "no replset config has been received"; do</span>
        <span class="s">echo "waiting for replication set initialization"</span>
        <span class="s">sleep </span><span class="m">2</span>
      <span class="s">done</span>
      <span class="s">echo "adding self to mongo-0"</span>
      <span class="s">/usr/bin/mongo --host=${HOST} \</span>
         <span class="s">--eval="printjson(rs.add('${HOSTNAME}.mongo'))"</span>
    <span class="s">fi</span>
    <span class="s">if [[ "${HOSTNAME}" == 'mongo-0' ]]; then</span>
      <span class="s">echo "initializing replica set"</span>
      <span class="s">/usr/bin/mongo --eval="printjson(rs.initiate(\</span>
          <span class="s">{'_id'</span><span class="pi">:</span> <span class="s1">'</span><span class="s">rs0'</span><span class="err">,</span> <span class="s1">'</span><span class="s">members'</span><span class="pi">:</span> <span class="pi">[{</span><span class="s1">'</span><span class="s">_id'</span><span class="pi">:</span> <span class="nv">0</span><span class="pi">,</span> <span class="nv">\</span>
           <span class="nv">'host'</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mongo-0.mongo:27017'</span><span class="pi">}]</span><span class="err">}</span><span class="s">))"</span>
    <span class="s">fi</span>
    <span class="s">echo "initialized"</span>
    <span class="s">while </span><span class="no">true</span><span class="s">; do</span>
      <span class="s">sleep </span><span class="m">3600</span>
<span class="s">done</span>
</code></pre></div></div>

<h4 id="persistent-volumes-and-statefulsets">Persistent volumes and StatefulSets</h4>

<p>For persistent storage you need to mount a persistent volume in the <code class="language-plaintext highlighter-rouge">/data/db</code> directory in the Pod template. Because the StatefulSet replicates more than one Pod you cannot simply reference a persistent volume claim. Instead you need to add a <strong>persistent volume claim template</strong>. A claim template is like a Pod template, but it creates volume clmains.</p>

<p>This following needs to be added to your stateful definition. When y ou add a volume claim template to a StatefulSet definition, each time the StatefulSet controller creates a Pod that is part of the StatefulSet it will create a persistent volume claim based on this tempalte as part of that Pod.  For these replicated persistent volumes to work correctly, you either need to have auotprovisions set up for persistent volumes or you need to prepopulate a collection of persistent volume object for the StatefulSet controller to draw from. If there are no claims that can be created, the StatefulSet controlller will not be able to create the corresponding Pods.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumeClaimTemplates</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">database</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">volume.alpha.kubernetes.io/storage-class</span><span class="pi">:</span> <span class="s">anything</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">accessModes</span><span class="pi">:</span> <span class="pi">[</span> <span class="s2">"</span><span class="s">ReadWriteOnce"</span> <span class="pi">]</span>
    <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">100Gi</span>
</code></pre></div></div>

<h4 id="readines-probes-for-the-mongo-cluster">Readines probes for the Mongo cluster</h4>

<p>For this example we can use the below for the readines probe.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">livenessProbe</span><span class="pi">:</span>
  <span class="na">exec</span><span class="pi">:</span>
    <span class="na">command</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">/usr/bin/mongo</span>
    <span class="pi">-</span> <span class="s">--eval</span>
    <span class="pi">-</span> <span class="s">db.serverStatus()</span>
  <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">10</span>
  <span class="na">timeoutSeconds</span><span class="pi">:</span> <span class="m">10</span>
</code></pre></div></div>

<h2 id="extending-kubernetes">Extending Kubernetes</h2>

<p>Even cluster admins should be careful and use diligence when extending Kuberneetes with third party tools. Some extensions can be used as a vector to steal secrets or run malicious code. Extending a cluster makes it different than stock Kubernetes. When running multiple clusters, it is very valuable to build tooling to maintain consistency of experience across clusters which shold include extensions that are installed.</p>

<h3 id="points-of-extensibility">Points of extensibility</h3>

<p>In addition to admission controllers and API extensions, there are a numbrer of ways to “extend” your cluster wihtout modifying the API server. These include DaemonSets that install automatic logging and monitoring tools that can scan your services for vulnerabilities.</p>

<p>Admission contorllers are called prior to the API object being written into the backing storage. Adminssion controllers can reject or modify API requests. There are several admission controllers that are built into the Kubernetes API server. The limit range admission controller sets default limits for Pods without default limits. Many systems use custom admission controllers to auto-inject sidecar continers into all Pods created on the system to create “auto-magic” experiences.</p>

<p>The other for of extension which can be used in conjunction with admission controllers is custom resources. With custom resources whole new API objects are added to the Kubernetes API surface area. These new APIs can be added to namespaces,are subject to RBAC and can be used with existing tools such as <code class="language-plaintext highlighter-rouge">kubectl</code>.</p>

<p>The first thing you need to do to create a custom resource is to create a CustomResourceDefinition. This objec tis a meta-resource (a resource that is the definition of another resource). As a concrete exmple consider defining a new resource to represent load tests in your cluster. When a new LoadTest resource is created, a load test is spun up in your Kubernetes cluster and drives traffic to a service.  The first steap in creating this resource is defining it through CustomResourceDefinition. Below is an example. The name of a custom resource has to have the form <code class="language-plaintext highlighter-rouge">&lt;resource plural&gt;.&lt;api-group&gt;</code> to ensure that each resource definition is unique to the cluster.  The API group in the spec musht match the suffix in the metadata. Once you use <code class="language-plaintext highlighter-rouge">kubectl create -f loadtest-resource.yaml</code> (loadtest-resource.yaml is the file name with below example) you can right away run the command <code class="language-plaintext highlighter-rouge">kubectl get loadtests</code>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apiextensions.k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">CustomResourceDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">loadtests.beta.kuar.com</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">group</span><span class="pi">:</span> <span class="s">beta.kuar.com</span>
  <span class="na">versions</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">served</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">scope</span><span class="pi">:</span> <span class="s">Namespaced</span>
  <span class="na">names</span><span class="pi">:</span>
    <span class="na">plural</span><span class="pi">:</span> <span class="s">loadtests</span>
    <span class="na">singular</span><span class="pi">:</span> <span class="s">loadtest</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">LoadTest</span>
    <span class="na">shortNames</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">lt</span>
</code></pre></div></div>

<p>Now that you have the resource created you can create the resource. The manifest for that is below. You can provide a schema for the CustomResourceDefinition, but unless you want to do validation you don’t need to do that. Even if you do want to do validation you can register a validating admission controller. The manifest below can bre created using <code class="language-plaintext highlighter-rouge">kubectl create -f loadtest.yaml</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">beta.kuar.com/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">LoadTest</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-loadtest</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">service</span><span class="pi">:</span> <span class="s">my-service</span>
  <span class="na">scheme</span><span class="pi">:</span> <span class="s">https</span>
  <span class="na">requestsPerSecond</span><span class="pi">:</span> <span class="m">1000</span>
  <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">/index.html</span>
    <span class="pi">-</span> <span class="s">/login.html</span>
    <span class="pi">-</span> <span class="s">/shares/my-shares/</span>
</code></pre></div></div>

<p>At this point we can list the loadtest resource but it does not do anything yet. You canuse the CRUD API to create read update delete objects but they will take no action. To actually do something a controller needs to be present in the cluster to react to the new API we defined. We need a piece of code that will continuously monitor the custom resources and create, modify, or delete LoadTests as necessary to implement the API. There is a Watch API on the API server that your code can use to implement this control loop. The API is a bit hard to use so you should look into supported mehcanisms such as <code class="language-plaintext highlighter-rouge">informer</code> in the <a href="https://pkg.go.dev/k8s.io/client-go/informers">client-go library</a></p>

<h3 id="patterns-for-custom-resources">Patterns for custom resources</h3>

<h4 id="just-data">Just data</h4>

<p>In the Just data pattern, you are simply using the API server for storage and retrieval of information for your application. You should not use the API server for storage and retrieval for your application; its snot designed for that. API extensions should be used for control or configuration object that help you manage the deployment or runtime of your application. An examle use case for “just data” might be configuration for canary deployments of your application. While you can use ConfigMaps; ConfigMaps are untyped and sometimes using a more strongly types API extension object provides clarity and ease of use. Extensiosn that are just data don’t need a corresponding controller to activate them, but they may have validating or mutating admission controlles to ensure that they are well formed.</p>

<h4 id="compilers">Compilers</h4>

<p>This pattern knows as “compiler” or “abstraction” pattern has the API extension object representing a higher-level abstraction that is “compiled” into a combination of lower level Kubernetes objects. The LoadTest extension we discussed prior is an example of this pattern. A user consumes the extension as a high level concept, in this examle a <code class="language-plaintext highlighter-rouge">loadtest</code>, but it comes into being by being deployed as a collection of Kubernetes Pods and services. To achive this, a compiled abstraction requries an API controller to be running somewehre in the cluster, to watch the current LoadTests and create the “compiled” represntation. There is however no online helath maintenances.</p>

<h4 id="operators">Operators</h4>

<p>Operator pattern provides online proactive management of the resources created by the extension. These extensions likely provide a higher level abstraction (for example a database) that is compield down to a lower level representation, but they also provide online functionality such as snapshot, backups, or upgrades. To achieve this, the controller not only monitors the running state of the application supplied by the extension to add or remove things as necessary, but also monitors the running state of the application supplied by the extension and takes actions to remediate unhealthy databases, take snapshots or restore from a snapshot if failure occurs. Operators are the most complicated pattern for API extension.</p>

<h4 id="getting-started">Getting started</h4>

<p>The <a href="https://kubebuilder.io">Kubebuilder project</a> is a good resource to use when getting started with extension development.</p>

<h2 id="organizing-your-application">Organizing your application</h2>

<h3 id="principles-to-guide-us">Principles to guide us</h3>

<ul>
  <li>Filesystems as the source of truth</li>
  <li>Code review to ensure the quality of changes</li>
  <li>Feature flags for staged roll forward and roll back</li>
</ul>

<h4 id="feature-gate-and-guards">Feature gate and guards</h4>

<p>Feature gates and guards play an important role in bridging the gap of developing new features in source control and the deployment of those features into production. You can do the development behind a feature flag or gate as in the sippet below. This enables commiting of code to the production branch long before the feature is ready to ship. The enabling of the feature requries just a configuration change to activate the flag. This makes it very clear what chagned in the production environment, and makes it easy to roll back without having to go back to an older version of the code.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="nx">featureFlags</span><span class="p">.</span><span class="nx">myFlag</span><span class="p">){</span>
  <span class="c1">// Feature implementation goes here</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="filesystem-layout">Filesystem layout</h4>

<p>The first cardinality on which you want to organize your application is the semantic component or layer (e.g., frontend, batch work queue etc.) This may seem like overkill when one team manages all these components but it makes it easy to scale the team later. An application with a front end that uses two services would have a file layour similar to the below. Within each directory the configuration for each application is stored. These are YAML files that directly represent the current state of the cluster. Its generally useful to include both service namea nd the object type withing the same file. While Kubernetes allows for the creation of YAML files with multiple objects in the same file, this should generally be avoided. The only good reason to group objects in the same file is if they are conceptually identical. If grouping objects togetehr doesn’t form a single concept, they probably shouldn’t be in a single file.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  frontend-deployment.yaml
  frontend-service.yaml
  frontend-ingress.yaml
service-1/
  service-1-deployment.yaml
  service-1-service.yaml
  service-1-configmap.yaml
service-2/
  ...
</code></pre></div></div>

<h4 id="managing-periodic-versions">Managing periodic versions</h4>

<p>It is very useful to be able to look back historically and see what your application deployment previously looked like. It is also useful to be able to iterate a configuration forward while still being able to deploy a stable release configuration. Thus its handy to be able to simultaneously store and maintain multiple different revisions of your configuration. There are two approaches to this.</p>

<p>The first is to use tags, branches and source-control features. This leads to a more simplified directory structure and aligns with how source code revisions are managed. The second options is to clone the configuration within the filesystem and use directoreis for different revisions. This approach is convinient because it makes simultaneous viewing of the configuration very straightforward. Both of these are essentially identical and are just a matter of preference.</p>

<h5 id="versioning-with-branches-and-tags">Versioning with branches and tags</h5>

<p>When you are ready for realease, you place a source-control tag (e.g <code class="language-plaintext highlighter-rouge">git tag v1.0</code>) on the config version and the HEAD continues to iterate forward. When you need to update the release configuration; first you commit the change to HEAD of the repository, then you create a new branch named v1 at the v1.0 tag. You then cherry-pick the desired change onto the release branch (<code class="language-plaintext highlighter-rouge">git cherry-pick &lt;edit&gt;</code>), and finally you tag this branch with the <code class="language-plaintext highlighter-rouge">v1.1</code> tag to indicate the new point release. <strong><em>Note:</em></strong> A common error is to cherry pick fixes into a release branch only. Its a good idea to cherry-pick it into all active releases, in case for some reason you need to roll back versions but the fix is still needed.</p>

<h5 id="versioning-with-directories">Versioning with directories</h5>

<p>In this approach your versioned deployment exists within its own directory as in the example below. All deployments occur from <code class="language-plaintext highlighter-rouge">HEAD</code> instead of from specific revision tags. When adding a new configuration it is done to the files in the <code class="language-plaintext highlighter-rouge">current</code> directory. When creating a new rlease the current directory is copied to create a new directory associated wit the new release. When performing a bugfix change to a release, the pull request must modify the YAML file in all the relevant release directories. This is a slightly better experience than the cherry-picking approach described earlier, since it is clear in a single change request that all of the relevant versions are bing updated with the same change, instead of requiring a cherry-pick per version.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  v1/
    frontend-deployment.yaml
    frontend-service.yaml
  current/
    frontend-deployment.yaml
    frontend-service.yaml
service-1/
  v1/
    service-1-deployment.yaml
    service-1-service.yaml
  v2/
    service-1-deployment.yaml
    service-1-service.yaml
  current/
    service-1-deployment.yaml
    service-1-service.yaml
...
</code></pre></div></div>

<h3 id="structuring-your-appliation-for-development-testing-and-deployment">Structuring your appliation for development, testing and deployment</h3>

<p>There are two goals for your application with regard to development and testing.</p>

<ul>
  <li>Each developer should be able to easily develop new features for the application. Developers should be able to work in their own environment with all services available.</li>
  <li>You should be able to easily and accurately test your application prior to deployment. This is essential to being able to quickly roll out features while maintaining availability.</li>
</ul>

<h4 id="progression-of-a-release">Progression of a release</h4>

<p>To achieve both these goals it is important to relate stages of development to the release versions described earlier. The stages of release are:</p>

<ul>
  <li>HEAD - The bleeding edge of the configuration; the latest changes</li>
  <li>Development - Largely stable, but not ready for deployment. Suitable for developers to use for building features.</li>
  <li>Staging - The beginning of testing, unlikely to change unless problems are found.</li>
  <li>Canary - The first realease to users, used to test for problems with real world traffic and likewise give user a chance to test what is coming next.</li>
  <li>Release - the current production release</li>
</ul>

<p>You should use a tag to mark the development stage. An automated process should be used to test the <code class="language-plaintext highlighter-rouge">HEAD</code> branch, and if test pass the <code class="language-plaintext highlighter-rouge">development</code> tag is moved to <code class="language-plaintext highlighter-rouge">HEAD</code>. Thus developers can track reasonably close to the latest changes when deploying their own environments.</p>

<p>To map the stage to a specific revision if using source control approach you would use tags and if using file system approach you would use symbolic links.</p>

<h4 id="parameterizing-your-applications-with-templates">Parameterizing your applications with templates</h4>

<p>Since its impractical to keep the development stages identical, but you want the environments to be as identical as possible its a good idea to have parametrirized environments with templates. This lets you use a template for the bulk of the configuration but have a limited set of parameters to produce the final configuration. This makes it easy to see the differences between environments.</p>

<h5 id="parameterizing-with-helm-and-templates">Parameterizing with Helm and templates</h5>

<p><a href="https://helm.sh">Helm</a> is a package manager for Kubernetes. They patterns described here for Helm will apply to whatever templating option you choose.</p>

<p>Helm templating language use the “mustache” syntax, so for exmple</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">-deployment</span>
</code></pre></div></div>

<p>indicates that <code class="language-plaintext highlighter-rouge">Release.Name</code> should be substituded into the name of a deployment. To pass a parameter for this value you use <code class="language-plaintext highlighter-rouge">values.yaml</code> file with contents like the below.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Release</span><span class="pi">:</span>
  <span class="na">Name</span><span class="pi">:</span> <span class="s">my-release</span>
</code></pre></div></div>

<p>After paramter substitution you would get …</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-release-deployment</span>
</code></pre></div></div>

<h5 id="filesystem-layour-for-parameterization">Filesystem layour for parameterization</h5>

<p>With parametarization, instead of treating each deployment lifecycle stage as a pointer to a version, each deployment lifecycle is the combination of a parameter file and a pointer to a specific version.  Below is an example of such a layout.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  staging/
    templates -&gt; ../v2
    staging-parameters.yaml
  production/
    templates -&gt; ../v1
    production-parameters.yaml
  v1/
    frontend-deployment.yaml
    frontend-service.yaml
  v2/
    frontend-deployment.yaml
    frontend-service.yaml
</code></pre></div></div>

<p>Doing this with version control looks similar, except that the parameters for each life-cycle stage are kept at the root of the configuration directory tree.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  staging-parameters.yaml
  templates/
  frontend-deployment.YAML
</code></pre></div></div>

<p>Should you need to have different configurations for different regions, your directory layout would lool like the below.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  staging/
    templates -&gt; ../v3/
    parameters.yaml
  eastus/
    templates -&gt; ../v1/
    parameters.yaml
  westus/
    templates -&gt; ../v2/
    parameters.yaml
...
</code></pre></div></div>

<p>If using version control your layout would look like…</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend/
  staging-parameters.yaml
  eastus-parameters.yaml
  westus-parameters.yaml
  templates/
    frontend-deployment.yaml
...
</code></pre></div></div>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">My References</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">My References</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
