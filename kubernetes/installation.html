<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kubernetes Installation | My References</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Kubernetes Installation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<link rel="canonical" href="http://0.0.0.0:4000/kubernetes/installation" />
<meta property="og:url" content="http://0.0.0.0:4000/kubernetes/installation" />
<meta property="og:site_name" content="My References" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kubernetes Installation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","headline":"Kubernetes Installation","url":"http://0.0.0.0:4000/kubernetes/installation"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="My References" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">My References</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Kubernetes Installation</h1>
  </header>

  <div class="post-content">
    <h2 id="kca-book-chapter-2-process">KCA book chapter 2 process</h2>

<p>Since the script in the book does not work here is the version that I created based on docs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt-get update
apt-get <span class="nb">install</span> <span class="nt">-y</span> docker.io
service docker restart
apt-get <span class="nb">install</span> <span class="nt">-y</span> apt-transport-https ca-certificates curl gnupg
curl <span class="nt">-fsSL</span> https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /etc/apt/keyrings/kubernetes-apt-keyring.gpg
<span class="nb">sudo chmod </span>644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg <span class="c"># allow unprivileged APT programs to read this keyring</span>
<span class="nb">echo</span> <span class="s1">'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /'</span> <span class="o">&gt;</span> /etc/apt/sources.list.d/kubernetes.list
<span class="nb">sudo chmod </span>644 /etc/apt/sources.list.d/kubernetes.list   <span class="c"># helps tools such as command-not-found to work correctly</span>
apt-get update
apt-get <span class="nb">install</span> <span class="nt">-y</span> kubeadm kubectl kubelet
</code></pre></div></div>

<h2 id="content-of-the-kubesh-script-once-fixed">Content of the kube.sh script once fixed</h2>

<p>kubectl apply -f <a href="https://docs.projectcalico.org/manifests/calico.yaml">https://docs.projectcalico.org/manifests/calico.yaml</a>
kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml</a></p>

<h2 id="chapter-2-lab-notes">Chapter 2 Lab notes</h2>

<h3 id="lab-2">Lab 2</h3>

<h3 id="init-cluster">Init cluster</h3>

<p><code class="language-plaintext highlighter-rouge">kubeadm init --pod-network-cidr=192.168.0.0/16</code></p>

<p>This gives you the command to run on the second host.</p>

<h3 id="kubectl-config-to-point-to-your-cluster">Kubectl config to point to your cluster</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$HOME</span>/.kube
<span class="nb">cp</span> <span class="nt">-i</span> /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</code></pre></div></div>

<h3 id="add-ingress-controller-and-mysterious-calico-kube-controller">Add ingress controller and mysterious calico kube controller</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> https://docs.projectcalico.org/manifests/calico.yaml
kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml
</code></pre></div></div>

<h3 id="lab-3">Lab 3</h3>

<p>use the pv.yaml file in your repo and run the command <code class="language-plaintext highlighter-rouge">kubectl create -f pv.yaml</code></p>

<p>Example of sorting the volumes by how much storage capacity they have. <code class="language-plaintext highlighter-rouge">kubectl get pv --sort=.spec.capacity.storage</code></p>

<h3 id="lab-4---performing-a-kubernetes-upgrade-with-kubeadm">Lab 4 - Performing a Kubernetes upgrade with kubeadm</h3>

<h4 id="control-plan-upgrade-steps">Control Plan Upgrade Steps</h4>

<p>Because the book I am using sucks, I need to do some hacking with versions to be able to rproduce the lab so all the commands below are just notes, I did not verify them.</p>

<ul>
  <li>Upgrade kubeadm on the control plane node.</li>
  <li>Drain the control plane node <code class="language-plaintext highlighter-rouge">kubectl drain ip-172-31-21-95 --ignore-daemonsets</code></li>
  <li>Plan the upgrade (kubeadm upgrade plan)</li>
  <li>Apply the upgrade (kubeadm upgrade apply) <code class="language-plaintext highlighter-rouge">sudo apt-get update &amp;&amp; sudo apt-get install -y --allow-change-held-packages kubeadm=1.27.2-00 &amp;&amp; kubeadm upgrade apply v1.27.2</code></li>
  <li>Upgrade kubelet and kubectl on the control plan node <code class="language-plaintext highlighter-rouge">sudo apt-get install -y --allow-change-held-packages kubectl=1.27.2-00 kublet=1.27.2-00 &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code></li>
  <li>Uncordon the control plane node <code class="language-plaintext highlighter-rouge">kubectl uncordor ip-172-31-21-95</code></li>
</ul>

<h4 id="worker-node-upgrade-steps">Worker node upgrade steps</h4>

<ul>
  <li>Drain the node</li>
  <li>Upgrade kubeadm <code class="language-plaintext highlighter-rouge">sudo apt-get install -y --allow-change-held-packages kubeadm=1.27.2-00</code></li>
  <li>Upgrade the kubelet configuration (kubeadm upgrade node) <code class="language-plaintext highlighter-rouge">sudo kubeadm upgrade node</code></li>
  <li>Upgrade kubelet and kubectl <code class="language-plaintext highlighter-rouge">sudo apt-get install -y --allow-change-held-packages kubectl=1.27.2-00 kubelet=1.27.2-00 &amp;&amp; systemctl daemon reload &amp;&amp; systemctl restart kubelet</code></li>
  <li>Uncordon the node <code class="language-plaintext highlighter-rouge">kubectl uncordon HOSTNAME</code></li>
</ul>

<h4 id="lab-5---working-with-namespaces">Lab 5 - Working with namespaces</h4>

<p>Each Kubernetes resource must be in only one namespace and namespaces cannot be nested inside one another.</p>

<p>You can define resource quotas and limits within each namespace to ensure that one tenant cannot monopolize cluster resources. This involves setting CPU, memory and storage limits for each tenant’s namespace.</p>

<p>To create a namespace named <code class="language-plaintext highlighter-rouge">dev</code> use the command <code class="language-plaintext highlighter-rouge">kubectl create namespace dev</code></p>

<h2 id="kca-book-chapter-3-notes">KCA book chapter 3 notes</h2>

<p>For HA in Kubernetes you need to have an multi master cluster.</p>

<p>Kubernetes datastore is, by default, Dqlite, a high availability SQLite.</p>

<p>For HA to work you need to have multiple instances of Kubernetes API running and that means that they need to be behind a load balancer.</p>

<p><strong>Stacked etcd</strong> If you have an HA setup with control plan running on multiple nodes, then you have etcd running on those nodes and that is stacked etcd. I guess the other option is to have etcd running on other nodes. There is a risk here of loosing both etcd and control plan if a node goes down, but that can be mitigated by adding more hosts.</p>

<p><strong>External etcd</strong> This is just etcd running on nodes other than your control plane nodes.</p>

<h3 id="k8s-management-tools-intro">K8s management tools intro</h3>

<h4 id="kubeadm">kubeadm</h4>

<p><code class="language-plaintext highlighter-rouge">kubeadm</code> utility provides <code class="language-plaintext highlighter-rouge">kubeadm init</code> and <code class="language-plaintext highlighter-rouge">kubeadm join</code> commands as best-practice quick path ways of establishing Kubernetes clusters.</p>

<h4 id="minikube">minikube</h4>

<p>Lets you set up a cluster using just a single server or machine for practice.</p>

<h4 id="helm">helm</h4>

<p>Helm provides templating and package management for Kubernetes objects. You can turn your Kubernetes objects or the applications running in your cluster into templates, referred to you by helm as charts to allow you to easily manage complex configurations of multiple Kubernetes objects. You can also download and use shared templates that others have already created.</p>

<p>helm is a package manager that allows you to search, share and use Kubernetes-specific applications. It simplifies the installation and maintenance of Kubernetes apps and is the Kubernetes equivalent of apt or yum.</p>

<p>helm uses a chart packaging format. A chart is a series of files that explain a group of Kubernetes resources. A single chart might be used to deploy anything basic, such as memcaced pod, or something sophisticated, sch as a whole we b app stack with HTTP servers, databases and caches.</p>

<h4 id="kompose">kompose</h4>

<p>Kompose is a tool that can convert docker compose files into kubernetes objects to help you move from local development to running in Kubernetes.</p>

<h4 id="kustomize">kustomize</h4>

<p>Kustomize is like helm, but it has a concept of layering where you have a base version and layering yaml artifacts (referred to as patching). This is a strong approach as most firms construct their project from a set of internally developed and off the shelf apps. You can leverage kustomize to absorb any base file modifications for your underlying components while preserving use-case-specific customization overrides.</p>

<h3 id="safely-draining-a-k8s-node">Safely Draining a K8s Node</h3>

<h4 id="draining">Draining</h4>

<p>Draining a node will cause the containers running causes containers running on the node to be gracefully terminated and moved to another nodes to prevent service interruption so that you can do maintenance on a node. When a node is being drained Kubernetes will also prevent fresh pods from coming in. If there are deamon set managed pods, the drain does not proceed unless the <code class="language-plaintext highlighter-rouge">--ignore-demonsets</code> option is used. Unless you use <code class="language-plaintext highlighter-rouge">--force</code>, the drain does not destroy any pods that do not mirror pods or are handled by a replication controller or are handled by a replicatiosn controller, replica set, DaemonSets, stateful set, or job. If the management resource of one or more pods is absent, <code class="language-plaintext highlighter-rouge">--force</code> enabled deletion to occur.</p>

<p>Format of the drain command is <code class="language-plaintext highlighter-rouge">kubectl drain &lt;node name&gt;</code></p>

<p>When you are ready for the node to operate again use <code class="language-plaintext highlighter-rouge">kubectl uncordon</code> to make the node schedulable again.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl drain</code> by default ignores system pods on the node that cannot be destroyed.</p>

<h3 id="upgrading-k8s-with-kubeadm">Upgrading K8s with kubeadm</h3>

<h4 id="control-plane-upgrade-steps">Control plane upgrade steps</h4>

<p>Following are the steps to upgrade the control plan node:</p>

<ul>
  <li>Upgrade kubeadm on the control plane node</li>
  <li>Drain the control plane node</li>
  <li>Plan the upgrade (kubeadm upgrade plan)</li>
  <li>Apply the upgrade (kubeadm upgrade apply)</li>
  <li>Uncordon the control plane node</li>
  <li>Upgrade kubelet and kubectl on the control plane node</li>
</ul>

<h4 id="worker-node-upgrade-steps-now-with-more-info">Worker node upgrade steps (now with more info)</h4>

<ul>
  <li>Upgrade kubeadm</li>
  <li>Drain the node</li>
  <li>Upgrade the kubelet configuration (kubeadm upgrade node)</li>
  <li>Upgrade kubelet and kubectl</li>
  <li>Uncordon the node</li>
</ul>

<h4 id="recovering-from-a-failure-state">Recovering from a Failure State</h4>

<p>If the kubeadm upgrade fails and does not roll back for example if you lost power or network during execution you can perform it again, the command is idempotent. You can also try <code class="language-plaintext highlighter-rouge">kubeadm upgrade apply --force</code> to recover from a problematic condition without altering the version of your cluster.</p>

<p>During the upgrading process, kubeadm creates the following backup directories in <code class="language-plaintext highlighter-rouge">/etc/kubernetes/tmp</code></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</code> - backup of this control plane nodes local etcd member data. If upgrade fails contents of this folder can be manually restored in /var/lib/etcd.</li>
  <li><code class="language-plaintext highlighter-rouge">kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code> - copies this control plane node’s static Pod manifest files. If an upgrade fails and the automated reollback also fails, the contents of this folder can be manually restored in <code class="language-plaintext highlighter-rouge">/etc/kubernetes/manifests</code>.</li>
</ul>

<h3 id="overview-of-upgrading-k8s-with-kubeadm">Overview of upgrading K8s with Kubeadm</h3>

<h4 id="stepscommands-to-upgrade-a-control-plane-node">Steps/commands to upgrade a control plane node</h4>

<p>Command to drain control plane node looks like <code class="language-plaintext highlighter-rouge">kubectl drain k8s-control --ignore-daemonsets</code></p>

<p>Command to execute the given update kubeadm looks like <code class="language-plaintext highlighter-rouge">sudo apt-get &amp;&amp; sudo apt-get install -y --allow-change-held-packages kubeadm=1.22.2-00</code> (this is from the crappy book real command is just similar)</p>

<p>To check kubeadm version use <code class="language-plaintext highlighter-rouge">kubeadm version</code></p>

<p>To “plan the upgrade” use the command <code class="language-plaintext highlighter-rouge">sudo kubeadm upgrade plan v1.22.2</code></p>

<p>To upgrade the control plan components the command looks like <code class="language-plaintext highlighter-rouge">sudo kubeadm upgrade apply v1.22.2</code></p>

<p>To upgrade kubelet and kubectl on the control plan node use the command <code class="language-plaintext highlighter-rouge">sudo apt-get update &amp;&amp; sudo apt-get install -y --allow-change-held-packages kubelet=1.22.2-00 kubectl=1.22.2-00</code></p>

<p>To reload the daemon use the command <code class="language-plaintext highlighter-rouge">sudo systemctl daemon-reload</code></p>

<p>To restart kubelet use the command <code class="language-plaintext highlighter-rouge">sudo systemctl restart kubelet</code></p>

<p>To uncordon the control plane node use the command <code class="language-plaintext highlighter-rouge">kubectl uncordon k8s-control</code></p>

<p>To verify the control plane is working use <code class="language-plaintext highlighter-rouge">kubectl get nodes</code></p>

<h4 id="stepscommands-to-upgrade-worker-node">Steps/commands to upgrade worker node</h4>

<p>To drain worker node use the command <code class="language-plaintext highlighter-rouge">kubectl drain k8s-worker1 --ignore-daemonsets --force</code></p>

<p>To upgrade kubeadm on the worker node <code class="language-plaintext highlighter-rouge">sudo apt-get update &amp;&amp; sudo apt-get install -y --allow-change-held=packages kubeadm=1.22.2-00</code></p>

<p>To upgrade the kubelet configuration on the worker node <code class="language-plaintext highlighter-rouge">sudo kubeadm upgrade node</code></p>

<p>To upgrade kubelet and kubectl on the worker node use <code class="language-plaintext highlighter-rouge">sudo apt-get update &amp;&amp; sudo apt-get install -y --allow-change-held-packages kubelet=1.22.2-00 kubectl=1.22.2-00</code></p>

<p>To reload the daemon on worker node <code class="language-plaintext highlighter-rouge">sudo systemctl daemon-reload</code></p>

<p>To restart kubelet on worker node 1 <code class="language-plaintext highlighter-rouge">sudo systemctl restart kubelet</code></p>

<p>To uncordon the worker node <code class="language-plaintext highlighter-rouge">kubectl uncordon k8s-worker1</code></p>

<p>To verify the worker node is working <code class="language-plaintext highlighter-rouge">kubectl get nodes</code></p>

<h3 id="backing-up-and-restoring-the-etcd-cluster-data">Backing up and restoring the etcd cluster data</h3>

<p>Some noteworthy features of etcd are</p>

<ul>
  <li>Fully replicated - Each node in an etcd cluster has full access to the datastore</li>
  <li>Highly Available - etcd is intended to have no single point of failure and endures hardware failures and network partitions gracefully</li>
  <li>Reliably Consistent - Each data ‘read’ returns the most recent write from all clusters</li>
  <li>Fast - The benchmark for etcd is 10,000 writes per second</li>
  <li>Secure - etcd supports automated Transport Layer Security (TLS) and optional SSL client certificate authentication. Its a best practice to use least priviledge to etcd as etcd maintains critical and highly sensitive configuration data.</li>
  <li>Simple - Using standard HTTP/JSON tools, any application may read or publish data to etcd</li>
</ul>

<h4 id="backing-up-and-restoring-etcd">Backing up and restoring etcd</h4>

<p>Etcd is where all Kubernetes object are kept. Backing up the etcd cluster data is critical for recovering Kubernetes cluster in catastrophe scenarios, such as losing all control plane nodes. All Kubernetes states and crucial information is contained int he snapshot file. Encrypt the snapshot files to keep important Kubernetes data safe.</p>

<p>There are two ways to back up an etcd cluster, built-in snapshots and volume snapshots.</p>

<h5 id="built-in-snapshot">Built in Snapshot</h5>

<p>You can backup etcd data using the etcd command-line tool <code class="language-plaintext highlighter-rouge">etcdctl</code> and its snapshot save command.  Example of running a backup below…</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ ETCDCTL_API</span><span class="o">=</span>3 etcdctl <span class="nt">--endpoints</span> <span class="nv">$ENDPOINT</span>
snapshot save &lt;file name&gt;
</code></pre></div></div>

<p>another example below because the book I am using is trash.</p>

<p><code class="language-plaintext highlighter-rouge">ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb</code></p>

<p>To verity the snapshot use <code class="language-plaintext highlighter-rouge">ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb</code></p>

<p>The snapshot would be taken on a live member. Taking the backup does not impact the members performance.</p>

<p>When using etcdctl there are some options to be aware of.  Below is an example of providing a an endpoint certificate.</p>

<p><code class="language-plaintext highlighter-rouge">ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacerts=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot save &lt;backup-file-location&gt;</code></p>

<p>To restore etcd the command would look like <code class="language-plaintext highlighter-rouge">ETCDCTL_API=3 etcdctl snaphsot restore &lt;file-name&gt;</code> There are options you can add to this command one of which has the restore creating a new logical cluster. The snapshot file could be from a backup or from a directory that still exits.</p>

<p>If the access URLs of the restored cluster differ from those of the prior cluster. In that case, the Kubernetes API server must be adjusted. You would do that by restarting the API server with the <code class="language-plaintext highlighter-rouge">--etcd-servers=$NEW_ETCD_CLUSTER $OLD_ETCD_CLUSTER</code> (replace the variables with the new and old cluster IPs). If a load balancer is in front of etcd cluster that may need to be adjusted as well.</p>

<p>A full example of the backup command is below which has hints on where to get certificates from.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3
etcdctl snapshot save <span class="nt">--ednpoints</span><span class="o">=</span>127.0.0.1:2379 <span class="se">\</span>
<span class="nt">--cacert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt <span class="se">\</span>
<span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt <span class="se">\</span>
<span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.key <span class="se">\</span>
/home/ubuntu/myk8scluster2.db
</code></pre></div></div>

<p>To get your nodes information go to the manifests directory look for the <code class="language-plaintext highlighter-rouge">etcd.yaml</code> file in <code class="language-plaintext highlighter-rouge">/etc/kubernetes/manifests/</code> that will have all the info you need for parameters in the backup command.</p>

<p>If its not obvious what restoring etcd would do: If for example you took an etcd backup and proceeded to delete a pod that was running when backup was taken.  When you restore the deleted pod would come back.</p>

<h5 id="volume-snapshot">Volume Snapshot</h5>

<p>This approach backs up etc data by taking a snapshot of the storage volume operating on a storage volume that supports backup such as Amazon EBS.</p>

<h2 id="kca-book-chapter-4-notes-kubernetes-object-management">KCA book chapter 4 notes (Kubernetes object management)</h2>

<h3 id="overview-of-kubectl">Overview of kubectl</h3>

<p>kubectl looks for a file named <code class="language-plaintext highlighter-rouge">config</code> in the <code class="language-plaintext highlighter-rouge">$HOME/.kube</code> directory for configuration. You can use the <code class="language-plaintext highlighter-rouge">KUBECONFIG</code> environment variable or the <code class="language-plaintext highlighter-rouge">kubeconfig</code> flag to spacify different config files.</p>

<p>Syntax of the kubectl command is <code class="language-plaintext highlighter-rouge">kubectl [command] [TYPE] [NAME] [flags]</code> where…</p>

<ul>
  <li>command - defines the action you want to take on one or more resources, such as create, get, describe, or delete full list in table below.</li>
  <li>TYPE - Indicates the type of resource. You can provide singular or plural or shortened forms of resource types, which are case-insensitive.</li>
  <li>NAME - Specifies the resource’s name. Case matters. If the name is missing all resources of the type are displayed.</li>
  <li>flags - Optional flags for the command. Flags will override any default values and environment variables.</li>
</ul>

<p>kubectl commands
<a href="&lt;&gt; (TODO: Sentance above int he book does not make sense, I need to verify)">comment</a>: &lt;&gt; (TODO: The syntax column is not very usable need to review and improve)</p>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Syntax</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>create</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl create -f FILENAME [flags]</code></td>
      <td>Create one or more resources from a file or stdin</td>
    </tr>
    <tr>
      <td>delete</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl delete (-f FILENAME or TYPE or NAME or NAME or -l label or --all) [flags]</code></td>
      <td>Delete resources either from file, or stdin, or specifying label selectors, names resource selectors, or resources</td>
    </tr>
    <tr>
      <td>describe</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl describe (-f FILENAME or TYPE  [NAME_PREFIX or NAME or -l label]) [flags]</code></td>
      <td>Display the detailed state of one or more resources</td>
    </tr>
    <tr>
      <td>exec</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [--COMMANDS] [args]</code></td>
      <td>Execute a command against a container in a pod</td>
    </tr>
    <tr>
      <td>get</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl get (-f FILENAME or TYPE or [NAME or /NAME or -l label]) [--watch][--sort-by=FIELD][[-o or --output]=OUTPUT_FORMAT][FLAGS]</code></td>
      <td>List one or more resources</td>
    </tr>
    <tr>
      <td>apply</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl apply -f FILENAME [flags]</code></td>
      <td>Apply a configuration change to a resource from a file or stdin.</td>
    </tr>
  </tbody>
</table>

<h3 id="lab-exploring-a-kubernetes-cluster-with-kubectl">Lab: exploring a Kubernetes Cluster with kubectl</h3>

<p>To get a capacity sorted list of persistent volumes use the command: <code class="language-plaintext highlighter-rouge">kubectl get pv --sort-by=.spec.capacity.storage</code></p>

<p>To run a command inside the “Quark Pod’s” container use <code class="language-plaintext highlighter-rouge">kubectl exec quark -n beebox-mobile -- cat /etc/key/key.txt</code></p>

<p>To create a deployment using a spec file use <code class="language-plaintext highlighter-rouge">kubectl apply -f /home/deployment.yml</code></p>

<h3 id="kubectl-tips">Kubectl tips</h3>

<p>Breakdown of kubernetes management techniques:</p>

<p>You get undefined behavior if you mix these techniques. Pick one and use that.</p>

<table>
  <thead>
    <tr>
      <th>Management technique</th>
      <th>Description</th>
      <th>Operates on</th>
      <th>Recommended environment</th>
      <th>Supported writers</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Imperative commands</td>
      <td>User operates directly on live items in a cluster</td>
      <td>Live objects</td>
      <td>Development projects</td>
      <td>1+</td>
      <td>Does not provide history of past configurations because it works on live objects</td>
    </tr>
    <tr>
      <td>Imperative object configuration</td>
      <td>DESC</td>
      <td>Individual files</td>
      <td>Production projects</td>
      <td>1</td>
      <td> </td>
    </tr>
    <tr>
      <td>Declarative object configuration</td>
      <td>DESC</td>
      <td>Directories of files</td>
      <td>Production projects</td>
      <td>1+</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>The <code class="language-plaintext highlighter-rouge">--record</code> flag can be used to write the command that was run in the kubernetes.io/change-cause resource annotation. The documented change can be used as a reference point in the future. For example, it can be used to look at the commands that were run in each Deployment revision.</p>

<h3 id="managing-k8s-role-based-access-control-rbac">Managing K8s role based access control (RBAC)</h3>

<p>RBAC is a Kubernetes feature that allows you to control who has access to what in the cluster.</p>

<p>There are 4 types of Kubernetes objects declared via the RBAC API.</p>

<ul>
  <li>Role</li>
  <li>ClusterRole</li>
  <li>RoleBinding</li>
  <li>ClusterRoleBinding</li>
</ul>

<h3 id="role-and-clusterrole">Role and ClusterRole</h3>

<p>An RBAC Role is a collection of rules that reflect a set of permissions. Permission are only used in conjunction (there are no deny rules). A Role always sets permissions within a specific namespace thus when creating a role you must specify the namespace to which it belongs.</p>

<p>A ClusterRole is a nameless resources. Because a Kubernetes object can only be namespaced or not namespaced the resources Role and ClusterRole have different names.  ClusterRoles can be used to …</p>

<ul>
  <li>Permissions on namespaced resources can be defined and granted within each namespace</li>
  <li>Permissions on named resources can be defined and granted across all namespaces</li>
  <li>Permissions on cluster-scoped resources can bed defined.</li>
</ul>

<p>TLDR: Use a Role to define a role within a namespace and a ClusterRole to define a role that spans the entire cluster.</p>

<h4 id="role-example">Role example</h4>

<p>Below is an example of a role that can be used to allow read access to pods in the “default” namespace</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">apiVersion:rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">role</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span> <span class="c1"># "" indicates the core API group</span>
    <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">pods"</span><span class="pi">]</span>
    <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">]</span>
</code></pre></div></div>

<h4 id="cluster-role-example">Cluster role example</h4>

<p>Cluster role can grant same permissions as a role but it has cluster scope.  Below are a few examples of permissions that make sense as cluster scope.</p>

<ul>
  <li>Resources with a cluster scope such as nodes</li>
  <li>Endpoints that are not resources such as <code class="language-plaintext highlighter-rouge">/healthz</code></li>
  <li>Spanning all namespaces, namespaced resources such as Pods</li>
</ul>

<p>For example a ClusterRole can allow a certain user to perform kubectl obtain pods –all-namespaces.</p>

<p>Below is an example of a ClusterRole used to allow read access to secrets across all namespaces.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="c1"># "namespace" omitted since ClusterRoles are not namespaced</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">secret-reader</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
    <span class="c1">#</span>
    <span class="c1"># at the HTTP level, the name of the resource for accessing Secret</span>
    <span class="c1"># objects is "secrets"</span>
    <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">secrets"</span><span class="pi">]</span>
    <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">]</span>
</code></pre></div></div>

<h3 id="role-binding-and-clusterrole-bindin">Role binding and ClusterRole bindin</h3>

<p>A role binding assigns a user or group of users the permissions indicated in a role. It contains a list of topics (users, groups, or service accounts) and a reference to the role that is being assigned. A valid path segment name must be used as the name of a roleBinding or ClusterRoleBinding object.</p>

<h4 id="role-binding-examples">Role binding examples</h4>

<p>Within the “default” namespace, below is an example of a roleBinding that grants the “pod-reader” Role to the user “jane”.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="c1"># This role binding allows "jane" to read pods in the "default" namespace.</span>
<span class="c1"># You need to already have a Role named "pod-reader" in that namespace.</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">RoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">read-pods</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="c1"># You can specify more than one "subject"</span>
<span class="pi">-</span> <span class="na">kind</span><span class="pi">:</span> <span class="s">User</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">jane</span> <span class="c1"># "name" is case sensitive</span>
    <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="c1"># 'roleRef' specifies the binding to a Role / ClusterRole</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span> <span class="c1"># this must be Role or ClusterRole</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader</span> <span class="c1"># this must match the name of the Role or ClusterRole you wish to bind to</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
</code></pre></div></div>

<p>A ClusterRole can be referenced by a RoleBinding to provide the rights described in that ClusterRole to resources inside the RoleBinding’s namespace. This type of reference enables you to define a set of shared roles for your cluster and then reuse them across multiple namespaces.</p>

<h4 id="clusterrolebinding-example">ClusterRoleBinding example</h4>

<p>A <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code> can be used to provide permissions across an entire cluster. Any user in the group <code class="language-plaintext highlighter-rouge">manager</code> can read secrets in any namespace using the <code class="language-plaintext highlighter-rouge">ClusterRoleBinding</code> below.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="c1"># This cluster role binding allows anyone in the "manager" group to read secrets in any namespace</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">read-secrets-global</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">kind</span><span class="pi">:</span> <span class="s">Group</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">manager</span> <span class="c1"># Name is case sensitive</span>
    <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">secret-reader</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
</code></pre></div></div>

<p>You cannot change the Role or ClusterRole that a binding refers to once created. You will get a validation error if you try to alter the roleRef of a binding. If you want to update a binding’s roleRef, you must first remove the binding object and construct a replacement.</p>

<h3 id="creating-a-service-account">Creating a service account</h3>

<p>Kubernetes service accounts are resources generated and managed using the Kubernetes API. They are used to authenticate in-cluster Kubernetes-produced entities like Pods to the Kubernetes API server or external services.</p>

<p>A service account in Kubernetes is an account that container processes in pods use to authenticate with the Kubernetes API. If your pods need to communicate with the API, you can restrict their access via service accounts. Service accounts use role-based access control objects for access control, just like any other user account. Therefor we can use ClusterRoles or ClusterRoleBindings to bind service accounts to give Kubernetes API functionality to our pods that use those service accounts.</p>

<h3 id="inspecting-pod-resource-usage">Inspecting Pod Resource Usage</h3>

<h4 id="kubectl-top">kubectl top</h4>

<p>The kubectl top command shows current CPU and memory usage for all pods or nodes in a cluster or a single pod or node if one is specified.  For example to see resource utilization across nodes use the command <code class="language-plaintext highlighter-rouge">kubectl top node</code></p>

<p>Another good example of kubectl top usage: <code class="language-plaintext highlighter-rouge">kubectl top pod -n beebox-mobile --sort-by-cpu --selector app=auth</code></p>

<h4 id="kubernetes-metrics-server">Kubernetes metrics server</h4>

<p>You can install kubernetes metric server with the command <code class="language-plaintext highlighter-rouge">kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml%20-0%20metric-server-components.yaml</code> (THe address here is probably wrong).  You can then validate that the metric server is running with the command <code class="language-plaintext highlighter-rouge">kubectl get --raw /apis/metrics.k8s.io/</code></p>

<h2 id="kca-book-chapter-5-notes-pods-and-container">KCA Book chapter 5 notes (pods and container)</h2>

<h3 id="kubernetes-pods">Kubernetes Pods</h3>

<p>A Pod is the lowest deployment and management unit in Kubernetes.</p>

<h3 id="managing-application-configuration">Managing Application Configuration</h3>

<p>You will probably at some point want to pass dynamic values to your applications at runtime to control how they behave. This process is known as application configuration, where you are just passing data to containers that control what they do and how they run.</p>

<ol>
  <li>
    <p>A file that defines the configuration for a Kubernetes object is known as an object <strong>configuration file</strong> or just <strong>configuration file</strong>. Configuration files are often kep in source control system like Git.</p>
  </li>
  <li>
    <p>Live object configuration or <strong>live configuration</strong> is the values of an objects live configuration as monitored by the Kubernetes cluster. These are often saved in the Kubernetes cluster storage <code class="language-plaintext highlighter-rouge">etcd</code>.</p>
  </li>
  <li>
    <p>A declarative configuration writer or declarative writer is a human or software component that modified a live object.</p>
  </li>
</ol>

<h3 id="configmaps">ConfigMaps</h3>

<p>A <strong>ConfigMap</strong> is simply a Kubernetes object representing a key-value store of configuration data. If you have configuration that you need to pass into your pods and your containers it is often a good idea to store those configuration in a ConfigMap that would offer you a centralized place to store your configuration data within the Kubernetes cluster. Once you have that key-value data inside your ConfigMap, you can pass it into your containers so they can use it. If you need to update those configurations, it is just a matter of updating the ConfigMap and potentially restarting any pods using that data.</p>

<p>There are two primary ways to store configuration data in Kubernetes: ConfigMap and Secrets. Secrets are very similar to ConfigMpas, but they are designed to store sensitive data securely.</p>

<h3 id="environment-variables">Environment Variables</h3>

<p>You can pass ConfigMap and secret data to containers using environment variables and those variables are visible at runtime to the container process. In the containers spec, you define an environment variable that you pull from a particular key within a ConfigMap, which will be passed as a container process environment variable. To set Kubernetes environment variables you may utilize the <code class="language-plaintext highlighter-rouge">env</code> or <code class="language-plaintext highlighter-rouge">envFrom</code> fields.</p>

<h3 id="configuration-volumes">Configuration Volumes</h3>

<p>The other way to pass data to containers is through configuration volumes. Within the configuration volume, configuration data from ConfigMaps or Secret is passed in the form of a mounted volume. It means that configuration data appear in files available on the container file system. Specifically, you have a file for each top-level key in the configuration data, and the contents of that file are going to be all of the data and sub-keys associated with that top-level key. To utilize a volume, describe where to mount those volumes into containers <code class="language-plaintext highlighter-rouge">in.spec.containers</code> and specify the volumes to supply for the Pod <code class="language-plaintext highlighter-rouge">in.spec.volumes[*]</code> Volumes are mounted within the image at the provided paths.</p>

<p>below is an example ConfigMay specification. You can create this using the command <code class="language-plaintext highlighter-rouge">kubectl create -f my-configmap.yml</code> and display it using <code class="language-plaintext highlighter-rouge">kubectl describe configmap my-configmap</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-configmap</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">key1</span><span class="pi">:</span> <span class="s">Hello, world!</span>
  <span class="na">key2</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">Test</span>
    <span class="s">multiple lines</span>
    <span class="s">more lines</span>
</code></pre></div></div>

<p>below is an example of a Secret specification. You need to store the secret in base 64 encryption which you can generate with <code class="language-plaintext highlighter-rouge">echo -n 'secret' | base64</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Secret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-secret</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">Opaque</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">secretkey1</span><span class="pi">:</span> <span class="s">base_64_value_goes_here</span>
  <span class="na">secretkey2</span><span class="pi">:</span> <span class="s">another_base_64_value_here</span>
</code></pre></div></div>

<p>Below is an example of secrets  being used in a Pod configuration naturally you would create this pod with the command <code class="language-plaintext highlighter-rouge">kubectl create -f env-pod.yml</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">env-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">echo</span><span class="nv"> </span><span class="s">"configmap:</span><span class="nv"> </span><span class="s">$CONFIGMAPVAR</span><span class="nv"> </span><span class="s">secret:</span><span class="nv"> </span><span class="s">$SECRETVAR"'</span><span class="pi">]</span>
    <span class="na">env</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">CONFIGMAPVAR</span>
      <span class="na">valueFrom</span><span class="pi">:</span>
        <span class="na">configMapKeyRef</span><span class="pi">:</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">my-configmap</span>
          <span class="na">key</span><span class="pi">:</span> <span class="s">key1</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">SECRETVAR</span>
      <span class="na">valueFrom</span><span class="pi">:</span>
        <span class="na">secretKeyRef</span><span class="pi">:</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">my-secret</span>
          <span class="na">key</span><span class="pi">:</span> <span class="s">secretkey1</span>
</code></pre></div></div>

<p>Below is an example of exposing data using configuration volumes. You an create this pod with <code class="language-plaintext highlighter-rouge">kubectl create -f volume-pod.yml</code> and execute it with <code class="language-plaintext highlighter-rouge">kubectl exec volume-pod -- ls /etc/config/configmap</code> or <code class="language-plaintext highlighter-rouge">kubectl exec volume-pod -- cat /etc/config/configmap/key1</code> (you get the idea)</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">volume-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">while</span><span class="nv"> </span><span class="s">true;</span><span class="nv"> </span><span class="s">do</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">3600;</span><span class="nv"> </span><span class="s">done'</span><span class="pi">]</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">configmap-volume</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/config/configmap</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secret-volume</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/etc/config/secret</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">configmap-volume</span>
    <span class="na">configMap</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">my-configmap</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secret-volume</span>
    <span class="na">secret</span><span class="pi">:</span>
      <span class="na">secretName</span><span class="pi">:</span> <span class="s">my-secret</span>
</code></pre></div></div>

<h3 id="managing-container-resources">Managing container resources</h3>

<p>Kubernetes has the ability to specify the resource requirements of each container. A containers memory and CPU requirements and resource limits can be defined in the spec.</p>

<h4 id="resource-requests">Resource Requests</h4>

<p>When you specify a resource request for a Pod’s containers, the kube-scheduler utilized this information to choose which node to deploy the Pod on. When you establish a resource limit for a container, the kubelet enforces those limitations preventing the running container from using more of that resource than the limit you specified. the kubelet additionally reserve the requested quantity of that system resource for that container’s exclusive usage. It is important to note the containers can use more than the requested resources. The resource requests do not force the container to stay within that limit, as resource requests only affect scheduling. Resource requests allow you to specify the number of necessary resources to run a container. They govern which worker nodes the containers should be scheduled on. When Kubernetes is getting ready to run a particular pod, it chooses a worker node based on the resource requests of that Pod’s containers. Kubernetes uses those values to choose a node with enough resources to run that Pod. If the Pod node has enough resources available, a container may (and is permitted) utilize more resources than its requested for that resource specifies. On the other hand, a container cannot exceed its resource limit.</p>

<p>Below is an example of setting resource limits in a spec.  Memory is measured in bytes, and CPU is measured in CPU units which are 1/1000 of one CPU.  In example below 250m or milli-CPUs is being requested which equates to 25% of one CPU.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">250m"</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>
</code></pre></div></div>

<h4 id="resource-limits">Resource Limits</h4>

<p>Resource limits provide us with a way to limit the amount of resources containers can use and stop containers from using more resource thant hey should. It is important to note that the container runtime is responsible for enforcing resource limits when you use resource limits. Different containers runtimes do this in different ways; for example, some runtimes might enforce resource limits by terminating the container process.If you are using resource limits your containers may get stopped if they attempt to use more resources then specified in the limit.</p>

<p>Resource limits allow you to put some constraints on how many resources you container can use and to prevent certain containers from consuming a bunch of resources and running away with all the resources in your cluster, potentially causing issues for other containers and applications. If you specify a 4GiB memory limit ofr that container, the kubelet (and container runtime) enforces it. The container is prevented from exceeding the set resource limit by the runtime. For example, suppose a process in the container attempts to use more memory than is permitted. In that case, the system kernel end the process with an Out-Of-Memory (OOM) error.</p>

<p>Below is an example of what resource limits look like in a container spec.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="s">-name</span><span class="pi">:</span> <span class="s">busybox</span>
  <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">limits</span><span class="pi">:</span>
      <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">250m"</span>
      <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>
</code></pre></div></div>

<h3 id="monitoring-container-health-with-probes">Monitoring Container Health with Probes</h3>

<h4 id="container-health">Container health</h4>

<p>Kubernetes has the ability to restart unhealthy containers. To make this work Kubernetes needs to accurately determine the status of applications.</p>

<h4 id="pod-lifecycle">Pod lifecycle</h4>

<p>Pods have a specified lifetime that begins with the <strong>Pending</strong> phase and progresses to the <strong>Running</strong> phase if at least one of its primary containers starts normally. then comes the <strong>Succeeded</strong> or <strong>Failed</strong> phases, depending on whether any container in the Pod failed.</p>

<p>While a Pod is operation, the kubelet can restart containers to manage various issues. Kubernetes tracks different container statuses within a Pod and determines the action to to take to restore the Pod’s health. Pods have a specification and a real status in the Kubernetes API. A Pod object’s state comprises a set of Pod conditions. If your application requires it, you may integrate specific readiness information into the condition data. Pods can only be scheduled once in their lives. When a Pod is scheduled (assigned) to a Node, it runs on that Node until it is stopped or terminated.</p>

<h4 id="pod-lifetime">Pod lifetime</h4>

<p>Kubernetes uses a higher-level abstraction known as a <strong>controller</strong> to manage the relatively disposable Pod instances. Pods are generated, given a Unique ID (UID), and allocated to nodes, where they remain until tey are terminated (according to the restart policy) or deleted. A given Pod (as specified by a UID) is never “rescheduled” to a different node. Instead, Pod can be replaced by a new and nearly identical one with a different UID. When a volume is said to have the same lifetime as a Pod it means that the object lives for as long as that precise Pod (with the exact UID). If that Pod is removed for any reason, the linked item (volume in this case) is destroyed and recreated.</p>

<h4 id="container-states">Container states</h4>

<p>Kubernetes maintains the status of each container within a Pod in addition to the overall phase of the Pod. Con-tainer lifecycle hooks can trigger events at certain stages in a containers’s lifespan. Once a Pod is assigned to a node by the scheduler, the kubelet begins building containers for that Pod using a container runtime. Containers can be in one of three states: waiting, Running or Terminated. Each state has a distinct meaning.</p>

<ul>
  <li>
    <p>Waiting - A container in the waiting state is still doing the actions required for a successful startup, such as retrieving the container image from a container image registry. Kubectl will shwo you the reason a container is in waiting state when you <code class="language-plaintext highlighter-rouge">kubectl describe pod pod-name</code></p>
  </li>
  <li>
    <p>Running - This status means container is running normally. If a postStart hook was configured, it was al-ready run and completed.</p>
  </li>
  <li>
    <p>Terminated - This state means the container either completed or failed for some reason. When use use kubectl to query a Pod with a terminated container, you will see a reason, an exit code, and the start and end times for the container’s runtime duration. If a preStop hook is configured on a container, it is executed before the container reaches terminated state.</p>
  </li>
</ul>

<h4 id="container-probes">Container Probes</h4>

<p>A probe may be used to inspect a container using one of the following mechanisms. A probe is a diagnostic done on a container regularly by Kubernetes.</p>

<ul>
  <li>
    <p>Exec - Inside the container, it executes a specific command. If the command exists with a status code of 0, the diagnostic is considered successful</p>
  </li>
  <li>
    <p>gRPC - Uses gRPC to make a remote procedure call, and gRPC health checks should be implemented by the target. If the status of the answer is SERVING, the diagnostic is regarded as successful. gRPC probes are an alpha feature that may be accessed only by enabling the gRPC ContainerProbe feature gate.</p>
  </li>
  <li>
    <p>httpGet - Executes an HTTP GET request on a given port and route against the Pod’s IP address. The diagnostic is considered successful if the answer is more than or equals 200 but less than 400.</p>
  </li>
  <li>
    <p>tcpSocket - Performs a TCP check on a given port against the Pod’s IP address. If the port is open, the diagnostic is considered successful. It is considered healthy if the distant system (the container) instantly shuts the connection once it opens.</p>
  </li>
</ul>

<h4 id="types-of-probes">Types of probes</h4>

<p>On a running container, the kubelet can optionally execute and respond to three types of probes:</p>

<ul>
  <li>
    <p>LivenessProbe - Liveness probes allow us to automatically determine whether or ot a container application is in a healthy state. By default, Kubernetes only considers a container to be down or broken if the container process stops, but liveness probes can allow us to customize this detection mechanism and make it more sophisticated. A container could be running, and the process has not stopped, but things are still broken. With liveness probes you can detect more sophisticated and subtle situations where things are not working, but the container is still running.</p>
  </li>
  <li>
    <p>StartupProbe - These are very similar to liveness probes, but they constantly run on a schedule. Startup probes only run at container startup and stop once they succeed. Essentially, startup probes detect when the application has successfully started up. They monitor container health during hte startup process. Startup probes are useful for legacy applications with long startup times. If a StartupProbe is used, all other probes are blocked until the StartupProbe succeeds. If a startup probe fails, the kubelet terminates the container and subjects it to the restart policy.</p>
  </li>
  <li>
    <p>ReadinessProbe - Applications are occasionally unavailable to serve traffic for a short period of time. For example, during startup, an application may need to load massive data or configuration files, or it may need to rely on external services. You do not want to destroy the application in this situation, but you also do not want to send it requests. To detect and mitigate these issues, Kubernetes provides readiness probes. A Pod with container reporting that they are not ready does not receive traffic.</p>
  </li>
</ul>

<h3 id="building-self-healing-pods-with-restart-policies">Building self-healing Pods with Restart Policies</h3>

<h4 id="restart-policies">Restart Policies</h4>

<p>There are 3 possible restart policies in Kubernetes: Always, OnFailure, and Never. The default setting is Always and the restart policy is specified in the Pod spec. All containers in the Pod are affected by the restartPolicy.</p>

<p><strong>Always</strong> Wit this policy containers are always restarted if they stop or become unhealthy. This policy should be used for an application that should essentially just always be running.</p>

<p><strong>OnFailure</strong> If you have some software designed to run once to successful completion and then stop, and it does not need to be run again you want to use the OnFailure policy. This policy automatically restarts an application if it fails, but will not be restarted if it succeeds or finishes.</p>

<p><strong>Never</strong> With this restart policy no matter what happens, if a container completes successfully, it will never be restarted. It would be best to use this for applications that only run once and never restart automatically.</p>

<h3 id="creating-multi-container-pods">Creating Multi Container Pods</h3>

<h4 id="multi-container-pod">Multi Container Pod</h4>

<p>In a multi-container pod, the containers share resources such as network and storage that can interact, working together to provide functionality.</p>

<h4 id="cross-container-interaction">Cross Container Interaction</h4>

<p>Once you have multiple containers in the same pod, how do they interact with each other? Containers in the same Pod can communicate with each other using shared resources. One such shared resources is the network so containers in the same Pod can communicate with each other on any port event if that port is not exported to the cluster. They can also share storage and use same volumes to share data within a Pod.</p>

<h4 id="shared-volumes-in-kubernetes-pod">Shared Volumes in Kubernetes Pod</h4>

<p>A shared volume is a siple and effective way to transfer data amongst containers in a Pod in Kubernetes. Data on Kubernetes Volumes can survive container restarts, but theses volumes have the same lifetime as the Pod.</p>

<h4 id="inter-container-network-communication">Inter-Container Network Communication</h4>

<p>Containers in a Pod can be accessing using “localhost” and share the same network namespace. In addition, for containers the observable hostname is the name of the Pod. Because containers share the same IP address and port space you need to coordinate port usage for apps within a Pod.</p>

<h4 id="inter-process-communication-ipc">Inter Process Communication (IPC)</h4>

<p>Containers in a Pod share the same IPC namespace thus they are able to interact via normal inter-process communication such as SystemV semaphores or POSIX shared memory.</p>

<h4 id="multi-container-pods-use-case">Multi-Container Pods Use Case</h4>

<h3 id="introduction-to-init-containers">Introduction to Init Containers</h3>

<p>An init container in Kubernetes is the one that starts and executes before other containers in the same Pod. Its purpose is to carry out initialization logic for the primary application hosted on the Pod. Create the appropriate user credentials, make database migrations, and design schemas. You can have more than one init containers in a Pod. Before the next init container can begin, the previous one must finish successfully. If a Pod’s init container fails the Pod wil be continuously restarted till it succeeds unless you have a restart policy of Never in which case the init container failing will fail the whole Pod. If you provide several init containers for a Pod, kubelet executes each init container in the order specified before the next init container may execute the previous one must succeed.</p>

<h4 id="how-init-containers-differ-from-regular-containers">How Init Containers Differ From Regular Containers</h4>

<p>All app container fields and features, including resource limitations, volumes, and security settings, are supported by init containers. The resource demands and limits for an init container, on the other hand, are handled differently. In addition because they must run to completion before a Pod may start, init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe.</p>

<h4 id="detailed-behavior-of-init-containers">Detailed behavior of init containers</h4>

<p>Kubelet pauses launching init containers at Pod startup until networking and storage are available. All init containers must execute again if a Pod is restarted.</p>

<h4 id="use-cases-for-init-containers">Use cases for init containers</h4>

<ul>
  <li>Pause a pod to wait for another service to become available. For example you can use an init container to pause startup until a service or another pod becomes available.</li>
  <li>Perform sensitive startups steps (such as loading secrets/certificates) outside of app containers.</li>
  <li>Populate data into a shared volume at startup.</li>
  <li>Communicate with another external service for example if you need to register Pod with some external service.</li>
</ul>

<h4 id="the-yaml-spec-for-an-init-containers">The yaml spec for an init containers</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">init-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.19.1</span>
  <span class="na">initContainers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">delay</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sleep'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">30'</span><span class="pi">]</span>
</code></pre></div></div>

<h2 id="chapter-6-advanced-pod-allocation">Chapter 6: Advanced Pod Allocation</h2>

<h3 id="intro">Intro</h3>

<p>The kube-scheduler in Kubernetes is in charge of scheduling pods to specific nodes in the cluster. By default nodes are picked based on each container’s resource demands and restrictions in the produced Pod. While scheduling pods based on resource limits works in many cases sometimes you want to assign ods to specific nodes. This is where advanced pod scheduling comes in.</p>

<h2 id="exploring-k8s-scheduling">Exploring K8s Scheduling</h2>

<h3 id="scheduling">Scheduling</h3>

<p>The kube-scheduler does the scheduling. The scheduler does the scheduling based on 1. whether or not the resources requests specified in a pod can be fulfilled by the available resources in a node. 2. Different configuration that affect the label of the node and nodeSelector specified in a pod to select a suitable node for it.</p>

<h4 id="nodeselector">nodeSelector</h4>

<p>nodeSelector is a field in the pod descriptor file to specify the node(s) on which it is to be run. The node’s label is used in the nodeSelector field to select and schedule a specif node.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">nodeSelector</span><span class="pi">:</span>
    <span class="na">special</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.19.1</span>
</code></pre></div></div>

<h4 id="nodename">nodeName</h4>

<p>If you want to bypass regular scheduling mechanisms you can it by using the name of a node rathern thatn the label of that node.  Under spec in the pod descriptor file, nodeName is the field used to specify the node’s name to select.</p>

<h2 id="using-daemonsets">Using DaemonSets</h2>

<p>A <strong>DaemonSet</strong> is an object runs a copy of a pod on each node automatically. When new node are added to the cluster, pod copies are added. If you delete a DaemonSet, all the copies of pods that DaemonSet created will also be deleted.</p>

<p>Example of a DaemonSet descriptor file</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">DaemonSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-daemonset</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-daemonset</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">my-daemonset</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.19.1</span>
</code></pre></div></div>

<p>In a simple case, one DaemonSet could be used for each type of daemon, covering all nodes. In a comlex case, multiple DaemonSets could be sued for a single type of daemon with different minimum requirements of the CPU and memory usage for different types of hardware.</p>

<h3 id="advanced-scheduling-and-pod-affinity-and-anti-affinity">Advanced Scheduling and Pod Affinity and Anti Affinity</h3>

<p>You can establish rules for how pods should be arranged in relations to other pods using pod affinity and anti affinity. Custom labels on nodes and label selectors supplied in pods are used to define the rules.</p>

<h2 id="using-static-pods">Using Static Pods</h2>

<p>Static pods are used when you want tot urn them on a node without the involvement of the control plane. You cannot manage these static pods through the k8s API server. These static pods are managed by kubelet instead of by the K9s API server.</p>

<p>To set up a static pod…</p>

<ol>
  <li>Log into a worker node</li>
  <li>Create the manifest file in the location <code class="language-plaintext highlighter-rouge">/etc/kubernetes/manifests/beebox-diagnostic.yml</code></li>
</ol>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">beebox-diagnostic</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">beebox-diagnostic</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">acgorg/beebox-diagnostic:1</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<ol>
  <li>Start the static pod by restarting kubelet to get it to pick up the static config file. <code class="language-plaintext highlighter-rouge">sudo systemctl restart kubelet</code> Once that is done you should be able to see the status of the static pod via the <code class="language-plaintext highlighter-rouge">kubectl get pods</code> command. The static pod you see when you run <code class="language-plaintext highlighter-rouge">kubectl get pods</code> is just a mirror of the static pod and if you try to <code class="language-plaintext highlighter-rouge">kubectl delete pod beebox-diagnostic-k8s-worker1</code> it will just re-appear because the API has no control over it.</li>
</ol>

<h2 id="chapter-7-deployments">Chapter 7 Deployments</h2>

<h3 id="k8s-deployments-overview">K8s Deployments Overview</h3>

<h4 id="what-is-a-deployment">What is a Deployment?</h4>

<p>A Deployment is a Kubernetes object described in a yaml descriptor file. It is used to create replicas of a pod. For example you would use a deployment when you wish to create five pods with similar specifications. You do not need to create each pod individually. Deployment is used to define the desired state for replicas of the pods. The deployment controller changes the original state to the desired state and maintains the desired state by deleting, creating, and replacing pods with new configurations.</p>

<p>Example of descriptor file for a deployment.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-deployment</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">my-deployment</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.19.1</span>
      <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<h4 id="desired-state">Desired State</h4>

<p>The number of replicas of the pods, a selector, and a template are all considered the desired state.</p>

<ul>
  <li>
    <p><strong>Replicas</strong> - Number of pod replicas managed by the deployment</p>
  </li>
  <li>
    <p><strong>Selector</strong> - A selector used to identify the replica pods based on their labels, to be managed by the deployment</p>
  </li>
  <li>
    <p><strong>Template</strong> - A pod definition template used to create replica pods</p>
  </li>
</ul>

<h4 id="benefits-of-kubernetes-deployment">Benefits of Kubernetes Deployment</h4>

<p>Since a Kubernetes deployment controller constantly monitors the status of pods and nodes, it can replace a failing pod or bypass down nodes, ensuring that vital applications continue to run.</p>

<p>Deployments have several use cases, such as:</p>

<ol>
  <li>An applications can easily be scaled up or down by increasing or decreasing the number of replicas.</li>
  <li>Rolling updates can be performed by deploying a newer version of the application.</li>
  <li>An application acn be rolled back to a previous version</li>
  <li>A new state of the pods can be declared by updating the specifications of the pod template. Pods are moved from the old replicaSet to the new replicaSet at a controlled rate by deployment.</li>
  <li>The rollout of a deployment can be paused to make multiple changes or fixes to its template specifications and then resumed after that.</li>
  <li>Find out if a rollout has stuck by using the deployment status(progressing, complete, or fail to progress as an indicator)</li>
  <li>Old replicaSets that are not needed anymore can be removed.</li>
</ol>

<h4 id="deployment-strategies">Deployment Strategies</h4>

<h5 id="recreate-deployment">Recreate Deployment</h5>

<p>The recreate strategy ‘recreates’ existing pod instances by terminating them and replacing them with the new versions. This is most commonly utilized in dev environments where user impact is not an issue. Recreate completely refreshes the pods and the applications state. As a result, both the shutdown of the old deployment and the recommencement of instances of the new deployment cause downtime.</p>

<h5 id="rolling-update-deployment">Rolling Update Deployment</h5>

<p>ling update allows for a smooth gradual transition from one application version to the next. A new ReplicaSet containing the new version is launched, and replicas of the old version are terminated as replicas of the new version are launched. All of the previous version pods are eventually decommissioned and replaced by the new version.</p>

<h5 id="bluegreen-deployment">Blue/Green Deployment</h5>

<p>The Blue/Green strategy allows for a quick transition from old to new version once the new version has been tested. The new ‘green’ version is used alongside the previous ‘blue’ version. When the ‘green’ version is confirmed to be working as intended, the version label in the selector field of the Kubernetes Service object that handles load balancing is replaced. This automatically redirects traffic to the latest version. Although this method allows for a quick rollout while avoiding versioning concerns, it consumes twice the resources because both versions are active until cut-over.</p>

<h5 id="canary-deployment">Canary Deployment</h5>

<p>A smaller group of users is routed to the new version of an application operating on a smaller subset of pods to test functionality in a production environment in a canary deployment. Once testing has been completed with out errors, clones of the new version are scaled up, and the old version is gradually phased out. Canary deployments are useful for testing new functionality on a small sample of users and can be easily undone.</p>

<h4 id="rollback">Rollback</h4>

<p>All the deployment versions are kept in Kubernetes so when needed they can be rolled back. Only the template specification part of the deployment can be updated or degraded as a rolling update or rollback. Any change made to this part creates a new revision or a new deployment version.</p>

<h2 id="chapter-8-networking">Chapter 8 Networking</h2>

<h3 id="k8s-networking-architectural-overview">K8s Networking Architectural Overview</h3>

<h4 id="pods">Pods</h4>

<p>Kubernetes uses an “IP-per-pod” architecture, in which each pod is given its IP address, and all containers in a pod share the same network namespace and IP address. The IP address of each Pod is unique to the cluster. The Pod is a group of containers that share the same node’s networking and storage resources. Each Pod has its own IP address, and all of the containers in the Pod share storage, IP addresses, and port space (network namespace).  All containers in a Pod can reach each others localhost port and thus that all container in a pod must coordinate port usage.</p>

<h4 id="containers">Containers</h4>

<p>Containers networking allows containers to communicate with other containers, a host, and external networks.</p>

<h4 id="nodes">Nodes</h4>

<p>Using a Pod’s IP address, any Pod can contact another Pod. This builds a virtual network that allows Pods to connect no matter which node they are on. In this way the Kubernetes network makes nodes completely transparent. For a pod to communicate with another Pod, there is no need to know what node its running on.</p>

<p>With the exception of intentional network segmentation regulations Kubernetes imposes the following criteria on any networking implementation.</p>

<ol>
  <li>
    <p>Pods on a node can connect with all pods on all nodes without the use of NAT agents. For example, system daemons and kubelets can communicate with all pods on a node.</p>
  </li>
  <li>
    <p>Without NAT, pods in a node’s host network can communicate with pods on all other nodes.</p>
  </li>
</ol>

<h3 id="how-does-networking-in-kubernetes-work">How does networking in Kubernetes work?</h3>

<p>Because in Kubernetes the internal network is insulated from the external network, one of the issues of Kubernetes networking is dealing with how internal (east-west) and external (north-south) traffic interact. External traffic can be injected into a Kubernetes cluster in a number of different ways.</p>

<h4 id="loadbalancer">LoadBalancer</h4>

<p>In this scenario redirects all external traffic to a service and every service is assigned a unique IP address.</p>

<h4 id="clusterip">ClusterIP</h4>

<p>The default Kubernetes service for internal communication is ClusterIP. External traffic, on the other hand, can use a proxy to connect to the default Kubernetes ClusterIP service. This is important for troubleshooting services and viewing internal dashboards.</p>

<h4 id="nodeport">NodePort</h4>

<p>NodPort is a service that opens ports on nodes or virtual machines and forwards traffic form the ports to the service. This is mostly used for services that do not need to be online all of the time such as demos.</p>

<h4 id="ingress">Ingress</h4>

<p>Ingress is a load balancer that functions as a router or controller, routing traffic to services. It is useful if you wish to provide various services using the same IP address.</p>

<h3 id="cni-plugins-overview">CNI Plugins Overview</h3>

<p>Kubernetes network plugins are knows as <strong>CNI plugins</strong>. These plugins offer network communication between pods by the Kubernetes network model’s specification.</p>

<h4 id="selecting-a-network-plugin">Selecting a Network Plugin</h4>

<p>CNI plugins are compliant with the Container Network Interface (CNI) specification, designed to ensure interoperability. The CNI specification v0.4.0 is followed by Kubernetes.</p>

<p><strong>kubenet plugin</strong> uses the bridge and host-local CNI plugins to implement basic cbr0.</p>

<p>Kubernetes nodes will stay in the NotReady state until a network plugin is implemented. As a result, unless you install that network plugin, you cannot run ods in your cluster because none of the nodes will be available, and any pods you try to create will remain pending, waiting for a node to become ready.</p>

<h4 id="installing-a-network-plugin">Installing a Network Plugin</h4>

<p>The kubelet comes with a single default network plugin and a cluster-wide default network. when it first starts up, it looks for plugins, remembers what it finds, and runs the selected plugin at the relevant points in the pod’s lifespan (this is only true for Docker, as CRI manages its own CNI plugins). When utilizing kubelet plugins, there are two command-line parameters to remember.</p>

<ol>
  <li>cni-bin-dir - On initialization, kubelet looks for plugins in this directory.</li>
  <li>network plugin - The cni-bin-dir network plugin to utilize. It has to be the same name as a plugin from the plugin directory that has been probed.</li>
</ol>

<h3 id="understanding-k8s-dns">Understanding K8s DNS</h3>

<p>Kube-DNS and CoreDNS are two well known DNS solutions for configuring DNS naming rules and mapping pod and service DNS to cluster IP addresses. Kubernetes services can be identified by a name that corresponds to any number of backend pods maintained by the service. DNS has a consistent naming convention, making the addresses of multiple services easier to remember. Services can be referred to not just by their Fully Qualified Domain Name (FQDN) but also by their own names.</p>

<h4 id="how-does-kubernetes-dns-work">How does Kubernetes DNS work?</h4>

<p>You may build up a DNS system in Kubernetes using two well-supported add-ons: Core DNS and Kube-DNS. CoreDNS is a recent add-on that, as of Kubernetes v1.12, has become the default DNS server.</p>

<p>Kubernetes DNS schedules a DNS Pod and Service on the cluster, then configures the kubelets to notify individual containers to resolve DNS names using the DNS Service’s IP. A DNS name is assigned to each service defined in the cluster (including the DNS server itself)</p>

<p>The DNS system assigns domain and subdomain names to pods, ports and services, allowing the components in your Kubernetes cluster to find them. DNS-based service discovery is extremely powerful as you do not have to hard-code network information like IPs and ports into your application.</p>

<h4 id="service-dns-record">Service DNS Record</h4>

<p>A records, CNAME records, and SRV records are all supported by Kubernetes.</p>

<h5 id="a-records">A Records</h5>

<p>The most basic sort of DNS record is an A record which is used to point a domain or subdomain to a certain IP address. The domain name, the IP address used to resolve it, and the TTL in seconds are all included in the record.</p>

<h5 id="cname">CNAME</h5>

<p>A domain or subdomain can point to another hostname using CNAME records. CNAME accomplish this by using the existing A record as their value. An A record, on the other hand, resolves to a specific IP address. CNAME records can also be utilized for cross-cluster service discovery with federated services in Kubernetes. There is a common service across several Kubernetes clusters in this instance. This service is discoverable by all pods, regardless of which cluster they are in. Such an approach enabled cross-cluster services discovery, which is a large topic in and of itself.</p>

<h5 id="srv-records">SRV Records</h5>

<p>By specifying the protocol(s) and address of some services, SRV records aid service discovery. An SRV record defines the priority, weight, port, and target for a given service adn the symbolic name and transport protocol (e.g, TCP) used as part of the domain name.</p>

<h4 id="pods-dns-records">Pod’s DNS Records</h4>

<h5 id="a-records-pod">A Records (pod)</h5>

<p>If DNS is enabled, pods are given a DNS address. Pod-ip-address.my-namespace.pod.cluster.local is a record type. An entry of form 172-12-3-4.default.pod.cluster.local would be created for a pod with IP 172.12.3.4 in the namespace default and a DNS name of cluster.local.</p>

<h5 id="pods-hostname-and-subdomain-field">Pods Hostname and Subdomain Field</h5>

<p>The metadata.name value of a pod determines its default hostname. The default hostname can be changed by entering a new value in the optional hostname box. In the subdomain box, users can also choose a custom subdomain name. For example, a pod in namespace my-namespace with the hostname custom-host and the subdomain custom-subdomain will have the Fully Qualified Domain Name custom-host.custom-subdomain.my-namespace.svc.cluster.local.</p>

<h3 id="networkpolicies">NetworkPolicies</h3>

<p>A network policy is an object that lets you manage the flow of network traffic to and from your pods. This enabled you to create a more secure cluster network by isolating pods from the traffic they do not require.</p>

<h4 id="pod-selector">Pod Selector</h4>

<p>The pod selector specifies which pods in the namespace are affected by the network policy. The pod selector can use labels to choose pods.  All pods in a cluster are isolated by default</p>

<h4 id="a-network-policy-can-apply-to-ingress-traffic-egress-traffic-or-both-types-of-traffic">A network policy can apply to ingress traffic, egress traffic, or both types of traffic</h4>

<p>Ingress traffic is network traffic that enters the pod from a source outside the pod, whereas egress traffic is network traffic that leaves the pod for another location. Therefore, network policies can regulate traffic entering a pod, traffic leaving a pod, or both. The network policy determines which traffic is allowed via the from and to selectors related to entrance and egress.</p>

<h4 id="from-and-to-selectors">From and To selectors</h4>

<h5 id="from-selector">From Selector</h5>

<p>Ingress traffic will be allowed if it is selected from the selector. It determines which inbound traffic is permitted to enter the pod.</p>

<h5 id="to-selector">To Selector</h5>

<p>For the outgoing egress traffic, a “to selector” achieves the same thing. It chooses which outgoing traffic will be allowed. Depending on on whether the policy applies to egress, ingress or both, we will find our from selectors in the ingress section and our to selectors in the egress section of the network policy definition. There are multiple selector types that are available for to and from.</p>

<ol>
  <li>
    <p>We can use a pod selector to choose pods based on their labels. if we use it in conjunction with an ingress rule in our from selector, pods with that app equal db label will be able to communicate with the network policy affected pods.</p>
  </li>
  <li>
    <p>We can choose a namespace based on labels, and then the network policy will allow any traffic coming from or going to pods within those namespaces.</p>
  </li>
  <li>
    <p>The ipBlock selector can be used to choose an IP range using CIDR notation. As a result, an IP range is chosen to allow incoming and outgoing traffic.</p>
  </li>
</ol>

<h4 id="limiting-resource-usage">Limiting Resource Usage</h4>

<p>The number and capacity of resources available to a namespace are limited by resource quota. This is most commonly used to limit how much CPU, RAM or persistent disk a namespace can allocate, but it can also be used to limit how many pods, services, or volumes each namespace can have.</p>

<h4 id="network-access-restrictions">Network access restrictions</h4>

<p>Application writers can use a namespace’s network policies to limit which pods from other namespaces can access pods and ports within their namespaces. Many of the Kubernetes networking providers that are supported now adhere to network policies.</p>

<p>Quota and limit ranges can also be used to govern whether users can request node ports or load-balanced services, which can affect whether those users’ applications are visible outside of the cluster on many clusters.</p>

<p>Additional precautions such as per-node firewalls, physically separating cluster nodes to avoid cross-talk, or advanced networking policies, may be available that manage network rules on a per-plugin or per-environment basis.</p>

<h4 id="restricting-cloud-metadata-api-access">Restricting Cloud Metadata API access</h4>

<p>Metadata services are frequently exposed locally to instances by cloud platforms (AWS, Azure, GCE and so on). By default, pods running on an instance can access these APIs, which can contain cloud credentials for that node or provisioning data like kubelet credentials. This account’s credentials can be sued to elevate within the cluster or to other cloud services. Limit the permissions given to instance credentials when running Kubernetes on a cloud platform, use network policies to restrict pod access to the metadata API, and avoid using provisioning data to deliver secrets.</p>

<h4 id="default-deny">Default Deny</h4>

<p>You can use the default deny network policy to block all traffic except for what is needed to avoid potential malicious traffic. Default deny network blocks all traffic by default unless we explicitly allow network communication.</p>

<p>Example of a default deny policy below and you can apply it with the command <code class="language-plaintext highlighter-rouge">kubectl create -f default-deny-all-np.yml</code></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkPolicy</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-deny-all</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">nptest</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">podSelector</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">policyType</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Ingress</span>
  <span class="pi">-</span> <span class="s">Egress</span>
</code></pre></div></div>

<h2 id="chapter-09-services">Chapter 09: Services</h2>

<h3 id="k8s-services-overview">K8s Services Overview</h3>

<h4 id="what-is-a-service">What is a Service?</h4>

<p>Kubernetes Services is an object which can bre created as a descriptor file in a yaml format.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">MyApp</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">9376</span>
</code></pre></div></div>

<p>The Kubernetes service provides a method for uncovering applications running as a bunch of pods without the client needed to know about any of the pods.</p>

<h4 id="service-routing">Service Routing</h4>

<p>Service is an abstract layer on to of pods. When users make requests to a service; Kubernetes routes the traffic to the pods backing a services so that the load is balanced among the pods.</p>

<h4 id="endpoints">Endpoints</h4>

<p>An Endpoint is a Kubernetes object that can be defined as a yaml descriptor file.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Endpoints</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-service</span>
<span class="na">subsets</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">addresses</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">ip</span><span class="pi">:</span> <span class="s">192.0.2.42</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">9376</span>
</code></pre></div></div>

<p>Endpoints are created automatically when the service is defined with a pod(s) selector, but if the service is defined without a pod selector, you have to create an endpoint object manually.</p>

<p>Traffic is proxies to the endpoints, and endpoints are mapped to pods.</p>

<p>Endpoints are the back end entities to which services redirect traffic. For a service that courses traffic to numerous pods, each pod has an endpoint related to that service.</p>

<p>The name of the Endpoint object must be a valid DNS subdomain name which follow the below rules.</p>

<ol>
  <li>A maximum of 253 characters are allowed.</li>
  <li>Lowercase alphanumeric characters or ‘.’ or ‘-‘ are allowed.</li>
  <li>The fist and last characters must be alphanumeric.</li>
  <li>No uppercase character is allowed.</li>
</ol>

<h3 id="using-k8s-services">Using K8s Services</h3>

<h4 id="service-types">Service Types</h4>

<p>There are four types of services. The service types differ by how and where they will uncover your application.</p>

<ol>
  <li>ClusterIP</li>
  <li>NodePort</li>
  <li>LoadBalancer</li>
  <li>ExternalName (outside the scope of CKA)</li>
</ol>

<h5 id="cluster-ip-services">Cluster IP Services</h5>

<p>ClusterIP Services uncover applications inside the same network (within the cluster). They are used at the point when the clients are other Pods inside the same cluster. This type can be used to expose the deployment’s Pods inside the cluster network.</p>

<h5 id="nodeport-services">NodePort Services</h5>

<p>NodePort Services uncover applications outside of the network of the cluster. They are used at the point when the clients are other users or applications outside the cluster. This type can be used to expose the Pods externally.</p>

<h5 id="loadbalancer-services">LoadBalancer Services</h5>

<p>LoadBalancer Services uncover applications outside the cluster’s network like NodePort Services, but they utilize an external cloud-based load balancer to do this. They are used at the point when the clients are other users or applications outside the cluster and traffic is gone through the load balancer to access the service. This can be used to expose the Pods externally and only works with the cloud platform with load balancing functionalities.</p>

<h4 id="discovering-k8s-services-with-dns">Discovering K8s Services with DNS</h4>

<p>Recall that Kubernetes schedules a Pod to run a DNS service and configures the kubelet to have individual containers use that DNS service. A DNS name is assigned to each service defined in the cluster (including the DNS service itself). The DNS search list of a client Pod includes the Pod’s own namespace and the cluster’s default domain by default.</p>

<h5 id="service-dns-names">Service DNS Names</h5>

<p>The default domain of the cluster is <strong>cluster.local</strong>.</p>

<h5 id="service-dns-and-namespaces">Service DNS and Namespaces</h5>

<p>A service can be accesses from any namespace within the cluster using the fully qualified domain ame of that Service.  Consider the format <code class="language-plaintext highlighter-rouge">my-service.my-namespace.svc.cluster.local</code>. Just the services name not FQDN can be used if the pods are in the same namespace. DNS queries that do not specify a namespace are limited to the pod’s namespace.</p>

<p>You can control where a pod searches by modifying its <code class="language-plaintext highlighter-rouge">/etc/resolve.conf</code> files which is controlled by Kubelet.</p>

<h4 id="managing-access-from-outside-with-k8s-ingress">Managing Access from Outside with K8s Ingress</h4>

<p>An Ingress is a Kubernetes object that allows access to a Service from outside the cluster (Ingress itself is inside the cluster). It can be considered an advanced version of the NodePort Service. It can perform functions like SSL termination, advanced load balancing, or name-based virtual hosting. Ingress uncovers Http and Https routes to the services inside the cluster. Rules defined on the Ingress resource control the traffic routing. It does not uncover irregular or other ports or protocols except HTTP and HTTPS to the internet.</p>

<h5 id="ingress-controllers">Ingress Controllers</h5>

<p>Ingress Controllers control the Ingress because they do not do tasks by themselves. Several kinds of Ingress Controllers can be installed. They all carry out various strategies for allowing external access to the services and are not automatically started with the cluster.</p>

<h5 id="routing-to-a-service">Routing to a Service</h5>

<p>There is a set of routing rules defined by the Ingress. In the ingress specifications, under the rules, there are paths defined. Requests that match the paths will be routed to their associated backend.</p>

<h5 id="routing-to-a-service-with-a-named-port">Routing to a Service with a Named Port</h5>

<p>In case a Service uses the port’s name instead of its number, Ingress uses this port name to determine to which port it should route the requests.</p>

<h2 id="chapter-10-storage">Chapter 10 Storage</h2>

<h3 id="k8s-storage-overview">K8s Storage Overview</h3>

<h4 id="container-file-systems">Container File Systems</h4>

<p>Container files system are ephemeral. If a container stops running, data on the file system will be lost. Whenever a container is restarted, it has not previous data.</p>

<h4 id="volumes">Volumes</h4>

<p>Volumes are external storage that does not depend on the lives of the container pods. Volumes allow storing data outside the container file system accessible by the container at runtime, hence data will not be lost even after a container has been restarted or removed.</p>

<h4 id="persistentvolumes">PersistentVolumes</h4>

<p>Persistent Volumes are in clusters like nodes and outside of pods. To use a persistent volume you need to use a PersistentVolumeClaim to request the storage resource and specify how much size of storage you need and what should be the access mode such as <code class="language-plaintext highlighter-rouge">ReadWriteOnce</code>, <code class="language-plaintext highlighter-rouge">ReadWriteMany</code> or <code class="language-plaintext highlighter-rouge">ReadOnlyMany</code>. One of the difference between a Volume and a PersistentVolume is Volumes are specified and mounted within a Pod descriptor file while PersistentVolumes are API object created separately like pods.</p>

<h4 id="volume-types">Volume Types</h4>

<p>Both Volumes and PersistentVolumes have several types that determine how these storages are handled. These volume types are used as plugins.</p>

<ol>
  <li>awsElasticBlockStore - EBS</li>
  <li>azureFile - Azure File</li>
  <li>azureDisk - Azure Disk</li>
  <li>fc - Fibre channel storage</li>
  <li>nfs - Network File system (NFS) storage</li>
  <li>local - Local storage devices mounted on nodes</li>
</ol>

<h5 id="awselasticblockstore">awsElasticBlockStore</h5>

<p>The EBS restrictions apply here just as with EC2. You need to use an EC2 instance as the node on which a pod is executing. The EBS volume must also be in the same availability zone and region as the node. An EBS volume can only be mounted by a single EC2 instance at a time.</p>

<h5 id="emptydir">emptyDir</h5>

<p>When a pod is assigned to a node, an emptyDir volume is created, and it exists as long as the Pod is running on that node. The empyDir volume is, as its name implies, initially empty. The empyDir volume can be mounted at the same or different directories in each container, allowing all containers in the Pod to read and write the same files. The data in the emptyDir is permanently erased when a Pod is removed from a node for whatever reason.</p>

<p>Reasons to use emptyDir are…</p>

<ol>
  <li>If you are using disk-mased merge sort, you will need some scratch space.</li>
  <li>For crash recovery, checking in a long computation.</li>
  <li>Storing files that are fetched by a content-manager container and served by a webserver container.</li>
</ol>

<p>EmptyDir volumes are stored on whichever medium backs the node, such as disk, SSD, or network storage, depending on your setup. If you change the emptyDir.medium option to “Memory”, Kubernetes instead mounts a tmpfs (RAM-backed filesystem). While tmpfs is quick, keep in mind that, unlike disks, it is cleaned when the node reboots, and any files you write count towards your containers memory limit.</p>

<h5 id="nfs">nfs</h5>

<p>An nfs volume is used to mount an existing NFS share into a Pod. Because NFS volume lives on the NFS host data can be pre populated with an NFS volume. Multiple writers can mount NFS at the same time.</p>

<h5 id="local">local</h5>

<p>A mounted local storage device, such as a disk, partition, or direcotry, is represneted by a local volume. Local volumes can only be utilized as PersistentVolumes that have been statically built. The use of dynamic provisioning is not permitted. Local voluems, as opposed to hostPath volumes are used in a more durable and portable manner wthout the need to manually schedule pods to nodes. By looking at the node affinity on the PersistentVolume, the system is aware of the volume’s node limitations.</p>

<p>Local volumes, on the other hand, are dependent on the underlying node’s availability and are not suited for many appliations. When a node become sick the pod looses access to the local volume.</p>

<h5 id="ephemeral-volumes">Ephemeral Volumes</h5>

<p>Some applications requrie storage but are unconcerend about whether that data is retained across restarts. Caching, configuration information are examples of this use case. Ephemerla volumes are meant for this scenario. Kubernetes offers a variety of ephemeral volume options.</p>

<ol>
  <li>emtyDir</li>
  <li>CSI ephemeral volumes</li>
  <li>Generic ephemeral volumes: can be created by any storage driver that supports persistentVolumes.</li>
</ol>

<h4 id="storage-classes">Storage Classes</h4>

<p>Administrators can use a StorageClass to specify the “classes” of storage they supply. Different classes could correspond to different quality of service levesl, backup policies, or arbitratry policies set by cluster administrators. This is what we call profiles in other storage systems.</p>

<h4 id="the-storageclass-resource">The StorageClass Resource</h4>

<p>Provisioner, parameters, and reclaimPolicy are field in each storageClass that are utilized when a persistenVolume belonging to the class has to be dynamically provisioned. The name of a storageClass object is mportant because it determines how users can request a certain class.</p>

<h4 id="volume-snapshots">Volume Snapshots</h4>

<p>A VolumeSnapshot is a user reqeust for a volume snapshot. It works in the same way as a PersistentVolume Claim. VolumeSnapshtos give Kubernetes users a standerdized way to dupliate the contents of a volume at a specific moment. This may be an otption for backing up databases.</p>

<h4 id="projected-volume">Projected Volume</h4>

<p>Serverl existing volume sources are mapped into the same directory via a projected volume. The following sorts of volume sources can be projected.</p>

<ol>
  <li>secret</li>
  <li>downwardAPI</li>
  <li>configMap</li>
  <li>serviceAccountToken</li>
</ol>

<h3 id="using-k8s-volumes">Using K8s Volumes</h3>

<h4 id="sharing-volumes-between-containers">Sharing Volumes Between Containers</h4>

<p>A pod can have multiple containesr that you can run by creating a multi-container pod. A volume specified in the pod spec can be mounted on multiple containers useing volumeMounts. This is the benefit of volumeMount. Therefore, you can share a single volume between contaienrs by mounting it multipe times.</p>

<h4 id="common-volumes-types">Common Volumes Types</h4>

<ol>
  <li>hostPath - This type of voluem allows storing data in a specified location on the disk.</li>
  <li>emptyDir - covered this one above.</li>
</ol>

<h3 id="exploring-k8s-persisten-volumes">Exploring K8s Persisten Volumes</h3>

<h4 id="persistentvolumes-1">PersistentVolumes</h4>

<p>A PersistentVolume (PV) is a piece of storage in a cluster that has been provided either manually or dynamically using StorageClasses. If functions as a cluster resource in thes ame way that a node does. A PV can be resused according to a propery called <code class="language-plaintext highlighter-rouge">PersistentVolumeReclaimPolicy</code>. It reclaims a PersistenVOlume to be used again if the PersistentVOlumeClaim associated with this PV is deleted. It has the following policies to be implemented.</p>

<ol>
  <li>Retain - It keeps all the data, and the admin has to clean it up manually to make PV resusable.</li>
  <li>Delte - This only works iwhen the storage resource is provided by a cloud service. It deletes the storage automatically.</li>
  <li>Recycle - It deletes all the data in the storage and makes it reusable.</li>
</ol>

<h4 id="storageclasses">StorageClasses</h4>

<p>StorageClass has an attribute called allowVolumeExpansion that allows the expansion or resizing of volumes after they are created.</p>

<h4 id="persistentvolumeclaims">PersistentVolumeClaims</h4>

<p>Persistnt Volumes are consumed by Persistent Volume Claims much like node resources are consumed by pods.</p>

<h2 id="chapter-11-troubleshooting">Chapter 11: Troubleshooting</h2>

<h3 id="kube-api-server">Kube API Server</h3>

<p>The Kubernetes API server checks and configures data for API objects such as pods, sevices, and replication controllers. The API server handles REST operations and serves as the interface to the cluster’s share dstate, via which all other components communicate.</p>

<p>The Kubernetes API server is the primary way you interat with the Kubernets cluster. If the Kubernets API server is down, you cannot use kubectl to interact with the cluster becasue kubectl depends on the Kube API server.</p>

<p>I you Kube API server is down, some possible fixes might include ensuring that the Docker and Kubelet services are up and running on your contorl plan nodes.</p>

<p>The API server is a Kubernetes contorl plane component that exposes The Kubernets API. The Kubernetes control plane’s frontent is the API server. The main Kubernetes API server implementation is kube-apiserver. The kube-apiserver is intended to scale horizontally, i.e. by deploying extra instances. You may run many instances of kube-apiserver and balance traffic between them.</p>

<h3 id="checking-node-status">Checking Node Status</h3>

<p>You can use <code class="language-plaintext highlighter-rouge">kubectl get nodes</code> to see the overall status of each node. You can use <code class="language-plaintext highlighter-rouge">kubectl describe node</code> to get more information on any nodes, not in the ready state.</p>

<p>If a node has problems, it may be because of ubernets service is down on that node. Hence, each node runs kubelet, and you are also running docker as a service. You can use <code class="language-plaintext highlighter-rouge">systemctl status</code> on any of those services to view the tatus of a service, and if a service is down, you might just try starting it an enabling it via <code class="language-plaintext highlighter-rouge">systemctl</code>.</p>

<h3 id="checking-system-pods">Checking System Pods</h3>

<p>In a kubeadm cluster several of the Kubernetes components run as pods in the Kube system namespace. You can check the status of those components simply with kubectl get pdos and kubectl describe pod within that kube-system namespace.</p>

<h3 id="checking-cluster-and-node-logs">Checking Cluster and Node Logs</h3>

<h4 id="basics-of-kubernete-logging">Basics of Kubernete Logging</h4>

<p>Because pods might be many and short-lived, this form of log gathering is discouraged in Kubernetes. Ku-bernete suggests allowing the application to output logs to stdout and stderr. Each node runs its own Kubelet, which collects the segmented output logs and merges them into a single log file. The log files for each container will be handled automatically and limited to 10MB.</p>

<h4 id="types-of-logs">Types of Logs</h4>

<p>It shoudl be remembered that ther are two sorts of logs in Kubernetes. Ther are two types of logs: node logs and component logs. Node loga re logs created by node and the services that operate on those nodes. On the other hadn, pods, Containers, Kubernetes components, DaemonSets, and other Kubernetes Services create component logs. Each node in a kubernetes Cluster runs services to host Pods, accepts instructions, and connects with other nodes. The host operating system determines the format of those logs and where they are stored. For example, on a Linux server, you may obtain the logs by using journalctl -u kubelet; however, on other platforms, Kubernetes stores the logs in the /var/log directory by default.</p>

<p>On the other hand, component logs are collected by Kubernetes and are normally accessible using the kubernets API. The greatest example would be pods, and all communications from applications are wirtten to STDOUT and STDERR.</p>

<h4 id="kubernetes-node-logging">Kubernetes node Logging</h4>

<p>The continer engine streams evertying that a containerize application writes to stdout or stderr someplace in Docker’s instance, to a logging driver. These logs are often stored in yoru hosts /var/log/containers directory. When a container restarts, the kubelet stores logs on the node. Kubernete has a log rotation stragety to avoid logs from taking up all of the available space on the node. As a result, when a pod is evicted from a node, all connected contaienrs, along with their logs, are likewise evicted.</p>

<p>Depending on your operatin system and services, you can collect variou node-level logs, such as kernel logs or systemd logs. On systemd nodes, both the kubelet and the contaienr runtime write to journald. They write to .log files in the <code class="language-plaintext highlighter-rouge">/var/log</code> directory if systemd is not present. The journalctl command allows you to see system logs, giving you a list of log lines.</p>

<h4 id="kubernetes-cluster-logging">Kubernetes Cluster Logging</h4>

<p>Kubernetes cluster logs relate to Kubernetes and all of its system component logs, and you can distinguish between components that operate in a continer and components that do not. Each serves a distinct puprose in providing information about hte health of your Kubernetes system. Kube-scheduler, kube-apiserver, etcd, and kube-proxy, for example run within contaienrs, but kubelet and the conteiner runtime run on the operatin system level often as a systemd service.</p>

<p>System components running outside of containrs, by default, publish files to journald, whereas components running within containers write to the <code class="language-plaintext highlighter-rouge">/var/log</code> directory. However, the container engine may be configured to send logs to a specific place.</p>

<p>Kubernetes does not have native solution for cluster-level logging. Thear are however additonal optoins accessible to you:</p>

<ul>
  <li>Make use of a node-level logging agent that operates on all nodes.</li>
  <li>Within the application pod, add a sidecar container for logging.</li>
  <li>Logs may be accessed straight from the application.</li>
</ul>

<h4 id="service-logs">Service Logs</h4>

<p>You can check the logs of your Kubernetes-related services on each node using journalctl. You can do somethign like journalctl -u kubelet or journalctl -u Docker to get the logs for kubelets and Docker, respectively.</p>

<h4 id="cluster-component-logs">Cluster Component Logs</h4>

<p>Another type of log you might want to check out is cluster component logs. In a Kubernetes cluster, components redirect log output to <code class="language-plaintext highlighter-rouge">/var/log</code> For example, you could look at <code class="language-plaintext highlighter-rouge">/var/log/kube-apiserver.log</code> to see the kube API server logs. You might need to be aware of these locations when working with Kubernetes in the real world, but node that these log files do not appear for clusters created using kubeadm. You might want to be aware of these log locatiosn, but the reason those are not going to show up in a kubeadm cluster is that all of these components run as system pods. They do not run directly on the host or within containers. those log files do not sow up on the host in a kubeadm cluster, but in a kubeadm cluster, you can still access those logs by using the kubectl logs command on those sytem pods.</p>

<p><code class="language-plaintext highlighter-rouge">/var/log/kube-apiserver.log</code></p>

<p><code class="language-plaintext highlighter-rouge">/var/log/kube-scheduler.log</code></p>

<p><code class="language-plaintext highlighter-rouge">/var/log/kube-controler-manager.log</code></p>

<p>The following command can be used to check the logs for your Kubernetes services <code class="language-plaintext highlighter-rouge">sube journalctl -u kubelet</code> You can use <strong>Shift+G</strong> to jump to the end of the logs.</p>

<p>To list sytem pods use the command <code class="language-plaintext highlighter-rouge">kubectl get pods -n kube-system</code></p>

<h3 id="checking-node-status-1">Checking Node Status</h3>

<p>Suppose that the kube API server is healthy, and you can interact with the clsuter using kubectl. The next step is to check the nodes status.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get nodes</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl describe node &lt;Node-Name&gt;</code></p>

<p><code class="language-plaintext highlighter-rouge">systemctl status kubelet</code> -&gt; <code class="language-plaintext highlighter-rouge">systemctl start kubelet</code> -&gt; <code class="language-plaintext highlighter-rouge">systemctl enable kubelet</code></p>

<h3 id="checking-system-pods-1">Checking System Pods</h3>

<p>If your services are healthy, you might need to look into your system pods.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get pods -n kube-system</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl describe pod &lt;Pod-Name&gt; -n kube-system</code></p>

<h3 id="service-logs-1">Service Logs</h3>

<p>Service logs are an importatnt part of troubleshooting a Kubernets cluster. You can check the logs for your Kubernets-related services on each node using journalctl.</p>

<p><code class="language-plaintext highlighter-rouge">sudo journalctl -u kubelet</code></p>

<p><code class="language-plaintext highlighter-rouge">sudo journalctl -u Docker</code></p>

<h3 id="troubeshooting-your-application">Troubeshooting Your Application</h3>

<h4 id="checking-pod-status">Checking Pod Status</h4>

<p>You can see a pod’s status simply with the kubectl get pods command, and you can ue kubectl describe Pod to get more information about what may be hapenning with an unhealthy pod. hence, it is kubectl describe Pod followed by the name of that Pod.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get pods</code></p>

<p><code class="language-plaintext highlighter-rouge">kubectl descirbe pod &lt;Pod-Name&gt;</code></p>

<h4 id="my-pod-stays-pending">My Pod stays pending</h4>

<p>You may not have enough resources or the host port may be in use.</p>

<h4 id="my-pod-stays-waiting">My Pod stays waiting</h4>

<p>If a Pod is in the Waiting state, it means it has been assigned to a worker node but is unable to function ont hat node. Reviewing the information from <code class="language-plaintext highlighter-rouge">kubectl describe</code> can be helpful in diagnosing the issue. The most common reason for waiting pods is the inability to retrieve the image. Ther are three specific things to check for:</p>

<ol>
  <li>Make certain that the image’s name is right.</li>
  <li>make sure you have installed the image in the registry.</li>
  <li>Try manually pulling hte image to see if it can be pulled. Fore example <code class="language-plaintext highlighter-rouge">docker pull &lt;image&gt; if you use Docker on yoru PC</code></li>
</ol>

<h4 id="my-pod-is-running-but-not-doing-what-i-told-it-to-do">My Pod is running but not doing what I told it to do</h4>

<p>The first step is to remove your Pod and recreate it using the <code class="language-plaintext highlighter-rouge">--validate</code> option. Run <code class="language-plaintext highlighter-rouge">kubectl --validate -f mypod.yaml</code> as an example. If will ensure you have not issues in your manifest.</p>

<p>The next step is to determine whetehr the Pod on the apiserver corresonds to the POd you intened to build (e.g. in a yaml file on yoru local machine). <code class="language-plaintext highlighter-rouge">kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code> and then manually check the origin pod description to the one returned. There will almost always be some lines on the “apiserver” version that do not appear on that original version. that is normal. Beyond that if you find differences there is an issue with your spec.</p>

<h4 id="debugging-replication-controllers">Debugging Replication Controllers</h4>

<p>Replications controllers either can or cannot generate Pods. If they cannot build pods, follow the pod troubleshooting steps. You can also use <code class="language-plaintext highlighter-rouge">kubectl describe rc &lt;CONTROLLER NAME&gt;</code> to see replication controller events.</p>

<h4 id="debugging-services">Debugging Services</h4>

<p>Load balancing over a collection of pods is provided via services. First, ensure that the service has endpoints. The apiserver makes an endpoints resourse available for each Service object. You may access the resource by using:</p>

<p><code class="language-plaintext highlighter-rouge">kubectl get endpoitns ${Service Name}</code></p>

<p>Check that the endpoitns correspond to the number of pods you plan to be members of your service. If your Service is for an nginx continer with three replicas, you would expect to see three separate IP addresses in the service’s endpoints.</p>

<h4 id="running-commands-inside-containers">Running Commands Inside Containers</h4>

<p>You can use kubectl exec to run commands inside containers. If ther are multiple containers in a pod, use <code class="language-plaintext highlighter-rouge">-c</code> to specify the container name, and then you do two dashes, and after that the command you want to run.</p>

<p><code class="language-plaintext highlighter-rouge">kubeclt exec podname -c containername -- &lt;command&gt;</code></p>

<p>You may connect to containers within yoru cluster using <code class="language-plaintext highlighter-rouge">kubectl exec</code>. It is a omponent of kubect CLI utlity, which is used to interface with Kubernetes deployments. The exec command, like ssh or docker exec, feds a shell session int your terminal.</p>

<p><code class="language-plaintext highlighter-rouge">kubeclt exec -it demo-pod -- /bin/sh</code></p>

<p>The <code class="language-plaintext highlighter-rouge">-it</code> option is equl to the <code class="language-plaintext highlighter-rouge">--stdin</code> <code class="language-plaintext highlighter-rouge">(-i)</code> and <code class="language-plaintext highlighter-rouge">--tty</code> <code class="language-plaintext highlighter-rouge">(-t)</code> flags.</p>

<p>You d on not ahve to start a shell in the contienr; instead you may run an arbitraty process, provide it with interactive input, and recieve its output:</p>

<p><code class="language-plaintext highlighter-rouge">kubectl exec -it demo-pod --node my-script.js</code></p>

<h3 id="checking-container-logs">Checking Container Logs</h3>

<h4 id="kubectl-logs">kubectl logs</h4>

<p>Kubernete has its build-in monitoring solution for simpel debugging or rapid troubleshooting. You can read logs and view basic metrics with kubectl. You can use the kubeclt logs command to view a container’s logs. You must specify which contaienr you want to get logs for using the -c flag.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl logs podname -c container-name</code></p>

<h4 id="kubectl-logs--previous">kubectl logs -previous</h4>

<p>The <code class="language-plaintext highlighter-rouge">-previous</code> flag to get the logs from a previously operating container.</p>

<h4 id="kubectl-logs--follow">kubectl logs -follow</h4>

<p>Adding <code class="language-plaintext highlighter-rouge">-follow</code> allow syou to stream logs from the operating system in real time.</p>

<h3 id="metrics">Metrics</h3>

<h4 id="kubectl-top-1">kubectl top</h4>

<p>In Kubernetes kubectl restursn two main sets of metrics: one for pods and one for nodes. You may use kubectl top pods to see how much CPU and RAM each pod uses. You may use kubeclt top nodes to examine a node’s CPU and RAM use. These simple instructions provide a basic yet consise summary of your resource utilization.</p>

<h3 id="troubleshooting-network-issues">Troubleshooting Network Issues.</h3>

<h4 id="kube-proxy--dns">Kube-proxy &amp; DNS</h4>

<p>The kube-proxy is a network proxy that runs on each node in your cluster and implements a component of the Kubernetes Servcie concept. kube proxy keeps netwrok rules up to doat on nodes. These netwok rules enable network connectivity to your Pods from network sessions inside and outside your cluster. If the operating system has a packet filtering layer available, kube-proxy utilizes it. Otherwise, the traffic is sent by kube proxy.</p>

<p>On each node, the Kubernetes netwrok proxy is running. It represents the services described in the Kubernetes API on each node and can do basic TCP, UDP, and SCTP steram forwarding and round-robin TCP, UDP, and SCTP forwarding over a group of backends. Service cluster IPs and ports are currently discovered using Docker-links-compatible environment variables that indicate ports opened by the service proxy. An optional addon is available that offers cluster DNS for these cluster IPs. To configure the proxy, the user needs first establish a service using the apiserver API.</p>

<p>Kubernetes DNS schedules a DNS Pod and Services on teh cluster and configures the kubelets to notify individual contaienrs to resolve DNS names using the DNS Services IP address.</p>

<p>In addition to checking on things like your networking plugin in Kubernete or potentially even your underlying networking infrastructure, you might want to look at kube-proxy and the Kubernetes DNS if you are experiencing roblems with thigns communicating with one another and yoru Kubernets cluster. In a kubeadm cluster, the DNS and kube-proxy both run as pods in the kube-system namespace. Hence, you can check out those kube-system pods to see any Kubernetes DNS or issues.</p>

<h4 id="netshoot">Netshoot</h4>

<p>If you need to troubleshoot networking, sometimes you need to do that from the perspective of somethign running in the cluster. There is a particular container image that is a great tool for this. It is called nicolaka/netshoot. This image contains a variety of different networking exploration and troubleshooting tools. Crteate a contaienr runnig this image. You can then use kubectl exec to run commands inside that netshoot container and explore away within your Kubernets network.</p>

<p>Network namespaces isolate the system resources involved in networking. Docker creates an isolated invironment for each continer using networks and various namespaces (pid, mount, user). Everyting, including interfaces, route, and IP addressses, is segregated wthing the containers network namespace.</p>

<p>Kubernetes makes use of netwrok namespaces as well. The kubelets generate a network namespace for each Pod, and all containers in that pod use that netwrok namespace (eths, IP, tcp, sockets). It is a significant distinction between Docker contaienrs and Kubernetes pods.</p>

<p>The cool thign about namespaces is that they can be switched between. you can access a separate container’s netwrok namesapce and debug its network stack using tools not even installed on that container. Netshoot may also be used to troubleshoot the host by utilizing the host’s network namespace.</p>

<p>If you with to create a temporary contienr for debugging purposes.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl run tmp-shell -rm -i -tty --image nicolaka/netshoot -- /bin/bash</code></p>

<p>If you wish to start a container in the network namespace of the host.</p>

<p><code class="language-plaintext highlighter-rouge">kubectl run tmp-shell --rm -i --tty --overrides='{"spec":{"hostNetwork":tunnel}}' --image nicolaka/netshoot -- /bin/bash</code></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">My References</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">My References</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
