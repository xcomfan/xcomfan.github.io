<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Service Discovery | My References</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Service Discovery" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<link rel="canonical" href="http://0.0.0.0:4000/kubernetes/service_discovery" />
<meta property="og:url" content="http://0.0.0.0:4000/kubernetes/service_discovery" />
<meta property="og:site_name" content="My References" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Service Discovery" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","headline":"Service Discovery","url":"http://0.0.0.0:4000/kubernetes/service_discovery"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:4000/feed.xml" title="My References" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">My References</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Service Discovery</h1>
  </header>

  <div class="post-content">
    
<h2 id="what-is-service-discovery">What is Service Discovery</h2>

<p>Kubernetes is a dynamic system where Pods are placed on nodes, and the numbers of Pods running can vary based on load. This makes it easy to run a lot of things but you also need to be able to find those running things. This class of problem is referred to as <strong>service discovery</strong>. Service discovery tools help solve the problem of finding which processes are listening at which addresses for which services.</p>

<h2 id="the-service-object">The Service object</h2>

<p>In Kubernetes service discovery starts with a <strong>Service object</strong>. A Service object is a way to create a named label selector, but it has some other functionality.</p>

<p>To imperatively create a Service object use the command similar to <code class="language-plaintext highlighter-rouge">kubectl expose deployment alpaca-prod</code> (assuming you have the alpaca-prod) deployment already running. The <code class="language-plaintext highlighter-rouge">kubectl expose</code> command will pull both the label selector and the relevant ports from the deployment definition to set up the service. It will also assign a virtual IP (called <strong>cluster IP</strong>) to the service. This is a special IP address which Kubernetes will use to load balance across all the Pods that are identified by the selector. This process of having Pods that match a selector get load balanced in the cluster IP is the service discovery mechanism (just my take away need to confirm)</p>

<p>You can get a list of services with the command <code class="language-plaintext highlighter-rouge">kubectl get services</code></p>

<h2 id="service-dns">Service DNS</h2>

<p>Because the cluster IP is virtual, it is stable and it is appropriate to give it a DNS address. Kubernetes provides a DNS service exposed to Pods running in the cluster and provides DNS names to cluster IPs.</p>

<p>An example of the DNS name and how it breaks down is <code class="language-plaintext highlighter-rouge">alpaca-prod.default.svc.cluster.local</code> where…</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">alpaca-prod</code> is the name of the service in questions</li>
  <li><code class="language-plaintext highlighter-rouge">default</code> is the namespace that the service is in</li>
  <li><code class="language-plaintext highlighter-rouge">svc</code> is for recognizing that this is a service</li>
  <li><code class="language-plaintext highlighter-rouge">cluster.local</code> is the base domain name for the cluster</li>
</ul>

<p>When referring to a service in your own namespace you can just use the service name. You can also refer to a service in another namespace by having the namespace in the specified name (<code class="language-plaintext highlighter-rouge">alpaca-prod.default</code> for example) or you can use the FQDN.</p>

<h2 id="readiness-checks">Readiness Checks</h2>

<p>One nice things the Service object does is track if a Pod is ready to accept requests or not. This is useful if your applications needs some time to initialize when it starts up. This is the readiness check functionality. You can specify the readiness check in your manifest file with code similar to</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">speck</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="s">template</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
      <span class="s">...</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">alpaca-prod</span>
      <span class="na">readinessProbe</span><span class="pi">:</span>
        <span class="na">httpGet</span><span class="pi">:</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/ready</span>
          <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">2</span>
        <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">0</span>
        <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">3</span>
        <span class="na">successThreshold</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p>In the example above the readiness check will look for successful GET request to <code class="language-plaintext highlighter-rouge">/ready</code> endpoint on port 8080. It will check every 2 seconds starting as soon as the Pod comes up. If three successive checks fail then the Pod will be considered not ready. If one checks succeeds the Pod will again be considered ready. Only ready pods are sent traffic. This functionality is not just useful at startup its also a good way for an overloaded or sick server to signal to the system that it doesn’t want to receive traffic anymore, and is a good way to implement a graceful shutdown. Server can signal it no longer wants to receive traffic complete all jobs or connections and shut down.</p>

<h2 id="exposing-a-service-outside-a-cluster">Exposing a service outside a cluster</h2>

<p>You can use the <code class="language-plaintext highlighter-rouge">NodePort</code> functionality to have your service be accessible from any cluster node. Essentially if you specify <code class="language-plaintext highlighter-rouge">spec.type</code> as <code class="language-plaintext highlighter-rouge">NodePort</code> or use <code class="language-plaintext highlighter-rouge">--type=NodePort</code> when calling expose command Kubernetes will assign a port to your service that can be used from any cluster node to access the service.</p>

<p>The Service object operates at Layer 4 of the OSI model which means that it only forward TCP and UDP connections and doesn’t look inside of those connections.</p>

<h3 id="cloud-integration">Cloud Integration</h3>

<p>If your cloud supports it, you can use the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> type which builds on the <code class="language-plaintext highlighter-rouge">NodePort</code> concept. Essentially the cloud provider will create a load balancer and direct it at nodes in your cluster. This functionality is cloud provider specific, but is a way to get your application exposed/usable to the world.</p>

<h3 id="endpoints">Endpoints</h3>

<p>Some applications (and the system itself) want to be able to use services without a cluster IP. This can be done with the <strong>Endpoints</strong> object. For every Service object Kubernetes creates a buddy Endpoints object that contains the IP address of that service. You can view the Endpoints object with the command <code class="language-plaintext highlighter-rouge">kubectl describe endpoints alpaca-prod</code>. To use a service an application that is aware of Endpoints (likely an application written to work with Kubernetes) can talk to Kubernetes API directly to look up endpoints and call them. The Kubernetes API even has the capability to “watch” objects and be notified as soon as they change. This allows the client to react immediately as soon as the IPs associated with a service change. Most applications don’t use this and just use stable IP addresses that don’t change often, but the alternative is there.</p>

<h3 id="kube-proxy-and-cluster-ips">kube-proxy and cluster IPs</h3>

<p>Cluster IPs are stable virtual IPs that load-balance traffic across all of the endpoints in a service. This is performed by a component running on every node in the cluster called <code class="language-plaintext highlighter-rouge">kube-proxy</code>. <code class="language-plaintext highlighter-rouge">kube-proxy</code> watches for new services in the cluster via the API server and then programs a set of <code class="language-plaintext highlighter-rouge">iptables</code> rules in the kernel of that host to rewrite the destinations of packets so they are directed at one of the endpoints for that service. If the set of endpoints for a service changes (due to Pods coming and going due to failed readiness checks) the set of <code class="language-plaintext highlighter-rouge">iptables</code> rules is rewritten.</p>

<p>The cluster IP itself is usually assigned by the API server as the service is created, however when creating the service the user can specify a specific cluster IP. Once set the cluster IP cannot be modified without deleting and recreating the Service object.</p>

<h2 id="connecting-with-other-environments">Connecting with other environments</h2>

<p>When you are connecting Kubernetes to legacy resources outside of the cluster, you can use selector-less services to declare a Kubernetes service with a manually assigned IP address that is outside of the cluster. That way, Kubernetes service discovery via DNS works as expected, but the network traffic itself flows to an external resource.</p>

<p>Connecting external resources to Kubernetes services is somewhat trickier. If your cloud provider supports it, the easiest thing to do is to create an “internal” load balancer that lives in your virtual private network and can deliver traffic from a fixed IP address into the cluster. You can then use traditional DNS to make this IP address available to the external resource. Another option is to run the full kube-proxy on an external resource and program that machine to use the DNS server in the Kubernetes cluster. Such a setup is significantly more difficult to get right and should really only be used in on-premise environments. There are also a variety of open source projects (for example, Hashicorp’s Consul) that can be used to manage connectivity between in-cluster and out-of-cluster resources.</p>

<h2 id="http-load-balancing-with-ingress">HTTP Load Balancing with Ingress</h2>

<p>There are some challenges with using <code class="language-plaintext highlighter-rouge">NodePort</code> based Service object to expose your service outside of the cluster. For one since <code class="language-plaintext highlighter-rouge">NodePort</code> Service objects operate at Layer 4, you will need to have clients connecting on to a unique port per service. If you use a <code class="language-plaintext highlighter-rouge">LoadBalander</code> based Service object you will need to allocate an expensive cloud based load balancer outside your cluster for each service.</p>

<p>For Layer 7 (HTTP) applications we can do better. Outside of Kubernetes this problem is typically solved with a reverse proxy which decodes the incoming requests and sends them to the right upstream service. In Kubernetes this functionality is called <strong>Ingress</strong>. Ingress is a Kubernetes native way to implement the virtual hosting pattern described above. This virtual hosting pattern requires the administrator to manage the load balancer configuration file. In a dynamic environment as the set of virtual hosts expands, this can be very complex. Ingress simplifies this by standardizing that configuration, moving it to a standard Kubernetes object, and merging multiple Ingress objects into a single config for the load balancer.</p>

<p>The ingress controller is a software system exposed outside the cluster using a service type: <code class="language-plaintext highlighter-rouge">LoadBalancer</code>. It then proxies requests to “upstream” servers. The configuration of how it does this is the result of reading and monitoring Ingress objects.</p>

<h2 id="ingress-spec-versus-ingress-controllers">Ingress spec versus Ingress controllers</h2>

<p>The implementation of Ingress is very different from other regular resource objects in Kubernetes. Ingress is split into a common resource specification and a controller implementation. There is not “standard” Ingress controller that is built into Kubernetes, and the user must install one of the many optional implementations.</p>

<p>Users can create and modify Ingress objects just like every other object; but by default, there is no code running to actually oct on those objects. It is up to the users (or the distribution they are using) to install and manage an outside controller. In this way the, the controller is pluggable.</p>

<p>There are multiple reasons for this pluggable approach in Ingress. First there is no single HTTP load balancer that can universally be used. In addition to many software load balancers (both open source and proprietary), there are also load balancing capabilities provided by cloud providers such as ELB on AWS, and hardware based load balancers. The second reason is that Ingress was added to Kubernetes before the standard extensibility options. In the future Ingress may change to follow those new extensibility options.</p>

<h2 id="configuring-dns">Configuring DNS</h2>

<p>To make Ingress work well, you need to configure DNS entries to the external address of your load balancer. You can map multiple host names to a single external endpoint and the Ingress controller will direct incoming requests to the appropriate upstream service based on that hostname.</p>

<h2 id="using-hostname">Using hostname</h2>

<p>The most common use of directing traffic based on properties of a request is using hostnames. The yaml file below defines an Ingress object that will route requests to <code class="language-plaintext highlighter-rouge">alpaca.example.com</code> to the alpaca service.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">host-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">alpaca.example.com</span>
  <span class="na">http</span><span class="pi">:</span> <span class="na">paths</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">backend</span><span class="pi">:</span>
    <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
    <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<h2 id="default-http-backends">Default http backends</h2>

<p>Certain Ingress controllers use a concept of a <code class="language-plaintext highlighter-rouge">default-http-backend</code>. This is used if a request comes in that does not have any http backend defined.  The default backend is uses as the default.</p>

<h2 id="paths">Paths</h2>

<p>You can use Ingress to direct traffic based on not just he hostname, but also the path in the HTTP request.  We can do this by specifying a path in the <code class="language-plaintext highlighter-rouge">paths</code> entry. In the example below, we are directing everything coming into <code class="language-plaintext highlighter-rouge">http://bandicoot.example.com</code> to the bandicoot service, but we send <code class="language-plaintext highlighter-rouge">http://bandicoot.example.com/a</code> to the alpaca service. This type of scenario can be used to host multiple services on different paths of a single domain.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">path-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">bandicoot.example.com</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">bandicoot</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/a/"</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<p>When there are multiple paths on the same host listed in the Ingress system, the longest prefix matches. In the example above, traffic staring with /a/ is forwarded to the alpaca service, while all other traffic (starting with /) is directed to the bandicoot service.  As requests get proxied to the upstream service, the path remains unmodified. That means a request to <code class="language-plaintext highlighter-rouge">bandicoot.example.com/a/</code> shows up to the upstream server that is configured for that request hostname path.</p>

<h2 id="advanced-ingress-topics-and-gotchas">Advanced Ingress topics and gotchas</h2>

<p>Many of Ingress’ extended features are exposed via annotations on the Ingress object. Be careful, as these annotations can be hard to validate and are easy to get wrong. Many of these annotations apply to the entire Ingress object and so can be more general than you might like. To scope the annotations down you can always split a single Ingress object into multiple Ingress objects. The ingress controller should read them and merge them together.</p>

<h2 id="running-multiple-ingress-controllers">Running multiple Ingress Controllers</h2>

<p>If you are running multiple Ingress controllers on a single cluster you can control which Ingress object is meant for which ingress controller using the <code class="language-plaintext highlighter-rouge">kubernetes.io/ingress.class</code> annotation. The value should be a string that specifies which Ingress controller should look at this object. The ingress controllers themselves, then, should be configured wth that same string and should only respect those Ingress objects with the correct annotation. If <code class="language-plaintext highlighter-rouge">kubernetes.io/ingress.class</code> annotation is missing, behavior is undefined. Its likely multiple controllers will fight to satisfy the Ingress and write the status field of the Ingress objects.</p>

<h2 id="multiple-ingress-objects">Multiple Ingress Objects</h2>

<p>If you specify multiple Ingress objects, the Ingress controllers should read them all and try to merge them into a coherent configuration. If duplicate or conflicting configurations are specified, the behavior is undefined.</p>

<h2 id="ingress-and-namespaces">Ingress and namespaces</h2>

<p>For security an Ingress object can only refer to an upstream service in the same namespace. This means that you can’t use an Ingress object to point a subpath to a service in another namespace. On the other hand multiple multiple Ingress objects in the different namespaces can specify sub-paths for the same host. These Ingress objects are then merged together to come up with the final config for the Ingress controller. This cross-namespace behavior means that it is necessary that Ingress be coordinated globally across the cluster; otherwise an Ingress object in one namespace can cause issues for another namespace. Advanced users may try to force controls on this using admission controller, but there are no restrictions out of the box.</p>

<h2 id="path-rewriting">Path rewriting</h2>

<p>Some ingress controller implementations support, optionally, doing path rewriting. This can modify the path in the HTTP request as it gets proxied. This is typically specified with an annotation ont he Ingress controller (check the documentation for controller you are using). Be careful with path rewriting as if your web applications tries to link within itself using absolute paths the rewrite may cause issues.</p>

<h2 id="serving-tls">Serving TLS</h2>

<p>To serve TLS via Ingress you will need to specify the certificate and key.  You can create the secret using <code class="language-plaintext highlighter-rouge">kubectl create secret tls &lt;secret-name&gt; --cert &lt;certificate-pem-file&gt; --key &lt;private-key-pem-file&gt;</code>. Once you have the certificate uploaded, you can reference it in an Ingress object. If multiple Ingress objects specify certificates for the same hostname, the behavior is undefined.</p>

<p>Check out <a href="https://github.com/cert-manager/cert-manager">cert-mager</a> for setting up an API driven local certificate authority.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tls-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">tls</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">alpaca.example.com</span>
      <span class="na">secretName</span><span class="pi">:</span> <span class="s">tls-secret-name</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">alpaca.example.com</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">backend</span><span class="pi">:</span>
      <span class="na">serviceName</span><span class="pi">:</span> <span class="s">alpaca</span>
      <span class="na">servicePort</span><span class="pi">:</span> <span class="m">8080</span>
</code></pre></div></div>

<h2 id="alternate-ingress-implementations">Alternate Ingress Implementations</h2>

<p>Cloud providers have their own L7 based Ingress implementation that exposes the specific cloud based load balancer.  Instead of configuring the lod balancer to run in a Pod, these controllers take Ingress objects and use them to configure the cloud based load balancer via an API. This reduces load on the cluster, but you need to pay for the load balancer resources used.</p>

<p>The most popular generic Ingress controller is probably the open source <a href="https://github.com/kubernetes/ingress-nginx/">NGINX Ingress controller</a>. There is also a commercial version based on this project.   <a href="https://github.com/emissary-ingress/emissary">Embassador</a> and <a href="https://github.com/solo-io/gloo">Gloo</a> are options you should look at if you are looking to build an API gateway.  <a href="https://traefik.io">Traefik</a> is a reverse proxy that can also function as an Ingress controller. It has a set of features and dashboards that are very developer-friendly.</p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">My References</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">My References</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
